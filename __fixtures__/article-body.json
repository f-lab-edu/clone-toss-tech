{
    "1": "토스 프론트엔드 챕터를 소개합니다!박서진ㆍFrontend Developer2021. 4. 28안녕하세요. 토스 프론트엔드 챕터 블로그에 오신 것을 환영합니다. 앞으로 토스팀에서 프론트엔드 개발을 하면서 생기는 이야기들을 정리하여 블로그로 전해드리려고 합니다.토스 프론트엔드 챕터는?토스에서는 같은 일을 하는 사람들을 모아서 챕터라고 합니다. 프론트엔드 챕터는 JavaScript, HTML, CSS를 이용하여 웹 프론트엔드 제품을 만드는 직군을 가리키는 말입니다.토스의 프론트엔드 개발자들은 ‘사일로’라고 하는 목적 조직에서 각자 독립적으로 일하고 있습니다. 각각의 사일로는 토스에서 하나의 제품을 전담하며, 작은 스타트업처럼 독립적으로 일합니다. 예를 들어, 송금을 담당하는 사일로는 송금 사일로, 결제를 담당하는 사일로는 결제 사일로입니다.사일로는 기본적으로 프로덕트 오너, 디자이너, 서버 개발자, 프론트엔드 개발자, 데이터 분석가 1명씩으로 구성됩니다. 사일로에서 프론트엔드 개발자는 다른 직군의 동료들과 가까운 자리에서 밀접하게 협업하면서, 주어진 반기별 OKR(목표)를 달성하기 위해 꾸준히 제품 개선에 참여하고 있습니다.→ 토스의 첫인상을 책임지는 사람들, 프론트엔드 개발자를 만나다함께 협업하기각 사일로에서 독립적으로 일하는 프론트엔드 개발자들이지만, 하나의 팀처럼 같이 협업하고 있습니다. 예를 들어서,함께 라이브러리를 개발하고 있습니다. UI 컴포넌트, 비동기 처리와 같이 프론트엔드 개발을 하면서 꼭 필요한 것들이 모두 라이브러리화되어 있습니다. 서비스를 개발하다가 적절히 추상화된 코드 조각이 생기면 라이브러리에 꾸준히 반영하고 있습니다.서로 코드를 리뷰합니다. 라이브러리 개발과 서비스 개발 모두에서 코드 리뷰가 의무화되어 있습니다. 코드 리뷰를 주고받으면서 더 나은 설계에 대해 토론하고, 새로운 기술 채택에 대해 의견을 나누고 있습니다.챕터 위클리에 참여합니다. 주기적으로 모든 프론트엔드 개발자들이 모여서 그동안 있었던 개발 업무를 주제로 이야기합니다. 챕터 위클리에서는 돌아가면서 “Tech Talk” 라고 하는 작은 발표를 열고 있는데, 개발 노하우나 새로운 기술 토픽을 소개합니다.그 이외에도 비정기적으로 기술 스터디를 열기도 합니다. 서로 자리가 떨어져 있어도 슬랙 채널이나 각자의 자리에서 활발하게 소통합니다.프론트엔드 챕터가 사용하는 기술React, TypeScript, Next.js가 토스 프론트엔드 챕터가 다루는 기술의 핵심을 이루고 있습니다.React: 토스 웹 페이지는 대부분 React로 구성되어 있습니다. 제품의 성격에 따라 React Suspense와 같은 실험적인 기술을 적극적으로 도입하고는 합니다.TypeScript: 토스에서는 모든 코드를 TypeScript로 작성합니다. 사람의 실수를 줄이고, 빌드 시간에 미리 오류를 찾아냄으로써 웹 서비스를 안정적으로 운영하는 데에 큰 도움이 되고 있습니다.Next.js: 토스 사용자들에게 더 빠른 웹을 보여주기 위해 적극적으로 SSR와 Static Export를 활용하고 있습니다.그 이외에 아래 기술을 보조적으로 사용하고 있어요.Yarn Berry: 토스에서 의존성을 관리하기 위해 사용하는 패키지 매니저입니다. NPM보다 엄격히 package.json을 관리함으로써 개발자의 실수를 더 빨리 발견할 수 있도록 해 줍니다. NPM보다 모듈을 로드하는 속도가 빠릅니다. (홈페이지)Emotion: CSS를 다루기 위해 emotion 라이브러리를 사용하고 있습니다. CSS Prop으로 생산적으로 스타일을 다룰 수 있습니다. 서버 사이드 렌더링을 했을 때 첫 렌더에 포함되는 Critical CSS만을 HTML에 포함해줌으로써 더 빠르게 화면을 보여줄 수 있도록 도와주기도 합니다.React Query, SWR: 비동기를 다루는 상황에서 대부분 사용하고 있는 라이브러리입니다. 선언적으로 비동기 자원을 관리하고 캐싱할 수 있습니다.Tech Talks토스에서는 모든 프론트엔드 챕터 구성원이 모이는 챕터 위클리에서 Tech Talk라고 하는 작은 발표를 열고 있습니다. 발표의 주제는 서비스 개발을 할 때의 꿀팁부터 React Suspense 소개까지 다양합니다. 2019년부터 지금까지 총 90개 이상의 크고 작은 발표가 있었습니다.토스 FE 블로그의 아티클은 위클리에서 있었던 Tech Talk의 내용을 간단히 정리한 것입니다.기술 블로그에서 공개된 내용보다 더 좋은 발표를 듣고 싶다면, 그리고 저희와 함께 웹 서비스의 문제를 풀어가고 싶다면, 언제든 토스팀에 지원해주세요. 모든 단계에서 최대한 빠르고 솔직하게 피드백을 드리겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "2": "웹 서비스 캐시 똑똑하게 다루기박서진ㆍFrontend Developer2021. 4. 29토스 프론트엔드 챕터에서는 웹 성능을 최대한으로 높이기 위해 HTTP 캐시를 적극적으로 사용하고 있습니다. 캐시를 잘못 관리했을 때, 원하는 시점에 캐시가 사라지지 않을 수 있습니다. 필요 이상으로 HTTP 요청이 발생하기도 합니다.HTTP 캐시를 효율적으로 관리하려면 Cache-Control 헤더를 섬세하게 조절해야 합니다. 토스 프론트엔드 챕터에서 다양한 생명 주기를 가지는 캐시를 다루면서 알게 된 노하우를 테크 블로그로 공유합니다.캐시의 생명 주기HTTP에서 리소스(Resource)란 웹 브라우저가 HTTP 요청으로 가져올 수 있는 모든 종류의 파일을 말합니다. 대표적으로 HTML, CSS, JS, 이미지, 비디오 파일 등이 리소스에 해당합니다.웹 브라우저가 서버에서 지금까지 요청한 적이 없는 리소스를 가져오려고 할 때, 서버와 브라우저는 완전한 HTTP 요청/응답을 주고받습니다. HTTP 요청도 완전하고, 응답도 완전합니다. 이후 HTTP 응답에 포함된 Cache-Control 헤더에 따라 받은 리소스의 생명 주기가 결정됩니다.캐시의 유효 기간: max-age서버의 Cache-Control 헤더의 값으로 max-age=<seconds> 값을 지정하면, 이 리소스의 캐시가 유효한 시간은 <seconds> 초가 됩니다.캐시의 유효 기간이 지나기 전한 번 받아온 리소스의 유효 기간이 지나기 전이라면, 브라우저는 서버에 요청을 보내지 않고 디스크 또는 메모리에서만 캐시를 읽어와 계속 사용합니다.메모리 캐시에서 불러온 HTTP 리소스예를 들어, 위 개발자 도구 캡처와 같이 어떤 JavaScript 파일을 요청하는 경우를 가정합시다. 이 리소스가 가지는 Cache-Control 헤더 값은 max-age=31536000 이기 때문에, 이 리소스는 1년(31,536,000초)동안 캐시할 수 있습니다.스크린샷에서는 유효한 캐시가 메모리에 남아 있기 때문에 (from memory cache) 라고 표기된 것을 확인할 수 있습니다.“서버에 요청을 보내지 않고” 라고 하는 말에 주의합시다. 한번 브라우저에 캐시가 저장되면 만료될 때까지 캐시는 계속 브라우저에 남아 있게 됩니다. 때문에 CDN Invalidation을 포함한 서버의 어떤 작업이 있어도 브라우저의 유효한 캐시를 지우기는 어렵습니다.Note: Cache-Control max-age 값 대신 Expires 헤더로 캐시 만료 시간을 정확히 지정할 수도 있습니다.캐시의 유효 기간이 지난 이후: 재검증그렇다면 캐시의 유효 기간이 지나면 캐시가 완전히 사라지게 될까요? 그렇지는 않습니다. 대신 브라우저는 서버에 조건부 요청(Conditional request)을 통해 캐시가 유효한지 재검증(Revalidation)을 수행합니다.재검증 결과 브라우저가 가지고 있는 캐시가 유효하다면, 서버는 [304 Not Modified] 요청을 내려줍니다. [304 Not Modified] 응답은 HTTP 본문을 포함하지 않기 때문에 매우 빠르게 내려받을 수 있습니다. 예를 들어, 위 스크린샷을 살펴보면 59.1KB 리소스의 캐시 검증을 위해 324바이트만의 네트워크 송수신만을 주고받았음을 볼 수 있습니다.If-None-Match와 If-Modified-Since가 포함된 요청대표적인 재검증 요청 헤더들로는 아래와 같은 헤더가 있습니다.If-None-Match: 캐시된 리소스의 ETag 값과 현재 서버 리소스의 ETag 값이 같은지 확인합니다.If-Modified-Since: 캐시된 리소스의 Last-Modified 값 이후에 서버 리소스가 수정되었는지 확인합니다.위의 ETag 와 Last-Modified 값은 기존에 받았던 리소스의 응답 헤더에 있는 값을 사용합니다.재검증 결과 캐시가 유효하지 않으면, 서버는 [200 OK] 또는 적합한 상태 코드를 본문과 함께 내려줍니다. 추가로 HTTP 요청을 보낼 필요 없이 바로 최신 값을 내려받을 수 있기 때문에 매우 효율적이죠. 😉max-age=0 주의보 정의대로라면 max-age=0 값이 Cache-Control 헤더로 설정되었을 때, 매번 리소스를 요청할 때마다 서버에 재검증 요청을 보내야 할 것입니다. 그렇지만 일부 모바일 브라우저의 경우 웹 브라우저를 껐다 켜기 전까지 리소스가 만료되지 않도록 하는 경우가 있습니다. 네트워크 요청을 아끼고 사용자에게 빠른 웹 경험을 제공하기 위해서라고 합니다.이 경우에는 웹 브라우저를 껐다 켜거나, 아래에서 소개할 no-store 값을 사용해주세요.no-cache와 no-storeCache-Control에서 가장 헷갈리는 두 가지 값이 있다면 바로 no-cache 와 no-store 입니다. 이름은 비슷하지만 두 값의 동작은 매우 다릅니다.no-cache 값은 대부분의 브라우저에서 max-age=0 과 동일한 뜻을 가집니다. 즉, 캐시는 저장하지만 사용하려고 할 때마다 서버에 재검증 요청을 보내야 합니다.no-store 값은 캐시를 절대로 해서는 안 되는 리소스일 때 사용합니다. 캐시를 만들어서 저장조차 하지 말라는 가장 강력한 Cache-Control 값입니다. no-store를 사용하면 브라우저는 어떤 경우에도 캐시 저장소에 해당 리소스를 저장하지 않습니다.캐시의 위치CDN과 같은 중간 서버를 사용할 때, 캐시는 여러 곳에 생길 수 있습니다. 서버가 가지고 있는 원래 응답을 CDN이 캐시합니다. CDN의 캐시된 응답은 사용자 브라우저가 다시 가져와서 캐시합니다. 이처럼 HTTP 캐시는 여러 레이어에 저장될 수 있기 때문에 세심히 다루어야 합니다.CDN Invalidation일반적으로 캐시를 없애기 위해서 “CDN Invalidation”을 수행한다고 이야기합니다. CDN Invalidation은 위 다이어그램에서 가운데에 위치하는 CDN에 저장되어 있는 캐시를 삭제한다는 뜻입니다. 브라우저의 캐시는 다른 곳에 위치하기 때문에 CDN 캐시를 삭제한다고 해서 브라우저 캐시가 삭제되지는 않습니다.경우에 따라 중간 서버나 CDN이 여러 개 있는 경우도 발생하는데, 이 경우 전체 캐시를 날리려면 중간 서버 각각에 대해서 캐시를 삭제해야 합니다.이렇게 한번 저장된 캐시는 지우기 어렵기 때문에 Cache-Control의 max-age 값은 신중히 설정하여야 합니다.Cache-Control: public과 privateCDN과 같은 중간 서버가 특정 리소스를 캐시할 수 있는지 여부를 지정하기 위해 Cache-Control 헤더 값으로 public 또는 private을 추가할 수 있습니다.public은 모든 사람과 중간 서버가 캐시를 저장할 수 있음을 나타내고, private은 가장 끝의 사용자 브라우저만 캐시를 저장할 수 있음을 나타냅니다.기존과 max-age 값과 조합하려면 Cache-Control: public, max-age=86400 과 같이 콤마로 연결할 수 있습니다.s-maxage중간 서버에서만 적용되는 max-age 값을 설정하기 위해 s-maxage 값을 사용할 수 있습니다.예를 들어, Cache-Control 값을 s-maxage=31536000, max-age=0 과 같이 설정하면 CDN에서는 1년동안 캐시되지만 브라우저에서는 매번 재검증 요청을 보내도록 설정할 수 있습니다.토스에서의 Cache-Control토스 프론트엔드 챕터는 리소스의 성격에 따라 세심히 Cache-Control 헤더 값을 조절하고 있습니다.HTML 파일일반적으로 https://service.toss.im/toss-card/introduction 과 같은 HTML 리소스는 새로 배포가 이루어질 때마다 값이 바뀔 수 있습니다. 때문에 브라우저는 항상 HTML 파일을 불러올 때 새로운 배포가 있는지 확인해야 합니다.이런 리소스에 대해 토스 프론트엔드 챕터는 Cache-Control 값으로 max-age=0, s-maxage=31536000 을 설정했습니다. 이로써 브라우저는 HTML 파일을 가져올 때마다 서버에 재검증 요청을 보내고, 그 사이에 배포가 있었다면 새로운 HTML 파일을 내려받습니다.CDN은 계속해서 HTML 파일에 대한 캐시를 가지고 있도록 했습니다. 대신 배포가 이루어질 때마다 CDN Invalidation을 발생시켜 CDN이 서버로부터 새로운 HTML 파일들을 받아오도록 설정했습니다.JS, CSS 파일JavaScript나 CSS 파일은 프론트엔드 웹 서비스를 빌드할 때마다 새로 생깁니다. 토스 프론트엔드 챕터는 임의의 버전 번호를 URL 앞부분에 붙여서 빌드 결과물마다 고유한 URL을 가지도록 설정하고 있습니다.고유 버전 번호가 붙어 있는 JavaScript 파일이렇게 JS, CSS 파일을 관리했을 때, 같은 URL에 대해 내용이 바뀔 수 있는 경우는 없습니다. 내용이 바뀔 여지가 없으므로 리소스의 캐시가 만료될 일도 없습니다.이런 리소스에 대해 토스 프론트엔드 챕터는 Cache-Control 값으로 max-age의 최대치인 max-age=31536000 을 설정하고 있습니다. 이로써 새로 배포가 일어나지 않는 한, 브라우저는 캐시에 저장된 JavaScript 파일을 계속 사용합니다.캐시 설정을 섬세히 제어함으로써 사용자는 더 빠르게 HTTP 리소스를 로드할 수 있고, 개발자는 트래픽 비용을 절감할 수 있습니다. 위에서 Cache-Control와 ETag 헤더를 리소스의 성격에 따라 잘 설정하는 것만으로 캐시를 정확하게 설정할 수 있다는 것을 살펴보았습니다. HTTP 캐시로 고민하고 있는 분들께 도움이 되었기를 기대합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "3": "JSCodeShift로 기술 부채 청산하기박지우ㆍFrontend Developer2021. 5. 4토스 프론트엔드 챕터에서는 100개 이상의 서비스들이 작은 패키지 단위로 쪼개져 활발하게 개발되고 있는데요. 공통으로 사용하는 라이브러리에서 인터페이스가 변경되는 Breaking Change가 발생하면, 의존하고 있는 모든 서비스의 코드를 수정해야 했습니다. 관리하는 코드베이스가 점점 커지면서 해야 하는 작업의 양도 계속 늘어나고는 했습니다.이에 프론트엔드 챕터는 JSCodeShift를 도입하여 대부분의 코드 수정 작업을 자동화할 수 있었습니다. 토스팀이 JSCodeShift를 도입하면서 알게 된 점과 노하우를 테크 블로그로 공유합니다.JSCodeShift란?JSCodeShift는 Facebook이 만든 JavaScript/TypeScript 코드 수정 도구입니다. JSCodeShift를 통해 코드를 수정하는 코드를 작성할 수 있습니다.찾아 바꾸기와의 비교JSCodeShift를 도입하기 전, 토스에서는 대량의 코드 수정이 필요할 때면 IDE의 찾아 바꾸기(Find & Replace)를 사용했습니다. 그러나 찾아 바꾸기로는 안전하게 코드를 수정하는 데에 한계가 많았습니다.예시 1: console.log() 모두 삭제하기프로젝트 전체에 있는 console.log() 호출을 모두 제거하고 싶은 상황을 생각해봅시다. 간단한 예제임에도 쉽게 고칠 수 없는 엣지 케이스들이 발생합니다. 우선 console.log 안에 들어가는 인자의 내용이 달라질 수 있습니다. console.log에 여러 인자를 넘겨서 함수 호출이 여러 줄에 걸칠 수도 있습니다.이것을 정규식을 이용하여 어느 정도 해결할 수도 있습니다. 그러나 다양한 엣지케이스에 대응하기 위해서 정규식이 점점 복잡해지는 경우가 발생했습니다. 또 정규식은 정규 언어이기 때문에 기술적으로 대응할 수 없는 경우도 존재했습니다.예시 2: default import된 객체의 프로퍼티 수정하기아래와 같은 코드가 있었다고 생각해봅시다.import A from '@tossteam/a';\n\nA.foo();어느 순간 A.foo() 함수가 A.bar() 함수로 이름이 변경되었다고 가정해봅시다.Default import의 변수 이름은 사용하는 사람마다 임의로 정할 수 있기 때문에, 어떤 사람은 이 라이브러리를 B 라고 하는 이름으로 사용하고 있을 수도 있습니다. 때문에 이 라이브러리를 B.foo() 처럼 사용하고 있던 코드가 있었다면, B.bar() 로 수정해주어야 합니다.이런 경우는 찾아 바꾸기로 쉽게 대응하기 어렵습니다.JSCodeShift 기초JSCodeShift는 추상 구문 트리(AST, Abstract Syntax Tree)를 이용하여 코드를 수정하는 방법을 제공함으로써 코드 수정 작업을 정확하고 편리하게 할 수 있도록 도와줍니다.추상 구문 트리 (AST)추상 구문 트리는 프로그램의 소스 코드를 쉽게 다룰 수 있도록 도와주는 자료구조입니다.예를 들어서, 다음 import 문을 추상 구문 트리로 옮기면 이런 모습이 됩니다.import React, { useMemo } from 'react';ImportDeclaration {\n  specifiers: [\n    ImportDefaultSpecifier {\n      local: Identifier {\n        name: \"React\"\n      }\n    },\n    ImportSpecifier {\n      local: Identifier {\n        name: \"useMemo\"\n      }\n    }\n  ],\n  source: Literal {\n    value: \"react\"\n  }\n}살펴보면 import 문이 ImportDeclaration 객체로 바뀌었습니다. 또 내부에서 사용되는 Default Import와 Named Import, 라이브러리 이름이 알맞은 객체로 옮겨진 것을 확인할 수 있습니다.ASTExplorer작성한 코드의 추상 구문 트리를 ASTExplorer로 쉽게 확인할 수 있습니다. 코드만 붙여넣으면 해당하는 구문 트리를 바로 확인할 수 있어 편리합니다. 소스 코드의 특정 부분에 커서를 옮기면 그 부분이 트리의 어떤 부분에 해당하는지 바로 볼 수 있기도 합니다. 😉 추상 구문 트리에 익숙하지 않다면, 사용해보시는 것을 권장합니다.라이브러리별 추상 구문 트리라이브러리마다 사용하는 추상 구문 트리의 모습은 다를 수 있습니다. 예를 들어서 같은 JavaScript를 다루더라도 ESLint가 사용하는 트리와 Babel이 사용하는 트리는 약간 다릅니다. JSCodeShift는 Babel이 사용하는 트리를 사용하고 있습니다.ASTExplorer 상단 메뉴에서 사용할 추상 구문 트리를 선택할 수 있습니다. JSCodeShift가 사용하는 트리는 @babel/parser 입니다.JSCodeShift 사용하기JSCodeShift로 코드를 수정하는 과정은 크게 4가지 작업으로 나눌 수 있습니다.AST로 파싱: 파일의 소스 코드를 AST로 파싱합니다.수정할 노드 선택: AST에서 수정할 노드를 선택합니다.수정하기: 검색한 노드를 JSCodeShift가 제공하는 유틸리티로 코드를 변경시킵니다.소스 코드로 내보내기: 수정된 AST를 JavaScript 소스 코드로 내보냅니다.예를 들어, 이런 형식으로 코드를 작성합니다./* transformSomeCode.js */\nfunction transformSomeCode(file, { jscodeshift }) {\n  // 1. AST로 파싱\n  const tree = jscodeshift(file.source);\n\n  // 2. 수정할 노드 선택\n  const nodes = tree.find(...);\n\n  // 3. 수정\n  jscodeshift(nodes)\n    .remove() | .replaceWith() | .insertBefore()\n\n  // 4. 소스 코드로 내보내기\n  return tree.toSource();\n}이후 JSCodeShift CLI를 이용하여 jscodeshift -t transformSomeCode.js <target> 와 같은 명령을 실행하면 <target> 에 있는 소스 코드들이 transformSomeCode.js 에 정의된 규칙에 맞게 수정됩니다.이제 본격적으로 JSCodeShift에서 자주 사용되는 메서드들을 살펴보겠습니다.수정할 노드 선택하기: find()기본적으로 수정할 노드를 선택하기 위해 find() 함수를 사용합니다.예를 들어, react 라이브러리의 useMemo 를 가져오는 import 구문들을 선택하기 위해서는 아래와 같이 코드를 작성할 수 있습니다.const nodes = tree.find(\n  /* 찾을 AST 노드 타입 */\n  jscodeshift.ImportDeclaration,\n  /* 필터링할 함수 */\n  node => {\n    return (\n      /* ImportDeclaration 중에서 */\n      node.type === 'ImportDeclaration' &&\n      /* react 라이브러리에서 */\n      node.source.value === 'react' &&\n      /* 가져오는 것 중에서 */\n      node.specifiers.some(specifier => {\n        /* useMemo를 포함하는 것을 */\n        return (\n          specifier.type === 'ImportSpecifier' &&\n          specifier.imported.name === 'useMemo'\n        );\n      })\n      /* 선택한다 */\n    )\n  }\n);노드 삭제하기: remove()선택한 노드를 삭제하기 위해 remove() 함수를 사용합니다.예를 들어서, 아래와 같이 코드를 작성함으로써 선택한 node 의 목록을 삭제할 수 있습니다.for (const node of nodes) {\n  jscodeshift(node).remove();\n}노드를 다른 노드로 치환하기: replaceWith()선택한 노드를 새로운 노드로 치환하려고 할 때 replaceWith() 함수를 사용할 수 있습니다.예를 들어서, 선택한 node 들을 다른 모습으로 치환하기 위해서는 아래와 같이 코드를 작성할 수 있습니다.for (const node of nodes) {\n  /* 노드를 만드는 방법에 대해서 아래에서 더 자세히 다룹니다. */\n  const newNode = createNode();\n\n  jscodeshift(node).replaceWith(newNode);\n}새로운 노드 만들기replaceWith() 와 같은 함수에서 사용하기 위해서 새로운 노드를 만들 때는 JSCodeShift에서 제공하는 도우미 함수들을 사용할 수 있습니다.각 노드를 만드는 방법을 모두 알 필요는 없습니다. TypeScript를 사용하는 경우, 각 함수가 어떤 인자를 받는지 바로 확인할 수 있습니다. JavaScript를 사용하는 경우, ast-types가 정의하는 타입 정보를 참고해주세요.변수 참조: foo와 같은 변수에 참조하는 노드를 만들기 위해서 jscodeshift.identifier() 를 사용할 수 있습니다.jscodeshift.identifier('foo');멤버 접근: 변수 foo의 멤버 bar 에 접근하는 노드를 만들기 위해서 jscodeshift.memberExpression() 을 사용할 수 있습니다.jscodeshift.memberExpression(\n  jscodeshift.identifier('foo'),\n  jscodeshift.identifier('bar')\n);import 문: import { useMemo } from 'react'; 와 같은 import 문을 만들기 위해서 jscodeshift.importDeclaration() 을 사용할 수 있습니다.jscodeShift.importDeclaration(\n  [\n    jscodeShift.importSpecifier(\n      jscodeshift.identifier('useMemo')\n    )\n  ],\n  jscodeshift.literal('react')\n);JSCodeShift 사용 예시토스 프론트엔드 챕터에서는 2020년 import { Adaptive } from '@tossteam/web-development-kits' 와 같은 import 문을 모두 import { adaptive } from '@tossteam/colors' 으로 수정해야 하는 필요성이 있었습니다.이런 경우는 찾아 바꾸기로 해결하는 데에 어려움이 있었습니다. 코드를 수정하는 규칙이 복잡했기 때문입니다.@tossteam/web-development-kits 라이브러리로부터 Adaptive 뿐 아니라 다른 변수나 함수를 import 하는 경우가 있었습니다. 그런 경우에는 전체 import 문을 지우는 것이 아닌, Adaptive 를 가져오는 부분만 삭제해야 했습니다.Adaptive 를 import하는 부분이 삭제된 경우에만 import { adaptive } from '@tossteam/colors'; 와 같이 새로운 import 문을 파일의 가장 처음에 추가해주어야 했습니다. 아닌 경우, 사용하지 않은 변수로 인해 컴파일 시간에 오류가 발생했습니다.Adaptive 를 import하는 부분이 삭제된 경우에만 그 파일에서 사용되는 모든 Adaptive 변수를 adaptive 로 바꿔줘야 했습니다.다행히 토스팀에서는 간단히 이 문제를 JSCodeShift로 해결할 수 있었습니다. 저희가 설계한 JSCodeShift 변환 코드의 구조는 다음과 같습니다.function transformLegacyImportToNewImport(file, { jscodeshift }) {\n  const root = jscodeshift(file.source);\n\n  /* 오래된 import 문들을 찾음 */\n  const oldImports = findOldImports(root, { jscodeshift });\n\n  /* 오래된 import 문이 없는 파일인 경우, 아무 작업을 하지 않음 */\n  if (oldImports.length === 0) {\n    return;\n  }\n\n  for (const oldImport of oldImports) {\n    /* 오래된 import 문에서 Adaptive를 가져오는 부분을 삭제 */\n    /* (Adaptive만을 가져오는 import 문인 경우, import 문 전체를 삭제) */\n    removeImportMember(root, oldImport, 'Adaptive', { jscodeshift });\n  }\n\n  /* @tossteam/colors에서 adaptive를 import하는 부분을 추가 */\n  /* (@tossteam/colors를 import하고 있지 않은 경우, import 문을 추가) */\n  addImportMember(root, '@tossteam/colors', 'adaptive', { jscodeshift });\n\n  /* Adaptive 변수를 모두 adaptive로 치환 */\n  const oldAdaptives = findIdentifiers(root, 'Adaptive', { jscodeshift });\n\n  for (const oldAdaptive of oldAdaptives) {\n    jscodeshift(oldAdaptive).replaceWith(\n      jscodeshift.identifier('adaptive')\n    );\n  }\n\n  /* 수정된 소스코드를 반환 */\n  return root.toSource();\n}\n\n이 중에서 removeImportMember 함수와 같은 경우, 아래와 같이 간단히 구현할 수 있었습니다.function removeImportMember(root, importNode, name, { jscodeshift }) {\n  const oldSpecifiers = importNode.value.specifiers;\n\n  /* name을 import하는 부분을 삭제 */\n  const newSpecifiers = oldSpecifiers.filter(specifier => {\n    return (\n      specifier.type !== 'ImportSpecifier' ||\n      specifier.imported.name !== name\n    );\n  }\n\n  /* 더 이상 import할 것이 남지 않은 경우에는, import 문을 삭제 */\n  if (newSpecifiers.length === 0) {\n    jscodeshift(importNode).remove();\n    return;\n  }\n\n  /* 그렇지 않은 경우, import 문에서 name을 가져오는 부분만 삭제 */\n  jscodeshift(importNode).replaceWith(\n    jscodeshift.importDeclaration(\n      newSpecifiers,\n      importNode.value.source\n    )\n  );\n}다른 함수의 경우에도 유사하게 JSCodeShift API를 이용하여 구현할 수 있었습니다.JSCodeShift 테스트하기JSCodeShift는 작성한 변환 코드가 잘 작동하는지 테스트할 수 있도록 testUtils 라고 하는 이름의 테스트 도구를 제공합니다. 테스트 파일의 디렉토리 구조를 JSCodeShift가 요구하는 대로 맞춰야 하지만, 손쉽게 Jest에 테스트를 붙일 수 있어서 편리합니다.테스트가 잘 붙어 있으면, JSCodeShift 코드의 문제점을 바로바로 찾을 수 있게 됩니다. 개발 속도도 절약되는 만큼, JSCodeShift를 개발할 때는 꼭 테스트와 함께 하는 것을 추천합니다.JSCodeShift 테스트와 관련된 자세한 내용은 JSCodeShift README에서 확인할 수 있습니다.토스팀과 JSCodeShift토스 프론트엔드 개발팀은 짧은 시간동안 빠르게 개발환경을 개선해오면서 대량의 레거시 코드를 최신 라이브러리와 코드 컨벤션에 맞추도록 수정해주어야 했습니다. 경우에 따라서는 작성된지 2년이 지난 오래된 코드가 수만 줄 이상 존재하기도 했습니다.이때 JSCodeShift를 사용함으로써 그런 코드도 한번에 최신 코드와 같이 일관성을 맞출 수 있었습니다. 이번 JSCodeShift 가이드가 레거시 시스템을 다루는 다른 프론트엔드 개발자 분들께 도움이 되었으면 합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "4": "node_modules로부터 우리를 구원해 줄 Yarn Berry박서진ㆍFrontend Developer2021. 5. 7토스 프론트엔드 챕터에서는 지난해부터 의존성을 관리하기 위해 Yarn Berry(v2)를 도입했습니다. 처음에는 일부 레포지토리부터 시작하여, 현재는 대부분의 레포지토리에 Yarn Berry가 적용되어 있는데요. 토스팀이 새로운 패키지 관리 시스템을 도입하게 된 배경과 사용하면서 좋았던 점을 테크 블로그를 통해 공유합니다.Yarn Berry란?Yarn Berry는 Node.js를 위한 새로운 패키지 관리 시스템으로, Yarn v1의 주요 개발자인 Maël Nison 씨가 만들었습니다. 2020년 1월 25일부터 정식 버전(v2)가 출시되어, 현재는 Babel과 같은 큰 오픈소스 레포지토리에서도 채택하고 있습니다. Yarn Berry는 GitHub yarnpkg/berry 레포지토리에서 소스코드가 관리되고 있습니다.Yarn Berry는 기존의 \"깨져 있는\" NPM 패키지 관리 시스템을 혁신적으로 개선합니다.NPM의 문제점NPM은 Node.js 설치 시에 기본으로 제공되어 범용적으로 사용되고 있으나, 비효율적이거나 깨져 있는 부분이 많습니다.비효율적인 의존성 검색NPM은 파일 시스템을 이용하여 의존성을 관리합니다. 익숙한 node_modules 폴더를 이용하는 것이 특징인데요. 이렇게 관리했을 때 의존성 검색은 비효율적으로 동작합니다.예를 들어, /Users/toss/dev/toss-frontend-libraries 폴더에서 require() 문을 이용하여 react 패키지를 불러오는 상황을 가정합시다.라이브러리를 찾기 위해 순회하는 디렉토리의 목록을 확인하려고 할 때, Node.js에서 제공하는 require.resolve.paths() 함수를 사용할 수 있습니다. 이 함수는 NPM이 검색하는 디렉토리의 목록을 반환합니다.$ node\nWelcome to Node.js v12.16.3.\nType \".help\" for more information.\n> require.resolve.paths('react')\n[\n  '/Users/toss/dev/toss-frontend-libraries/repl/node_modules',\n  '/Users/toss/dev/toss-frontend-libraries/node_modules',\n  '/Users/toss/node_modules',\n  '/Users/node_modules',\n  '/node_modules',\n  '/Users/toss/.node_modules',\n  '/Users/toss/.node_libraries',\n  '/Users/toss/.nvm/versions/node/v12.16.3/lib/node',\n  '/Users/toss/.node_modules',\n  '/Users/toss/.node_libraries',\n  '/Users/toss/.nvm/versions/node/v12.16.3/lib/node'\n]\n\n목록에서 확인할 수 있는 것처럼, NPM은 패키지를 찾기 위해서 계속 상위 디렉토리의 node_modules 폴더를 탐색합니다. 따라서 패키지를 바로 찾지 못할수록 readdir, stat과 같은 느린 I/O 호출이 반복됩니다. 경우에 따라서는 I/O 호출이 중간에 실패하기도 합니다.TypeScript 4.0까지는 node_modules를 이용한 패키지 탐색이 너무 비효율적인 나머지, 패키지를 처음으로 import 하기 전까지는 node_modules 내부의 타입 정보를 찾아보지 않기도 했습니다. (TS 4.0 Changelog)환경에 따라 달라지는 동작NPM은 패키지를 찾지 못하면 상위 디렉토리의 node_modules 폴더를 계속 검색합니다. 이 특성 때문에 어떤 의존성을 찾을 수 있는지는 해당 패키지의 상위 디렉토리 환경에 따라 달라집니다.예를 들어, 상위 디렉토리가 어떤 node_modules를 포함하고 있는지에 따라 의존성을 불러올 수 있기도 하고, 없기도 합니다. 다른 버전의 의존성을 잘못 불러올 수 있는 여지도 존재합니다.이렇게 환경에 따라 동작이 변하는 것은 나쁜 징조입니다. 해당 상황을 재현하기 까다로워지기 때문입니다.비효율적인 설치NPM에서 구성하는 node_modules 디렉토리 구조는 매우 큰 공간을 차지합니다. 일반적으로 간단한 CLI 프로젝트도 수백 메가바이트의 node_modules 폴더가 필요합니다. 용량만 많이 차지할 뿐 아니라, 큰 node_modules 디렉토리 구조를 만들기 위해서는 많은 I/O 작업이 필요합니다.node_modules 폴더는 복잡하기 때문에 설치가 유효한지 검증하기 어렵습니다. 예를 들어, 수백 개의 패키지가 서로를 의존하는 복잡한 의존성 트리에서 node_modules 디렉토리 구조는 깊어집니다.이렇게 깊은 트리 구조에서 의존성이 잘 설치되어 있는지 검증하려면 많은 수의 I/O 호출이 필요합니다. 일반적으로 디스크 I/O 호출은 메모리의 자료구조를 다루는 것보다 훨씬 느립니다. 이런 문제로 인해 Yarn v1이나 NPM은 기본적인 의존성 트리의 유효성까지만 검증하고, 각 패키지의 내용이 올바른지는 확인하지 않습니다.유령 의존성 (Phantom Dependency)NPM 및 Yarn v1에서는 중복해서 설치되는 node_modules를 아끼기 위해 끌어올리기(Hoisting) 기법을 사용합니다.예를 들어, 의존성 트리가 왼쪽의 모습을 하고 있다고 가정합시다.왼쪽 트리에서 [A (1.0)]과 [B (1.0)] 패키지는 두 번 설치되므로 디스크 공간을 낭비합니다. NPM과 Yarn v1에서는 디스크 공간을 아끼기 위해 원래 트리의 모양을 오른쪽 트리처럼 바꿉니다.오른쪽 트리로 의존성 트리가 바뀌면서 package-1 에서는 원래 require() 할 수 없었던 [B (1.0)] 라이브러리를 불러올 수 있게 되었습니다.이렇게 끌어올리기에 따라 직접 의존하고 있지 않은 라이브러리를 require() 할 수 있는 현상을 유령 의존성(Phantom Dependency)이라고 부릅니다.유령 의존성 현상이 발생할 때, package.json에 명시하지 않은 라이브러리를 조용히 사용할 수 있게 됩니다. 다른 의존성을 package.json 에서 제거했을 때 소리없이 같이 사라지기도 합니다. 이런 특성은 의존성 관리 시스템을 혼란스럽게 만듭니다.Plug'n'Play (PnP)Yarn Berry는 위에서 언급한 문제를 새로운 Plug'n'Play 전략을 이용하여 해결합니다.Plug'n'Play의 배경Yarn v1은 package.json 파일을 기반으로 의존성 트리를 생성하고, 디스크에 node_modules 디렉토리 구조를 만듭니다. 이미 패키지의 의존성 구조를 완전히 알고 있는 것입니다.node_modules 파일 시스템을 이용한 의존성 관리는 깨지기 쉽습니다. 모든 패키지 매니저가 실수하기 쉬운 Node 내장 의존성 관리 시스템을 사용해야 할까요? 패키지 매니저들이 node_modules 디렉토리 구조를 만드는 것에 그치지 않고, 보다 근본적으로 안전하게 의존성을 관리하면 어떨까요?Plug'n'Play는 이런 생각에서 출발했습니다.Plug'n'Play 켜기NPM에서 최신 버전의 Yarn을 내려받고, 버전을 Berry로 설정하면 Yarn Berry를 사용할 수 있습니다.$ npm install -g yarn\n$ cd ../path/to/some-package\n$ yarn set version berryYarn Berry는 기존 Node.js 의존성 관리 시스템과 많이 다르기 때문에 하위호환을 위해 패키지 단위로만 도입할 수 있습니다.Plug'n'Play의 동작 방법Plug'n'Play 설치 모드에서 yarn install 로 의존성을 설치했을 때, 기존과 다른 모습을 볼 수 있습니다.Yarn Berry는 node_modules를 생성하지 않습니다. 대신 .yarn/cache 폴더에 의존성의 정보가 저장되고, .pnp.cjs 파일에 의존성을 찾을 수 있는 정보가 기록됩니다. .pnp.cjs를 이용하면 디스크 I/O 없이 어떤 패키지가 어떤 라이브러리에 의존하는지, 각 라이브러리는 어디에 위치하는지를 바로 알 수 있습니다.예를 들어, react 패키지는 .pnp.cjs 파일에서 다음과 같이 나타납니다./* react 패키지 중에서 */\n[\"react\", [\n  /* npm:17.0.1 버전은 */\n  [\"npm:17.0.1\", {\n    /* 이 위치에 있고 */\n    \"packageLocation\": \"./.yarn/cache/react-npm-17.0.1-98658812fc-a76d86ec97.zip/node_modules/react/\",\n    /* 이 의존성들을 참조한다. */\n    \"packageDependencies\": [\n      [\"loose-envify\", \"npm:1.4.0\"],\n      [\"object-assign\", \"npm:4.1.1\"]\n    ],\n  }]\n]],\n\nreact 17.0.1 버전 패키지의 위치와 의존성의 목록을 완전하게 기술하고 있는 것을 확인할 수 있습니다. 이로부터 특정 패키지와 의존성에 대한 정보가 필요할 때 바로 알 수 있습니다.Yarn은 Node.js가 제공하는 require() 문의 동작을 덮어씀으로써 효율적으로 패키지를 찾을 수 있도록 합니다. 이 때문에 PnP API를 이용하여 의존성 관리를 하고 있을 때에는 node 명령어 대신 yarn node 명령어를 사용해야 합니다.$ yarn node일반적으로 Node.js 앱을 실행할 때에는 package.json의 scripts 에 실행 스크립트를 등록하여 사용하게 됩니다. 이때 Yarn v1에서 사용하던 것처럼 Yarn으로 스크립트를 실행하기만 하면 자동으로 PnP로 의존성을 불러옵니다.$ yarn devZipFS (Zip Filesystem)zip으로 묶인 라이브러리가 저장된 .yarn/cache 폴더Yarn PnP 시스템에서 각 의존성은 Zip 아카이브로 관리됩니다. 예를 들어, Recoil 0.1.2 버전은 recoil-npm-0.1.2-9a0edbd2b9-c69105dd7d.zip과 같은 압축 파일로 관리됩니다.이후 .pnp.cjs 파일이 지정하는 바에 따라 동적으로 Zip 아카이브의 내용이 참조됩니다.Zip 아카이브로 의존성을 관리하면 다음과 같은 장점이 생깁니다.더 이상 node_modules 디렉토리 구조를 생성할 필요가 없기 때문에 설치가 신속히 완료됩니다.각 패키지는 버전마다 하나의 Zip 아카이브만을 가지기 때문에 중복해서 설치되지 않습니다. 각 Zip 아카이브가 압축되어 있음을 고려할 때, 스토리지 용량을 크게 아낄 수 있습니다.실제로 토스팀에서 의존성이 차지하는 크기를 대폭 감축할 수 있었습니다.한 서비스의 경우 NPM을 이용했을 때 node_modules 디렉토리가 약 400MB를 차지했지만, Yarn PnP를 사용했을 때 의존성 디렉토리의 크기는 120MB에 불과했습니다.의존성을 구성하는 파일의 수가 많지 않으므로, 변경 사항을 감지하거나 전체 의존성을 삭제하는 작업이 빠릅니다.없는 의존성이나 더 이상 필요 없는 의존성을 쉽게 찾을 수 있습니다.Zip 파일의 내용이 변경되었을 때에는 체크섬과 비교하여 쉽게 변경 여부를 감지할 수 있습니다.Plug'n'Play 도입 결과토스 프론트엔드 챕터가 Plug'n'Play를 도입한 결과, 다양한 장점을 느낄 수 있었습니다.의존성을 검색할 때의존성을 검색할 때, 더 이상 node_modules 폴더를 순회할 필요가 없습니다. .pnp.cjs 파일이 제공하는 자료구조를 이용하여 바로 의존성의 위치를 찾기 때문입니다. 이로써 require()에 걸리는 시간이 크게 단축되었습니다.재현 가능성패키지의 모든 의존성은 .pnp.cjs 파일을 이용하여 관리되기 때문에 더 이상 외부 환경에 영향받지 않습니다. 이로써 다양한 기기 및 CI 환경에서 require() 또는 import 문의 동작이 동일할 것임을 보장할 수 있게 되었습니다.의존성을 설치할 때더 이상 설치를 위해 깊은 node_modules 디렉토리를 생성하지 않아도 됩니다. 또 NPM이 설치하는 것처럼 같은 버전의 패키지가 여러 번 복사되어 설치 시간을 극단적으로 단축할 수 있습니다. 이에 더해 Zero-install을 사용하면 대부분 라이브러리를 설치 없이 사용할 수 있습니다.이를 이용하면 CI와 같이 반복적으로 의존성 설치 작업이 이루어지는 곳에서 시간을 크게 절약할 수 있습니다. 토스팀에서는 원래 CI에서 60초씩 걸리던 설치 작업을 Yarn PnP를 도입함으로써 수 초 이내로 단축했습니다.엄격한 의존성 관리Yarn PnP는 node_modules에서와 같이 의존성을 끌어올리지 않습니다. 이로써 각 패키지들은 자신이 package.json에 기술하는 의존성에만 접근할 수 있습니다. 기존에 환경에 따라 우연히 작동할 수 있었던 코드들이 보다 엄격히 관리되는 것입니다. 이로써 예기치 못한 버그를 쉽게 일으키던 유령 의존성 현상을 근본적으로 막을 수 있었습니다.의존성 검증node_modules를 사용하여 의존성을 관리했을 때에는 올바르게 의존성이 설치되지 못해서 의존성 폴더 전체를 지우고 다시 설치해야 하는 경우가 발생하고는 했습니다. node_modules 폴더를 검증하기 어려웠기 때문입니다. 전체 재설치를 수행할 때 node_modules 디렉토리 구조를 다시 만드느라 1분 이상의 시간이 허비되기도 했습니다.Yarn PnP에서는 Zip 파일을 이용하여 패키지를 관리하기 때문에 빠진 의존성을 찾거나 의존성 파일이 변경되었음을 찾기 쉽습니다. 이로써 의존성이 잘못되었을 때 쉽게 바로잡을 수 있습니다. 이로써 올바르게 의존성이 설치되는 것을 100%에 가깝게 보장할 수 있습니다.Zero-Install위에서 Yarn Berry의 PnP를 도입함으로써 얻을 수 있는 다양한 장점들을 살펴보았습니다. 여기에서 한 발 더 나아간 생각을 해 볼 수 있습니다. 바로 의존성도 Git 등을 이용하여 버전 관리를 하면 어떨까? 라고 하는 생각인데요.Yarn PnP은 의존성을 압축 파일로 관리하기 때문에 의존성의 용량이 작습니다. 또한 각 의존성은 하나의 Zip 파일로만 표현되기 때문에 의존성을 구성하는 파일의 숫자가 NPM만큼 많지 않습니다. 예를 들어, 일반적인 node_modules 는 1.2GB 크기이고 13만 5천개의 파일로 구성되어 있는 반면, Yarn PnP의 의존성은 139MB 크기의 2천개의 압축 파일로 구성됩니다.이처럼 용량과 파일의 숫자가 적기 때문에 Yarn Berry를 사용하면 의존성을 Git으로 관리할 수 있습니다. 그리고 이렇게 의존성의 버전을 관리할 때 더욱 큰 장점들을 발견할 수 있습니다.이렇게 Yarn Berry에서 의존성을 버전 관리에 포함하는 것을 Zero-Install이라고 합니다.Yarn Berry Git 레포지토리에서 사용하는 Zero-install의존성을 버전 관리에 포함하면 많은 장점들이 생깁니다.새로 저장소를 복제하거나 브랜치를 바꾸었다고 해서 yarn install을 실행하지 않아도 됩니다. 일반적으로 다른 의존성을 사용하는 곳으로 브랜치를 변경했을 때, 잊지 않고 의존성을 설치해주어야 했습니다. 경우에 따라서는 잘못된 의존성 버전이 사용됨으로써 웹 서비스가 알 수 없는 이유로 오동작하기도 했습니다. Zero-Install을 사용했을 때 이런 문제는 완전히 해결됩니다. 더해서 네트워크가 끊어진 곳에서는 오프라인 캐시 기능을 해주기도 합니다.CI에서 의존성 설치하는 시간을 크게 절약할 수 있습니다. 토스에서는 일반적으로 캐시가 존재하지 않을 때 의존성을 설치하기 위해서 60초~90초의 시간이 필요했습니다. Zero-Install을 사용하면 Git Clone으로 저장소를 복제했을 때 의존성들이 바로 사용 가능한 상태가 되어, 의존성을 설치할 필요가 없습니다. 이로써 CI 시간을 크게 절약할 수 있었습니다.토스 프론트엔드 챕터에서는 Zero-install 기능을 적극적으로 레포지토리에 도입함으로써 빌드와 배포 시간을 크게 단축할 수 있었습니다.그 외 Yarn Berry에서 좋았던 점이 외에 Yarn Berry는 다양한 개발자 친화적인 기능을 제공합니다.플러그인 시스템: Yarn Berry는 핵심 기능도 플러그인을 이용하여 개발되어 있을 만큼 플러그인 친화적인 환경을 자랑합니다. 필요한 만큼 Yarn의 기능을 확장하여 손쉽게 CLI로 사용할 수 있습니다.토스 프론트엔드 챕터에서는 이현섭님께서 변경된 워크스페이스를 계산하는 플러그인을 며칠만에 만들어주시기도 하셨습니다. 이처럼 Yarn Berry의 기능이 부족하다면 손쉽게 플러그인을 만들 수 있습니다.워크스페이스: Yarn Berry는 Yarn v1와 비교할 수 없을 정도로 높은 완성도의 워크스페이스 기능을 제공합니다. Yarn Berry의 Git 레포지토리에서 대표적으로 사용하는 모습을 확인할 수 있습니다. TypeScript를 사용함에도 한 패키지의 소스 코드의 변경사항이 즉시 다른 패키지에 반영되는 모습이 인상적입니다.토스 프론트엔드 챕터에서도 적극적으로 워크스페이스 기능을 사용하고 있습니다.패치 명령어 기본 지원: 경우에 따라서 NPM에 배포된 라이브러리의 일부분만 수정해서 사용하고 싶은 니즈가 있습니다. Yarn Berry는 yarn patch 명령어를 제공함으로써 쉽게 라이브러리의 일부분을 수정해서 사용할 수 있도록 합니다. 이렇게 만든 패치 파일은 patch: 프로토콜을 이용해서 쉽게 의존성 설치에 사용할 수 있습니다.토스팀은 이렇게 Yarn Berry를 도입함으로써 JavaScript 의존성을 효율적이고 안전하게 다룰 수 있었습니다. 오래 걸리던 CI 속도를 60초 이상 단축하기도 했습니다.다음 Yarn Berry 아티클에서는 실제로 거대한 서비스 모노레포를 Yarn Berry로 이전한 경험을 소개드리면서 실사용에서 주의할 점에 대해 보다 자세히 소개드리겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "5": "Template Literal Types로 타입 안전하게 코딩하기박서진ㆍFrontend Developer2021. 5. 142020년 11월 TypeScript 4.1이 출시되면서 \"Template Literal Type\"을 사용할 수 있게 되었습니다. TypeScript로 JSON Parser를 만들거나, document.querySelector 의 결과 타입을 추론할 수 있게 되어 화제가 되었는데요. 이번 아티클에서는 Template Literal Type이란 무엇인지, 이를 바탕으로 어떻게 그런 결과물을 만들 수 있었는지 간단히 예시로 소개드리고자 합니다.Template Literal Type이란?간단히 말해, Template Literal Type이란 기존 TypeScript의 String Literal Type을 기반으로 새로운 타입을 만드는 도구입니다. 구체적인 예시로 Template Literal Type에 대해 자세히 살펴보겠습니다.예시 1: 가장 간단한 형태type Toss = 'toss';\n\n// type TossPayments = 'toss payments';\ntype TossPayments = `${Toss} payments`;TypeScript Playground가장 간단한 형태로, 원래 있던 'toss' 라고 하는 타입을 바탕으로 'toss payments' 라고 하는 타입을 만드는 경우를 생각할 수 있습니다.TypeScript 4.1 이전에는 이런 문자열 작업이 불가능했지만, Template Literal Type을 이용함으로써 보다 넓은 타입 연산이 가능해졌습니다.예시 2: 하나의 Union Typetype Toss = 'toss';\ntype Companies = 'core' | 'bank' | 'securities' | 'payments' | 'insurance';\n\n// type TossCompanies = 'toss core' | 'toss bank' | 'toss securities' | ...;\ntype TossCompanies = `${Toss} ${Companies}`TypeScript PlaygroundTemplate Literal Type을 Union type(합 타입)과 함께하면, 결과물도 Union Type이 됩니다.예를 들어, 위 예시에서 'toss' 타입과 'core' | 'bank' | 'securities' | ... 타입을 Template Literal Type으로 연결하면 'toss core' | 'toss bank' | 'toss securities' | ... 와 같이 확장되는 것을 확인할 수 있습니다.예시 3: 여러 개의 Union Typetype VerticalAlignment = \"top\" | \"middle\" | \"bottom\";\ntype HorizontalAlignment = \"left\" | \"center\" | \"right\";\n\n// type Alignment =\n//   | \"top-left\"    | \"top-center\"    | \"top-right\"\n//   | \"middle-left\" | \"middle-center\" | \"middle-right\"\n//   | \"bottom-left\" | \"bottom-center\" | \"bottom-right\"\ntype Alignment = `${VerticalAlignment}-${HorizontalAlignment}`;TypeScript Playground여러 개의 Union Type을 연결할 수도 있습니다.예를 들어, 위에서는 VerticalAlignment 타입과 HorizontalAlignment 타입을 연결하여, ${VerticalAlignment}-${HorizontalAlignment} 타입을 만들었습니다.원래라면 중복해서 Alignment 타입을 다시 정의해야 했겠지만, Template Literal Type을 사용함으로써 중복 없이 더욱 간결히 타입을 표현할 수 있게 되었습니다.예시 4: 반복되는 타입 정의 없애기문제 상황// 이벤트 이름이 하나 추가될 때마다....\ntype EventNames = 'click' | 'doubleClick' | 'mouseDown' | 'mouseUp';\n\ntype MyElement = {\n    addEventListener(eventName: EventNames, handler: (e: Event) => void): void;\n\n    // onEvent() 도 하나씩 추가해줘야 한다\n    onClick(e: Event): void;\n    onDoubleClick(e: Event): void;\n    onMouseDown(e: Event): void;\n    onMouseUp(e: Event): void;\n};이벤트에 대한 핸들러를 등록할 때, addEventListener('event', handler) 와 onEvent = handler 의 두 가지 형식을 모두 사용할 수 있는 MyElement 타입을 생각해봅시다.// 두 가지 방법 모두 사용할 수 있는 경우\nelement.addEventListener('click', () => alert('I am clicked!'));\nelement.onClick = () => alert('I am clicked!');예를 들어, click 이벤트를 구독할 때, 위의 두 가지 방법을 모두 사용할 수 있는 것입니다.요소에 추가할 수 있는 이벤트의 종류는 자주 변경되고는 합니다. 예를 들어, 브라우저 API가 바뀌면서 'pointerDown' 과 같은 이벤트가 새로 추가될 수 있습니다.이런 경우, TypeScript 4.1 이전에는 매번 수동으로 여러 곳의 타입을 수정해야 했습니다. 우선 addEventListener의 인자로 사용되는 이벤트 이름 EventNames 타입에 'pointerDown' 을 넣어야 했습니다. 또 onPointerDown 메서드를 명시해야 했습니다. 잊지 않고 두 곳을 수정해야 했기 때문에, 실수하기 쉬웠습니다.하지만 Template Literal Type을 이용하면 한 곳만 수정해도 모두에 반영되도록 할 수 있습니다.type EventNames = 'click' | 'doubleClick' | 'mouseDown' | 'mouseUp';\n\n// CapitalizedEventNames = 'Click' | 'DoubleClick' | ...;\ntype CapitalizedEventNames = Capitalize<EventNames>;\n\n// type HandlerNames = 'onClick' | 'onDoubleClick' | 'onMouseDown' | 'onMouseUp';\ntype HandlerNames = `on${CapitalizedEventNames}`;\n\ntype Handlers = {\n  [H in HandlerNames]: (event: Event) => void;\n};\n\n// 원래 MyElement 그대로 작동!\ntype MyElement = Handlers & {\n  addEventListener: (eventName: EventNames, handler: (event: Event) => void) => void;\n};위 코드를 한번 자세히 살펴봅시다.CapitalizedEventNames 타입을 정의할 때, TypeScript 4.1에서 추가된 Capitalize<T> 타입을 이용하여 EventNames의 첫 글자를 대문자로 만들었습니다.HandlerNames 타입을 만들 때, Template Literal Type으로 onClick 과 같이 on 접두사를 붙였습니다.Handlers 타입에서는 기존의 onClick, onMouseDown 과 같은 이벤트 핸들러를 메서드로 가지도록 했고,마지막으로 MyElement 에서는 addEventListener 메서드를 가지는 객체와 연결하여 원래와 동일한 동작을 하는 타입을 만들 수 있었습니다.이제 EventNames 만 수정하면 MyElement 에서 이벤트를 구독하는 양쪽 모두 대응이 되므로, 코드가 깔끔해지고 실수의 여지가 적어졌습니다. ✨Conditional Type과 더 강력한 추론하기Template Literal Type은 Conditional Type과 함께 더욱 강력하게 사용할 수 있습니다.Conditional Type 되짚어보기Conditional Type은 JavaScript의 삼항 연산자와 비슷하게 분기를 수행하면서, 타입을 추론하는 방법인데요. 고급 TypeScript 사용에서 강력한 타입 연산을 하기 위해서 빠지지 않습니다.Template Literal Type을 더 잘 다루기 위해 반드시 필요한 개념이므로, 간단한 예시로 Conditional Type을 사용하는 방법에 대해 살펴보겠습니다.예시 1: 제네릭 타입 인자 꺼내오기Conditional Type을 가장 자주 사용하는 경우로,  Promise<number>와 같은 타입에서 number 를 꺼내오고 싶은 상황을 생각해봅시다.type PromiseType<T> = T extends Promise<infer U> ? U : never;\n\n// type A = number\ntype A = PromiseType<Promise<number>>;\n\n// type B = string | boolean\ntype B = PromiseType<Promise<string | boolean>>;\n\n// type C = never\ntype C = PromiseType<number>;TypeScript Playground위 코드를 살펴보면, PromiseType<T> 타입에 Promise<number> 타입을 인자로 넘기면 number 타입을 얻고 있습니다.Conditional Type이 동작하는 방식을 간단히 알아봅시다.삼항 연산자처럼 생긴 부분 가운데 X extends Y 와 같이 생긴 조건 부분은 X 타입의 변수가 Y 타입에 할당될 수 있는지에 따라 참값이 평가됩니다.예시:true extends boolean: true 는 boolean 에 할당될 수 있으므로 참으로 평가됩니다.'toss' extends string: 'toss' 는 string 에 할당될 수 있으므로 참으로 평가됩니다.Array<{ foo: string }> extends Array<unknown>: 마찬가지로 참으로 평가됩니다.string extends number: 문자열은 숫자 타입에 할당될 수 없으므로 거짓입니다.boolean extends true: boolean 타입 가운데 false 는 true 에 할당될 수 없으므로 거짓입니다.조건식이 참으로 평가될 때에는 infer 키워드를 사용할 수 있습니다. 예를 들어, Promise<number> extends Promise<infer U> 와 같은 타입을 작성하면, U 타입은 number 타입으로 추론됩니다. 이후 참인 경우에 대응되는 식에서 추론된 U 타입을 사용할 수 있습니다.예를 들어, Promise<number> extends Promise<infer U> ? U : never 에서는 조건식이 참이고 U 타입이 number로 추론되므로, 이를 평가한 타입의 결과는 number 가 됩니다.반대로 number extends Promise<infer U> ? U : never 에서는 조건식이 거짓이므로 이를 평가한 결과는 never가 됩니다.예시 2: Tuple 다루기[string, number, boolean] 과 같은 TypeScript의 Tuple Type에서 그 꼬리 부분인 [number, boolean] 과 같은 부분만 가져오고 싶은 상황을 생각해봅시다.Conditional Type과 Variadic Tuple Type을 활용함으로써 이를 간단히 구현할 수 있습니다.type TailOf<T> = T extends [unknown, ...infer U] ? U : [];\n\n// type A = [boolean, number];\ntype A = TailOf<[string, boolean, number]>;TypeScript Playground첫 요소를 제외하고 ...infer U 구문을 이용하여 뒤의 요소들을 모두 선택한 것을 확인할 수 있습니다.이 외에 간단한 형태로 특정한 튜플이 비어 있는지 검사하기 위해서, 아래와 같은 IsEmpty<T> 타입을 정의할 수도 있습니다.type IsEmpty<T extends any[]> = T extends [] ? true : false;\n\n// type B = true\ntype B = IsEmpty<[]>;\n\n// type C = false\ntype C = IsEmpty<[number, string]>;TypeScript PlaygroundConditional Type에 대해 더 궁금하신 분은 TypeScript 공식 문서를 참고하시기 바랍니다.이제 Conditional Type과 Template Literal Type을 함께 사용했을 때 어떤 결과를 얻을 수 있는지 살펴봅시다.초급 예시 1: 간단한 추론type InOrOut<T> = T extends `fade${infer R}` ? R : never;\n\n// type I = \"In\"\ntype I = InOrOut<\"fadeIn\">;\n// type O = \"Out\"\ntype O = InOrOut<\"fadeOut\">;가장 간단한 예시로, 'fadeIn' | 'fadeOut' 과 같은 타입에서 앞의 fade 접두사를 버리고 'In' | 'Out' 만 가져오고 싶은 상황을 생각해봅시다.Promise<number> 에서 number 를 가져오는 것과 유사하게, Conditional Type을 이용하여 접두사를 제외할 수 있습니다.중급 예시 1: 문자열에서 공백 없애기위의 예시를 응용하면 문자열의 공백을 없애는 타입을 정의할 수 있습니다. 예를 들어, 아래와 같이 오른쪽의 공백을 모두 제거한 타입을 만들 수 있습니다.// type T = \"Toss\"\ntype T = TrimRight<\"Toss      \">;TrimRight<T> 타입은 재귀적 타입 선언을 활용합니다.type TrimRight<T extends string> =\n  T extends `${infer R} `\n    ? TrimRight<R>\n    : T;TypeScript Playground위 코드를 살펴보시면, infer R 문 뒤에 하나의 공백이 있는 것을 확인하실 수 있습니다.즉, T 타입의 오른쪽에 공백이 하나 있다면, 공백을 하나 빠뜨린 것을 R 타입으로 추론하고, 다시 TrimRight<R> 을 호출합니다.만약 공백이 더 이상 존재하지 않는다면, 원래 주어진 타입 그대로를 반환합니다.TypeScript에는 if 문이 존재하지 않지만, 만약 존재한다고 가정했을 때 아래와 같이 작성해볼 수 있습니다.type TrimRight<T extends string> =\n  if (T extends `${infer R} `) {\n    return TrimRight<R>;\n  } else {\n    return T;\n  }보다 재귀적인 구조를 잘 확인할 수 있습니다.중급 예시 2: 점으로 연결된 문자열 Split하기재귀적 타입 정의를 활용하면 'foo.bar.baz' 와 같은 타입을 ['foo', 'bar', 'baz'] 로 나누는 타입을 정의할 수 있습니다.type Split<S extends string> =\n  S extends `${infer T}.${infer U}`\n    ? [T, ...Split<U>]\n    : [S];\n\n// type S = [\"foo\", \"bar\", \"baz\"];\ntype S = Split<\"foo.bar.baz\">;TypeScript Playground주어진 S 타입에서 첫번째 점(.) 을 찾고, 그 앞 부분을 T, 뒷 부분을 U 로 추론합니다. 이후 이를 [T, ...Split<U>]와 같이 재귀적으로 하나씩 값을 이어 나가면서 원하는 결과 타입을 만들어 나갑니다.이 경우에도 if 문이 있다는 가정 하에 pseudo-code로 정리해볼 수 있습니다.type Split<S extends string> =\n  if (S extends `${infer T}.${infer U}`) {\n    return [T, ...Split<infer U>];\n  } else {\n    return [S];\n  }고급 예시: lodash.set() 함수 타입 추론하기lodash.set()는 아래와 같이 문자열로 된 접근자를 이용하여 객체의 깊은 프로퍼티까지 수정할 수 있는 함수입니다.const someObject = {\n  toss: {\n    core: {\n      client: {\n        platform: \"foo\"\n      }\n    }\n  }\n};\n\n// OK!\nlodashSet(someObject, \"toss.core.client\", { platform: 'bar' });\n\n// Error: 'bar' is not assignable to type '{ platform: string }';\nlodashSet(someObject, 'toss.core.client', 'bar');Template Literal Type이 있기 전, 이런 함수는 타입 안전하게 사용할 수 없어 세 번째 인자를 any 로 규정해야 했습니다. 그러나 위에서 살펴본 타입 정의를 조합하면 lodash.set() 를 더욱 안전하게 타이핑할 수 있습니다. 💯lodash.set() 함수를 정확하게 타이핑하기 위해서는 아래의 ValueOf<T, P> 타입이 필요합니다. ValueOf<T, P> 타입은 객체 T 와 접근 경로 P가 주어졌을 때, T 를 P 경로로 순서대로 접근했을 때 결과로 나오는 타입을 나타냅니다.interface Foo {\n  foo: {\n    bar: {\n      baz: string;\n    }\n  }\n}\n\n// type A = { bar: { baz: string } };\ntype A = ValueOf<Foo, ['foo']>;\n\n// type B = { baz: string };\ntype B = ValueOf<Foo, ['foo', 'bar']>;\n\n// type C = string;\ntype C = ValueOf<Foo, ['foo', 'bar', 'baz']>;만약에 위와 같은 ValueOf<T, P> 이 있다면, 위에서 만들었던 Split<S> 과 조합하여 쉽게 lodash.set() 함수에 타입을 부여할 수 있을 것입니다.function lodashSet<Type, Path>(\n  obj: Type,\n  path: Path,\n  value: ValueOf<Type, Split<Path>>\n): void;이제 ValueOf<T, P> 타입을 만들어봅시다. if 문과 내부 타입 선언이 있는 pseudo-code로 나타낸다면, 아래와 같이 코드를 작성할 수 있습니다.type ValueOf<Type, Paths> =\n  type Head = Paths[0];\n  type Tail = TailOf<Paths>;\n\n  if (/* Tail의 길이가 0이다 */) {\n    return Type[Head];\n  } else {\n    return ValueOf<Type[Head], Tail>;\n  }ValueOf<T, P> 타입이 그렇게 동작한다면, 위의 Foo 예시에서는 아래와 같이 차례대로 값이 계산될 것입니다.ValueOf<Foo, ['foo', 'bar']>\n== ValueOf<Foo['foo'], ['bar']>\n== ValueOf<Foo['foo']['bar'], []>\n== Foo['foo']['bar']작성했던 의사 코드를 유효한 TypeScript 코드로 나타내면 다음과 같습니다.type ValueOf<Type, Paths extends any[]> =\n  /*\n   * IsEmpty<TailOf<Paths>>가 참이면\n   * == TailOf<Paths>가 빈 Tuple이면\n   */\n  IsEmpty<TailOf<Paths>> extends true\n    ? Type[HeadOf<Paths>]\n    : ValueOf<Type[HeadOf<Paths>], TailOf<Paths>>;위 내용을 모두 조합하면 lodash.set()을 안전하게 다룰 수 있는데요. 실제로 동작하는 방식을 TypeScript Playground에서 확인해보실 수 있습니다. 😉Template Literal Type의 응용위에서 살펴본 바와 같이, Template Literal Type을 Conditional Type과 사용하면 더욱 많은 코드를 안전하게 사용할 수 있습니다. awesome-template-literal-types 레포지토리에는 상상력을 자극하는 Template Literal Type의 사용 예시들이 모여 있습니다.대표적으로 화제가 되었던 예시들에 대한 링크를 남기고 글을 맺습니다.1. TypeScript로 JSON 파서 만들기// type Json = { key1: ['value1', null]; key2: 'value2' };\ntype Json = ParseJson<'{ \"key1\": [\"value1\", null], \"key2\": \"value2\" }'>;코드와 같이 JSON 문자열을 바로 TypeScript 타입으로 옮길 수 있다는 Proof-of-concept로 화제가 되었습니다.2. document.querySelector를 타입 안전하게 사용하기const a = querySelector('div.banner > a.call-to-action'); //-> HTMLAnchorElement\nconst b = querySelector('input, div'); //-> HTMLInputElement | HTMLDivElement\nconst c = querySelector('circle[cx=\"150\"]') //-> SVGCircleElement\nconst d = querySelector('button#buy-now'); //-> HTMLButtonElement\nconst e = querySelector('section p:first-of-type'); //-> HTMLParagraphElementa 태그를 선택했을 때 결괏값이 HTMLAnchorElement가 되는 것을 확인하실 수 있습니다.3. Express의 Route Parameter로부터 타입 추론하기Express에서 사용하는 경로 문자열에서 Route Parameter의 타입을 추론할 수 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "6": "개발자의 애질리티강병훈ㆍHead of Technology2021. 10. 14이 글은 토스페이먼츠에 입사하신, 혹은 입사를 고려 중인 개발자분들을 위해 작성된 글입니다.애자일하게 일하기애자일하게 일한다는 것은 어떠한 의미일까요? 한 시간을 일하면 한 시간 만큼의 가치를 만들어 내는 방식이 아닐까 합니다. 예를 들어, 동작하는 함수를 구현하거나 난해한 개념을 이해하는 식으로요. Big up-front 설계과거에는 프로젝트 진행 초기부터 분석과 설계에 많은 시간을 투자했습니다. 전체 프로젝트의 ⅓ 이상이 분석과 설계일 정도로요. 문제는 프로젝트 기간의 ½ 시점에 요구사항이 크게 변한다면 결과물 하나 없이 다시 새로 시작해야 한다는 점이었습니다. 세상의 변화속도가 엄청나게 빨라진 지금, 초기 설계 비용이 큰 big up-front 설계가 항상 잘 들어맞지 않는다는 사실을 이제는 대부분의 사람이 알게 되었습니다.애자일 개발 방법론애자일 개발 방법론에서는 애초에 크게 설계하지 않습니다. 간단한 사용 시나리오를 작성하고 그것의 구현을 목표로 하는 짧은 주기를 가집니다. 거대한 아키텍처를 설계하지 않고 이번 주기(이터레이션(Iteration) 혹은 스프린트(Sprint))에서 달성할 수 있는 만큼의 설계를 추구합니다. (앞으로는 주기가 아닌 스프린트라고 표현하겠습니다.)예를 들어 카드 승인이 100가지의 하위 기능으로 구분할 수 있다면, 이번 스프린트의 목표를 그중에 3가지로 결정할 수 있습니다. 나머지 기능을 함께 확인하고 싶으면 어떻게 하냐구요? 애초에 3가지 기능 외에는 개발하지 않는다는 목표로 일을 진행하기 때문에 스프린트 목표 달성 여부를 확인할 때에도 오직 이 3가지 기능만 확인합니다. 이번 스프린트를 성공적으로 달성했다면 팀은 최소한 3개의 작은 기능을 획득할 수 있게 됩니다. 개발자들은 요구사항 전체가 아닌 이번 스프린트에 필요한 요구사항에 더 집중할 수 있게 되고, 만약 스프린트가 성공적으로 진행되었다면 3개의 기능을 제공하는 소프트웨어를 가지게 됩니다. 팀은 점진적으로 비즈니스에 대한 학습을 진행하면서 작지만 동작하는 일부 기능을 확실히 정복해 갈 수 있게 되는 것이죠. 이런 방식으로 팀은 매 스프린트 목표에 맞추어 성장하고, 그에 맞추어 제품도 계속 성장해 나가게 됩니다. 나아가 다음 스프린트에 전혀 다른 기능을 개발하더라도 충분히 다시 쓸 수 있는 동작하는 코드를 확보하게 됩니다. 이러한 점에서 애자일 개발 방법론은 투자한 만큼의 가치를 만드는 방식이라고 볼 수 있습니다.그림: 애자일 프로젝트에서의 기능과 시간의 관계그림: big up-front 설계가 적용된 프로젝트에서의 기능과 시간의 관계애자일 프로젝트는 시간에 비례하여 기능의 수가 증가하고, 비-애자일 프로젝트에서는 상대적으로 후반부에 기능이 집중적으로 증가함을 표현해 봤습니다.품질과 생산성어떤 코드를 보고 최고의 품질인지 판단하는 것은 매우 어려운 일입니다. 피카소조차도 더 탁월한 화가를 만난다면 자신의 작품에 몇 점을 매겨야 할지 애매할 수 있습니다.평범한 화가가 심혈을 기울인다고 하여 피카소 만큼 훌륭한 그림을 그릴 확률 역시 극히 낮을 것입니다. 이러한 논리는 개발자에게도 그대로 적용 가능합니다. 개인의 품질 역량을 10이라고 가정했을 때 평균적으로 8정도의 품질을 보여줄 확률이 높으며, 상당한 노력을 투입해야 10의 품질을 만들어 낸다고 생각할 수 있습니다. 따라서 11의 품질을 추구하게 된다면 생산성이 극적으로 저하될 수 있습니다. 기존의 코드를 개선하여 품질을 높이고자 한다면 어느 정도의 품질 개선을 목표로 해야 할까요? 만약 전체 코드를 복제한다면 품질 개선율 0%, 공정률 100%를 즉시 달성할 수 있습니다. 만약 품질 개선율 1%, 생산 공정률 100%를 달성하고자 한다면 전체 코드의 1%를 개선하는 만큼의 시간을 더 사용해야 합니다.품질을 더 개선하고 싶은데 스스로 해내기가 쉽지 않다고 판단된다면 다양한 주변 환경(동료, 메이트, 멘토, 팀, 단위 테스트 등)을 활용하여 성장 환경을 만들어 가는 것이 중요한 것 같습니다. 특히 토스페이먼츠에는 기술 논의를 즐기는 기술 덕후들이 꽤 많습니다. 내 주변이 나 때문에 활기 넘치게 만들어 보세요.핵심은 코드 리딩의 생산성대부분의 회사에는 다른 개발자들이 생산한 코드가 항상 산적해 있습니다. 어떤 기능을 개선하고 싶다면 다른 사람이 작성한 코드를 읽어야 하죠. 그래서 보통은 코드를 읽는 시간이 작성하는 시간보다 훨씬 깁니다. 따라서 읽기 좋은 코드를 만드는 것은 개발자의 삶에 굉장히 중요합니다.기존 코드를 읽는 것에 과도한 시간을 써야 한다면 기능 개선을 위한 준비 작업에만 상당한 시간을 소비하게 되어 생산성이 떨어지게 됩니다.읽기 좋은 코드를 만들어서 코드 리딩의 생산성을 향상시키는 것이 중요한 이유입니다.가능하다면 코드를 읽을 때 리팩토링 기술(Rename Method, Extract Method 등)을 활용하는 것이 좋습니다. 이러한 리팩토링을 Michael Feathers는 ‘탐색적 리팩토링(Exploratory Refactoring)’ 이라고 부르며, 이 과정에서 수정된 코드가 최종적으로 코드 저장소에 반영되지 않는다고 하더라도 충분히 가치있는 일입니다. 제가 느끼기에 Exploratory Refactoring은 정말로 효과적인 학습 프로세스이기 때문입니다.Exploratory Refactoring을 수행하게 되면 코드를 읽은 즉시 나의 해설을 표시하기 때문에 굉장히 적극적으로 코드 리딩이 되며, 코드 리딩의 주도권을 자연스럽게 리더(reader)가 가져가게 됩니다. 책을 읽었는데도 이해가 안되서 다시 읽어야 하는 것과 같은 수동적인 상태에서 벗어날 수 있게 됩니다. 따라서 Exploratory Refactoring은 탁월한 개발자가 탁월해지게 만들어주는 진정한 OP 기술입니다.리팩토링의 가치개발자들이 리팩토링의 욕구를 강하게 느낄 때는 보통 유지보수 비용이 과도한 경우입니다. 유지보수 비용이 높은 이유는 기능을 수정해야 하는데 어느 코드를 수정해야 할지, 몇 줄의 코드를 바꿔야 할지, 변경을 했다면 올바르게 변경했는지 등을 파악하기 어렵기 때문입니다.따라서 요구사항에 대응하는 코드가 어디인지 명확하게 찾아낼 수 있고, 수정한 부분의 동작이 정확한지 파악하기 쉽다면 유지보수 비용을 낮출 수 있습니다. 리팩토링의 목적은 이러한 부분을 달성하는 데 있습니다.따라서 리팩토링이 잘 수행되었다면 새로운 개발자(혹은 미래의 자신)가 기능 변경 요청을 받았을 때 아래의 3가지를 쉽게 해낼 수 있습니다.코드 위치 파악코드 수정 기능 테스트리팩토링으로 잘 설계된 코드는 이러한 핵심 과업을 쉽게 이행하는데 큰 도움이 됩니다.결론애자일 기법은 요구사항이라는 큰 덩어리를 작지만 동작하는 작은 기능으로 나누고 매 스프린트 마다 목표한 바를 착실히 정복해가는 방식이라고 할 수 있습니다.이 때 안전하게 기능을 수정 혹은 추가하기 위해서는 3개의 핵심 과업을 잘 수행해야 하는데요,코드로부터 도메인 파악하기(a.k.a. 코드 고고학)수정에 필요한 코드 파악하기수정된 코드 쉽게 검증하기토스페이먼츠에서는 애자일하게 일하는 방법을 동료들과 함께 일 하면서 쉽게 터득할 수 있는 좋은 문화와 프로세스를 만들기 위해 서툰 실험을 계속하고 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "7": "조금만 신경써서 초기 렌더링 빠르게 하기 (feat. JAM Stack)한재엽ㆍFrontend Developer2022. 2. 9들어가면서SPA(Single Page Application) 구조로 웹 프론트엔드 애플리케이션이 개발되면서 초기 렌더링 속도는 프런트엔드 개발자에게 중요한 과제 중 하나가 되었습니다. 사용자 경험에 영향을 줄 수 있는 가장 큰 요소 중 하나가 바로 속도이기 때문입니다. 이번 개선은 Web Vitals 지표를 중심으로 측정했습니다.주어진 과제들과제 1. 번들 사이즈애플리케이션에 기능이 추가되면서 번들 사이즈가 커졌고 이로 인해 초기 렌더링이 늦어지는 문제가 발생하게 됩니다. 네트워크 비용을 줄이기 위해 Webpack으로 번들링했던 소스코드를 다시 적절한 단위로 코드 스플리팅(Code Splitting)을 하기도 하고 사용되지 않는 코드, 불필요한 코드들을 덜어내기 위한 트리 세이킹(Tree Shaking)을 위한 작업을 하기도 합니다.→ [SLASH 21] 이한 – JavaScript Bundle Diet이러한 노력을 하더라도 개선할 수 있는 부분엔 한계가 존재했습니다. 초기에 렌더링되는 index.html 자체가 비어있는 문서(Document)이기 때문에 스크립트가 실행되어 실제로 렌더링이 되기까지의 시간이 존재하기 때문입니다.과제 2. 렌더링 시점그렇다면 이제 렌더링 시점을 어떻게 앞당길 것인가에 대한 문제를 해결해야 됩니다. 사용자가 tosspayments.com 에 접근했을 때, 사용자가 최종적으로 볼 수 있는 화면을 서버에서 미리 그리고 그 화면을 브라우저에 전달해주면 초기 렌더링 시점이 앞당겨지지 않을까요?물론 인터랙션이 가능해지기 까지는 하이드레이트(Hydrate) 시간이 필요하지만, 사용자 입장에서는 우선 화면이 보여지는 것이 중요합니다. 초기에 렌더링 되는 index.html이 비어있는 문서가 아니라 무언가 렌더링되어 있는 문서라면 LCP(Largest Contentful Paint) 시점을 크게 앞당길 수 있을 것입니다.JAM Stack서론이 길었는데요, 토스페이먼츠에서 만들고 있는 일부 제품에서 SSR(Server Side Rendering)없이 초기 렌더링 속도를 개선해 보았습니다. 어떤 결과를 낳았으며 어떻게 개선했는지 이야기하고자 합니다.JAM Stack이란 JavaScript와 Markup에 해당하는 HTML, CSS 정적 리소스들을 활용하여 웹 애플리케이션을 구성하는 스택을 말합니다. 그리고 이 정적 리소스들을 CDN(Content Delivery Network)에 배포하여 서버 관리를 최소화 할 수 있습니다.토스페이먼츠에서는 AWS S3, CloundFront, Lambda@edge 를 사용하여 인프라를 운영하고 있습니다.→ JAM Stack에 대해 더 알아보기SSGStatic Site Generation이라는 개념인데요, 앱을 빌드하는 시점에 미리 그려두고 이를 서빙(serving)하는 방식을 말합니다. JAM Stack에서 정적 리소스를 생성하는 용도로 사용합니다.컴파일 단계에서 미리 그릴 수 있는 부분을 최대한 그려서 사용자에게 도달하는 최초 index.html 파일이 비어있지 않도록 합니다.미리 그릴 수 있다는 것은 말 그대로 컴파일 단계에서 리액트 코드를 읽어 HTML로 렌더링 할 수 있는 부분을 말합니다. 정적인 부분을 포함하여 인증이 필요하지 않은 데이터 또한 서버로부터 가져와 미리 그릴 수 있습니다.결과 (지표)구체적인 내용을 다루기에 앞서 어느 정도의 개선이 있었는지 먼저 소개하고자 합니다. 기대한 것 이상의 결과가 나와서 매우 즐거웠던 경험이었습니다.토스페이먼츠 상점관리자 초기 로딩 화면Lighthouse 지표before개선 하기 전 지표After개선 후 지표구체적인 지표 측정Chrome Browser에서 FP(First Paint)부터 LCP(Largest Contentful Paint)까지 걸린 시간을 측정해봤습니다.before(FP → LCP: 484ms)after(FP → LCP: 0ms)Large Contents에 해당하는 것을 일단 그려버리고 시작하니 0ms입니다.최대한 그릴 수 있는 영역을 미리 그림으로써 사용자는 흰 화면을 마주하지 않고 바로 제품을 만나는 것과 같은 느낌을 받을 수 있습니다.How?Next.js토스페이먼츠의 프런트엔드 애플리케이션은 Next.js 라는 프레임워크를 사용하고 있습니다. Next.js는 서버 사이드 렌더링은 물론이고 앞서 설명드린 Static Site Generate 또한 지원합니다. (Next.js Automiatic Static Optimization)Suspense우선 토스 대부분의 프런트엔드 애플리케이션 제품은 React의 Suspense를 통해 비동기를 제어하고 있으며 토스페이먼츠 제품 또한 예외가 아니었습니다. 이와 동시에 에러 핸들링 또한 ErrorBoundary를 통해 제어하면서 비동기 상황을 제어하고 있습니다.→ [SLASH 21] 박서진 – 프론트엔드 웹 서비스에서 우아하게 비동기 처리하기→ 선언적으로 에러 상황 제어하기이 Suspense를 Next.js와 함께 사용하기 위해선 약간의 추가 작업이 필요한데요, 앞서 설명드렸다시피 Next.js는 서버사이드 렌더링 또한 지원하는 프레임워크이기 때문에 Isomophic한 코드를 작성해야 합니다. 아쉽게도 Suspense는 서버사이드 렌더링이 지원되지 않습니다. (글을 작성하는 시점에 알파로 공개되어 있는 React 18에서 개선될 예정)그래서 다음과 같이 Suspense를 한번 감싸서 사용해줄 수 있습니다.import { useState, useEffect, Suspense as ReactSuspense } from 'react';\n\nexport function Suspense({ fallback, children }: ComponentProps<typeof ReactSuspense>) {\n  const [mounted, setMounted] = useState(false);\n\n  useEffect(() => {\n    setMounted(true);\n  }, []);\n\n  if (mounted) {\n    return <ReactSuspense fallback={fallback}>{children}</ReactSuspense>;\n  }\n  return <>{fallback}</>\n\n이렇게 수정된 Suspense로 제어하고 있는 컴포넌트를 SSG로 빌드하게 되면 fallback이 렌더링됩니다.다음과 같은 코드일 경우, SSG 시점엔 <Loading /> 컴포넌트만 그려지게 됩니다.function UserPage() {\n  return (\n    <Suspense fallback={<Loading />}> // <- Render!\n      <UserProfile />\n      <UserDetailInfo />\n    </Suspense>\n\n즉, 빌드 단계에서 SSG로 미리 그려주고자 했던 UserPage에는 Loading 컴포넌트만 렌더링 될 뿐, UserProfile , UserDetailInfo 컴포넌트는 전혀 렌더링 되지 않습니다. 미리 렌더링하는 것에 대한 이점을 전혀 얻지 못하게 되는 것입니다.번들 사이즈를 아무리 줄여도 사용자는 일단 로딩만 돌고 있는 흰 화면을 마주하게 되는 것입니다.컴포넌트 배치 되돌아보기우선 Suspense가 정말 필요한 컴포넌트인지, 레이아웃 영역인지 되돌아 볼 필요가 있습니다.정말 Suspense가 필요한 영역이라면 fallback 컴포넌트를 정의해줄 때 로딩 컴포넌트만 정의해주지 않는다면 어떨까요? API 응답이 돌아오고 결국 그려질 컴포넌트와 응답이 오지 않았을 경우 보여줄 이 fallback 컴포넌트를 최대한 비슷하게 구성해주는 겁니다. 그렇다면 컴파일 시점에 그릴 수 있는 영역이 늘어나지 않을까요?즉, 위와 같이 Loading 컴포넌트만 렌더링하지 않으려면 API 응답이 돌아왔을 때 그려져야 할 컴포넌트와 응답이 아직 돌아오지 않았을 때 보여줄 컴포넌트 두 벌이 최대한 비슷하게 구성되어 있어야 합니다.컴포넌트와 API를 가깝게처음 보셨던 화면에서는 총 16개의 API call이 존재합니다. 너무나 당연하게도 이 모든 API 응답은 제각각으로 올 것이고 모든 응답이 돌아오기를 기다렸다가 그려주는 것은 정말 낭비입니다.각각의 API들을 따로 격리시켜 서로의 렌더링을 block하지 않도록 합니다.데이터가 필요한 곳에서 가장 가까운 곳에서 API를 호출합니다. client caching이 이젠 너무나도 자연스럽기 때문에 이를 최대한 활용해줍니다.UserPage 컴포넌트의 구조를 다음과 같이 변경해 볼 수 있습니다.function UserPage() {\n  return (\n    <Layout>\n      <h1>사용자 정보</h1>\n      <dl>\n        <dt>이름</dt>\n        <Suspense fallback={<dd>Loading</dd>}>\n          <UserName />\n        </Suspense>\n      </dl>\n      <h2>사용자 상세 정보</h2>\n      <Suspense fallback={<div>Loading</div>}>\n        <UserDetailInfo />\n      </Suspense>\n    </Layout>\n  )\n}페이지 컴포넌트(UserPage) 전체를 감싸고 있던 Suspense 컴포넌트가 사라지고 비동기로 처리되는 영역이 좁게 정의가 되었습니다. 또한 비동기 처리 과정 중 노출되는 컴포넌트의 모습도 원래 보여질 컴포넌트와 비슷하게 정의해줬습니다.디자인이 필요한 영역이 늘었어요. API를 호출하고 기다리는 순간에 대해서도 디자인이 필요해요. 그대로 컴포넌트도 만들어줘야 하고 그만큼 손도 많이 갑니다. 하지만 서버 관리하는 비용보다 더 신경써줄 필요는 없다고 생각합니다.조금만 신경쓰더라도 많은 개선을 볼 수 있는 방법입니다.더 나아가기지난 Next.js Conf에서 공식적으로 React의 Server Component를 사용한 렌더링 방식이 공개되었습니다. React 18도 알파 단계이니 프런트엔드 애플리케이션을 개발하면서 성능 상 이점을 많이 챙길 수 있는 환경으로 뒤바꿈 될 것 같습니다. ISR 방식과 컴포넌트 단위의 캐싱이 적용되어 웹이 더 빨라질 수 있을 것이라 기대합니다.마무리초기 로딩 속도가 중요한 것은 비즈니스에도 영향을 미치기 때문입니다. web.dev에서 초기 로딩 속도를 개선하여 성과가 개선된 사례가 소개된 바 있습니다.(https://web.dev/vitals-business-impact/)당장에 SSR 도입이 쉽지 않은 상황이라면 SSG를 통한 초기 렌더링을 최적화 할 수 있습니다.토스 팀은 계속해서 초기 로딩 속도를 계속해서 개선 중입니다. 곧 있을 SLASH22에서는 ‘매달, 유저가 기다리는 시간을 2.3년씩 아낄 수 있는 초기 렌더링 개선기 (feat. SSR)’라는 제목으로 초기 렌더링 개선 경험을 공유할 예정이니 많은 관심 부탁드립니다.👉 토스페이먼츠 프런트엔드 챕터에 대해 더 알아보기감사합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "8": "Kotlin으로 DSL 만들기: 반복적이고 지루한 REST Docs 벗어나기한규주ㆍServer Developer2022. 4. 11REST Docs 테스트 코드량을 70% 줄여주는 DSL 개발기읽는 데 걸리는 시간: 6분DSLDomain Specific Languages(DSL)은 코드의 내부 로직을 숨기고 재사용성을 올려줍니다. 어떤 경우는 비 개발자가 사용하도록 고안되는 경우도 있어서, 일반적인 프로그래밍 언어보다 훨씬 쉬운 사용성을 가집니다. 핵심은 해당 도메인을 아는 사람이면 누구나 쉽게 해당 도메인을 제어할 수 있도록 DSL을 제공하는것이 목적이며, 그렇기 때문에 프로그래밍 언어가 아닌 일반적인 언어에 가깝도록 호출 방식을 설계합니다. 때문에 DSL 호출 내부에서 어떤 로직이 작동하는지는 사용자가 알도록 할 필요가 없으며 훨씬 더 간결하고 빠르게 코드를 작성할 수 있습니다.Spring REST Docs, 더 쉽고 간결하게 쓸 수 없을까토스페이먼츠에서는 API docs를 REST Docs를 사용해서 작성할 수 있도록 권장하고 있습니다. docs를 작성하는 행위 자체에서부터 API를 통합테스트할 수 있다는 점이 매력적이며, 인터페이스의 의도치 않은 변경을 감지할 수 있다는 장점이 있습니다. 문제는 독스를 작성할 때마다 테스트 코드를 작성해줘야 하기 때문에 Swagger 보다 더 번거롭게 작업하게 된다는 문제가 있습니다.이 글에서는 DSL을 통해서 API 인터페이스의 안정성과 개발자의 생산성을 모두 가져갈 수 있는 방법을 소개합니다.REST Docs DSL먼저 기존의 작성법(AS-IS)과 DSL을 이용한 작성법(TO-BE)을 비교해보겠습니다.AS-IS.TO-BE.한 눈에 봐도 간결해보이지 않나요? AS-IS에서 볼 수 있듯, 기존의 작성법은 여러 문제가 있습니다.반복적인 코드 호출이 많음. 기존 작성법으로 작성할 때마다 생산성 저하를 느꼈습니다. API를 만드는 시간만큼이나 docs를 생성하는 시간이 걸린다니, 이것 참 비효율이지 않나요?코드가 장황하여 읽히지 않음. 인터페이스에 변화가 생기면 REST Docs 테스트 코드를 수정해야 하는데, 어떤 코드를 수정해야 하는지 빠르게 찾기가 어려웠습니다. 즉 해당 코드가 무엇을 수행하는지 한번에 읽기가 힘들고, 이 코드 수행 결과가 어떤 docs를 만들어낼지 단번에 떠올리기 어렵다는 단점이 있었습니다.첫 번째 단점은 기존의 다른 코드로부터 복붙으로 시간을 좀 줄여낼 수는 있었지만, 두 번째 단점은 참 신경 쓰였습니다. 저는 JSON과 같은 간결한 구조로부터 docs를 테스트하는 코드가 만들어지길 원했습니다.Kotlin으로 DSL 만들기다행히도 Kotlin은 여러 함수 선언 방식이 존재하여서, 이런 문제를 풀기에 매우 좋습니다. Kotlin의 테스트 코드 라이브러리인 Kotest와 MockK이 대표적인 사례라고 생각합니다.infix 함수Infix Notation (kotlinlang.org)잘 만들어진 DSL은 인간의 자연어를 사용하듯이 자연스럽게 쓰고 읽힐 수 있어야 한다고 생각합니다. Kotlin의 infix notation은 이 목표를 달성하기에 최적의 도구입니다.\"data.businessId\" type NUMBER는 \"data.businessId\".type(NUMBER)와 동일한 효과를 낳습니다.infix fun String.type(                    // (1)\n    docsFieldType: DocsFieldType\n): Field {                                // (2)\n    ...                                   // (3)\n}(1):infix notation으로 해당 함수를 선언해줍니다.type이라는 함수는 String을 receiver로 받는 함수입니다.파라미터는 docsFieldType 하나만 받습니다 (DocsFieldType는 아래에서 서술합니다.)(2): 원래 restdocs가 제공하던 FieldDescriptor를 유연하게 다루기 위해 Field라는 Wrapper 클래스를 정의합니다.(3): 원래의 RestDocs를 만들던 동작을 수행합니다infix 함수를 사용할때는 제한사항이 있습니다.호출할때는 receiver와 parameter가 명시적으로 있어야 함 (this로 암시적인 전달 불가능)parameter는 하나여야 함 (default value도 지정할 수 없음)그래야만 \"data\" type OBJECT 처럼 간결한 구조를 만들어 낼 수 있기 때문입니다.DocsFieldTypeREST Docs에서는 응답, 요청 필드의 type을 JsonFieldType으로서 구분합니다.여기에 저는 자주 사용하는 format인 Date, DateTime을 쉽게 정의할 방법을 찾고 싶었고, enum class도 간단히 전달하여 어떤 필드가 사용될 수 있는지 docs에 쉽게 표기하고 싶었습니다. date, datetime, enum은 모두 JsonFieldType.STRING이지만 format과 sample이 다르게 표시될 필요가 있는 특이 케이스이기 때문입니다.이런 식으로 정의한다면 아래 예시와 같이 간단하게 Field를 생성해내면서 DocsFieldType을 정의해낼 수 있습니다.\"data\" type OBJECT\n\"id\" type NUMBER\n\"createdAt\" type DATETIMEDocsFieldType - enum다만 enum을 정의하고 싶을때는 조금 디테일이 필요합니다.\"companyType\" type STRING example CompanyType::class로도 선언할 수는 있지만 매번 example을 호출해주는 건 조금 귀찮습니다. 어차피 enum이 string이라는건 누구나 다 아는 사실인데 두 함수 호출을 나눠야 할까요?\"companyType\" type ENUM(CompanyType::class)훨씬 간결해졌습니다.다음과 같이 DocsFieldType을 확장한 sealedSubclass를 만든다면 위와 같은 dsl 작성이 가능합니다.data class ENUM<T : Enum<T>>(val enums: Collection<T>) : DocsFieldType(JsonFieldType.STRING) {\n  constructor(clazz: KClass<T>) : this(clazz.java.enumConstants.asList())   // (1)\n}(1): secondary constructor 덕분에 모든 enum값이 아니라 특정 조건에 맞는 enum 값을 collection으로 넘길수도 있습니다.ex) 개인사업자에 해당하는 companyType만 해당 필드에 존재할 수 있을 때 \"individualCompanyType\" type ENUM(CompanyType.values().filter { it.isIndividual() })이로써 type infix 함수는 아래와 같이 완성할 수 있습니다.infix fun String.type(docsFieldType: DocsFieldType): Field {\n    val field = createField(this, docsFieldType.type)\n    when (docsFieldType) {\n        is DATE -> field formattedAs RestDocsUtils.DATE_FORMAT\n        is DATETIME -> field formattedAs RestDocsUtils.DATETIME_FORMAT\n        else -> {}\n    }\n    return field\n}\n\ninfix fun <T : Enum<T>> String.type(enumFieldType: ENUM<T>): Field {\n    val field = createField(this, JsonFieldType.STRING, false)\n    field.format = EnumFormattingUtils.enumFormat(enumFieldType.enums)\n    return field\n}\n\nprivate fun createField(value: String, type: JsonFieldType, optional: Boolean): Field {\n    val descriptor = PayloadDocumentation.fieldWithPath(value)\n        .type(type)\n        .attributes(RestDocsUtils.emptySample(), RestDocsUtils.emptyFormat(), RestDocsUtils.emptyDefaultValue())\n        .description(\"\")\n\n    if (optional) descriptor.optional()\n\n    return Field(descriptor)\n}\n\nField 클래스에서 DSL 확장하기이제 좀 더 욕심을 내봅시다. 위 예시처럼 얼마든지 함수 호출을 chaining할 수 있습니다.어떤가요? 괄호로 계속 호출하는 것보다 좀 더 직관적이지 않나요?type이라는 infix function이 Field를 반환할 수 있도록 했으니, Field에서 더 많은 DSL을 호출하도록 확장할 수 있게 되었습니다.open class Field(\n    val descriptor: FieldDescriptor,\n) {\n    val isIgnored: Boolean = descriptor.isIgnored\n    val isOptional: Boolean = descriptor.isOptional\n\n    protected open var default: String\n        get() = descriptor.attributes.getOrDefault(RestDocsAttributeKeys.KEY_DEFAULT_VALUE, \"\") as String\n        set(value) {\n            descriptor.attributes(RestDocsUtils.defaultValue(value))\n        }\n\n    protected open var format: String\n        get() = descriptor.attributes.getOrDefault(RestDocsAttributeKeys.KEY_FORMAT, \"\") as String\n        set(value) {\n            descriptor.attributes(RestDocsUtils.customFormat(value))\n        }\n\n    protected open var sample: String\n        get() = descriptor.attributes.getOrDefault(RestDocsAttributeKeys.KEY_SAMPLE, \"\") as String\n        set(value) {\n            descriptor.attributes(RestDocsUtils.customSample(value))\n        }\n\n  \topen infix fun means(value: String): Field {\n        return description(value)\n    }\n\n    open infix fun attributes(block: Field.() -> Unit): Field {\n        block()\n        return this\n    }\n\n    open infix fun withDefaultValue(value: String): Field {\n        this.default = value\n        return this\n    }\n\n    open infix fun formattedAs(value: String): Field {\n        this.format = value\n        return this\n    }\n\n    open infix fun example(value: String): Field {\n        this.sample = value\n        return this\n    }\n\n    open infix fun isOptional(value: Boolean): Field {\n        if (value) descriptor.optional()\n        return this\n    }\n\n    open infix fun isIgnored(value: Boolean): Field {\n        if (value) descriptor.ignored()\n        return this\n    }\n}\n\n이렇게 얼마든지 코드를 확장해나갈 수 있을뿐더러, 해당 프로젝트에서 사용하는 REST Docs snippet의 attribute를 코드 상으로 좀 더 명확하게 정의할 수 있게 되었습니다.마무리이 글은 REST Docs의 반복적인 코드를 제거하고, docs의 생성이라는 본래의 목적을 달성하고자 기존 MockMvc 테스트코드 작성법에서 벗어나, REST Docs DSL을 만드는 방식으로 문제를 해결하고자 했습니다.우리가 흔히 쓰는 gradle configuration 작성 방식인 build.gradle.kts 또한 org.gradle.kotlin.dsl에서 그 선언 방식을 찾아볼 수 있고, MockK이나 Kotest에서도 다양한 방식으로 Kotlin의 장점을 최대한 끌어낸 모습을 확인할 수 있습니다.build.gradle.kts (https://github.com/gradle/kotlin-dsl-samples)MockK의 every(https://mockk.io/#dsl-examples),Kotest의 여러 Testing Styles(https://kotest.io/docs/framework/testing-styles.html)혹시나 여러분도 반복적인 작업을 일일히 복붙으로 하고 있다면 여러분의 팀만을 위한 DSL을 만들어보는 건 어떨까요?이 REST Docs DSL은 토스페이먼츠 *엔지니어링 데이에 장태영(Server Developer, taeyoung.jang@tosspayments.com)님과 함께 만들었습니다.토스페이먼츠에서는 매주 목요일에 엔지니어링 데이를 진행하고 있어요. 이 시간에는 평소 업무에 병목이 되는 문제들을 해결하거나, 인프라를 개선하는 등의 작업을 진행합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "9": "에러 핸들링을 다른 클래스에게 위임하기 (Kotlin 100% 활용)한규주ㆍServer Developer2022. 5. 14TL;DRResult를 이해한다면, MSA 환경에서 에러가 전파되지 않도록 막을 수 있습니다.runCatching과 Result를 사용하면 에러 핸들링을 클라이언트에게 위임할 수 있습니다.예제: 로그인 요청을 전달하는 서비스 흐름에서 에러 처리하기아래와 같은 서비스 호출 흐름이 있다고 가정해보겠습니다.Server A 입장에서는 Server B에서 발생하는 에러 처리를 해야하는 고민에 빠집니다.API를 호출하는 코드에서 API의 에러 응답에 따른 비즈니스 로직을 다르게 가져가고 싶은 경우가 있습니다. 예를 들어 위 사례에서 비밀번호가 틀리거나 이메일 주소가 틀린 경우 이 에러를 캐치해서 다른 메세지를 던지고 싶을 수 있고, 어떤 코드에서는 그 에러를 무시하고 다른 로직을 수행하고 싶을 수 있습니다.에러 처리를 API Client 단에서 하지 않고 다른 클래스에 위임을 하고 싶은 이런 경우에는 어떤 방법을 사용할 수 있을지 아래 코드 예시로 알아보겠습니다.// API client\n@FeignClient\ninternal interface LoginApi {\n  @PostMapping\n  fun login(\n    @RequestBody request: LoginRequestDto\n  ): OtherServiceResponse<LoginResponseDto>\n}\n\n@Component\nclass LoginApiClient internal constructor(\n  private val loginApi: LoginApi\n) {\n  fun login(request: LoginRequestDto): LoginResult {\n    return loginApi.login(request).result.toResult()\n  }\n}\n\n@Service\nclass LoginService(\n  private val loginApiClient: LoginApiClient\n) {\n  fun login(id: String, pw: String): LoginResult {\n    return try {\n      loginApiClient.login(LoginRequestDto(id, pw))\n    } catch {\n      // 에러 핸들링\n    }\n  }\n}\n\n이 경우에 아래와 같은 두 케이스를 해결하고 싶어집니다.이 API를 사용하는 쪽(ex. LoginService)에서 에러 핸들링을 강제하고 싶습니다.API 호출 로직마다 에러 핸들링을 다른 방식으로 가져가게 하고 싶습니다.LoginService가 아닌 다른 호출 로직에서는 에러를 다르게 처리하고 싶을 수 있습니다.위 고민을 해결할 방법이 있습니다. 바로 Result입니다.@Component\nclass LoginApiClient internal constructor(\n  private val loginApi: LoginApi\n) {\n  fun login(request: LoginRequestDto): Result<LoginResult> {\n    return runCatching {\n      loginApi.login(request).result.toResult()\n    }\n  }\n}\n\n@Service\nclass LoginService(\n  private val loginApiClient: LoginApiClient\n) {\n  fun login(id: String, pw: String): LoginResult {\n    return loginApiClient.login(LoginRequestDto(id, pw))\n      .onFailure {\n        // 에러 핸들링\n      }\n  }\n}코틀린의 runCatching💡 이미 runCatching을 잘 사용하고 있다면 넘겨도 좋습니다.위 코드를 이해하기에 앞서서 runCatching을 알아둘 필요가 있습니다. 코틀린은 물론 자바의 try ... catch를 동일하게 지원하지만 이와는 조금 다른 방법으로 에러 핸들링을 할 수도 있습니다.예제아래 요구사항이 있다고 가정합시다.LoginApiClient 호출 시 LoginException이 발생했는데,errorCode가 INVALID_PASSWORD 인 경우 예외를 발생시키지 않고 null을 반환한다.그 외 모든 에러 상황에서는 예외를 발생시킨다.try ... catch를 사용했을때try {\n  loginApiClient.login(request)\n} catch (e: LoginException) {\n  if (e.errorCode == \"INVALID_PASSWORD\") {\n    return null\n  } else {\n    throw e\n  }\n}Java에서 위와 같이 작성하는 코드를 runCatching을 사용하면 아래처럼 표현할 수 있습니다.runCatching을 사용했을 때return runCatching {\n  loginApiClient.login(request)\n}.onFailure { e ->\n  if (e.errorCode != \"INVALID_PASSWORD\") throw e\n}.getOrNull()kotlin.runCatching@InlineOnly\n@SinceKotlin(\"1.3\")\npublic inline fun <R> runCatching(block: () -> R): Result<R> {\n  return try {\n    Result.success(block())\n  } catch (e: Throwable) {\n    Result.failure(e)\n  }\n}try..catch 로직을 그대로 사용하지만 Result로 감싸서 반환하는 것을 알 수 있습니다.에러가 발생하지 않았을 때에는 Result.success 반환에러가 발생했을 때에는 Result.failure 반환Result가 뭔가요?Result가 무엇인지 알아보기 위해서 Kotlin 1.3 표준 라이브러리의 코드를 살펴봅시다.@SinceKotlin(\"1.3\")\n@JvmInline\npublic value class Result<out T> @PublishedApi internal constructor(\n  @PublishedApi\n  internal val value: Any?\n) : Serializable {\n\n  public val isSuccess: Boolean get() = value !is Failure\n\n  public val isFailure: Boolean get() = value is Failure\n\n  /* ... */\n\n  public companion object {\n    @Suppress(\"INAPPLICABLE_JVM_NAME\")\n    @InlineOnly\n    @JvmName(\"success\")\n    public inline fun <T> success(value: T): Result<T> =\n      Result(value)\n\n    @Suppress(\"INAPPLICABLE_JVM_NAME\")\n    @InlineOnly\n    @JvmName(\"failure\")\n    public inline fun <T> failure(exception: Throwable): Result<T> =\n      Result(createFailure(exception))\n  }\n\n  internal class Failure(\n    @JvmField\n    val exception: Throwable\n  ) : Serializable {\n    /* ... */\n  }\n}\n\n즉, Result의 value는성공일 경우 T를 타입으로 하는 값을 가지게 되고실패일 경우는 Failure를 wrapper class로 하는 exception을 값으로 가지게 됩니다.Result가 제공하는 함수들은 다음과 같습니다.inline fun <T> Result<T>.getOrThrow(): T\n\ninline fun <R, T : R> Result<T>.getOrElse(\n  onFailure: (exception: Throwable) -> R\n): R\n\ninline fun <R, T : R> Result<T>.getOrDefault(defaultValue: R): R\n\ninline fun <R, T> Result<T>.fold(\n  onSuccess: (value: T) -> R,\n  onFailure: (exception: Throwable) -> R\n): R\n\ninline fun <R, T> Result<T>.map(transform: (value: T) -> R): Result<R>\n\nfun <R, T> Result<T>.mapCatching(transform: (value: T) -> R): Result<R>\n\ninline fun <R, T : R> Result<T>.recover(transform: (exception: Throwable) -> R): Result<R>\n\ninline fun <T> Result<T>.onFailure(action: (exception: Throwable) -> Unit): Result<T>\n\ninline fun <T> Result<T>.onSuccess(action: (value: T) -> Unit): Result<T\n\nResult 사용 예시runCatching은 Result<T>를 반환하게 되는데, Result가 제공하는 함수를 이용해서 다양하게 활용할 수 있습니다.에러를 무시하고 null 반환val response = runCatching {\n  login()\n}.getOrNull()기본값 반환val response = runCatching {\n  login()\n}.getOrDefault(emptyList())에러 발생 시 다른 동작 수행val response = runCatching {\n  login()\n}.getOrElse { ex ->\n  logger.warn(ex) { \"에러 발생\" }\n\n  // 에러를 던지고 싶다면\n  throw ex\n}에러가 발생한 경우에만 해당 에러 객체 반환val exception = runCatching {\n  login()\n}.exceptionOrNull()\n\n// 위에서 받은 에러로 로직 수행\nwhen (exception) {\n  /* ... */\n}에러가 발생하는지 아닌지만 확인하고 싶을 때에도 유용할 수 있습니다.val isValidCredential = runCatching { tryLogin() }.exceptionOrNull() != null성공/에러 시 각각 특정 동작 수행 후 에러 던지기val response = runCatching {\n  login()\n}.onSuccess {\n  logger.info(\"성공!\")\n}.onFailure {\n  logger.info(\"실패!\")\n}.getOrThrow()runCatching으로 try .. finally 구현하기runCatching {\n  request()\n}.also {\n  doSomething()\n}.getOrThrow()Result를 사용해서 예외 처리를 다른 클래스에 위임하기runCatching을 사용하면 Result가 제공하는 다양한 함수의 편의에 기댈 수 있다는 것을 배웠습니다.Result에 대한 처리를 즉시 하지 않고 함수의 반환 값으로 반환하게 된다면, Result에 대한 핸들링을 다른 클래스에 위임할 수도 있습니다.LoginApiClient@Component\nclass LoginApiClient internal constructor(\n  private val loginApi: LoginApi\n) {\n  fun login(request: LoginRequestDto): Result<LoginResult> {\n    return runCatching {\n      loginApi.login(request).result.toResult()\n    }\n  }\n}\n\nResult를 반환하여 다른 클래스가 에러 핸들링을 하도록 위임합니다.LoginService@Service\nclass LoginService(\n  private val loginApiClient: LoginApiClient\n) {\n  fun login(id: String, pw: String): LoginResult? {\n    return loginApiClient.login(LoginRequestDto(id, pw))\n      .getOrNull()\n  }\n}\n\n에러가 발생한 경우 에러를 무시하고 기본값으로 null을 반환합니다.하지만 아래처럼 다른 컴포넌트에서는 에러를 핸들링하고 싶을 수도 있습니다.PasswordChangeService@Component\nclass PasswordChangeService(\n  private val loginApiClient: LoginApiClient,\n  private val errorStatusWriter: ErrorStatusWriter,\n  private val passwordChanger: PasswordChanger\n) {\n  fun change() {\n    loginApiClient.login(request)\n      .onFailure { exception ->\n        errorStatusWriter.write(exception)    // (1)\n      }.onSuccess { loginResult ->\n        passwordChanger.change(loginResult)   // (2)\n      }.getOrThrow()                          // (3)\n  }\n}[1] 에러가 발생한 경우 에러를 기록합니다.[2] 성공한 경우 해당 값을 받아서 다른 컴포넌트를 호출합니다.→ [1], [2]번 두 케이스는 배타적이고 동시에 일어날 수 없습니다.[3] 그리고 에러인 경우 예외를 발생시킵니다.결론정리하자면 Result(runCatching)는 다음의 용도에서 사용할 수 있습니다.외부 서비스에 의존하는 로직이라 예외 발생 가능성이 빈번한 컴포넌트해당 컴포넌트에서 에러가 발생할 수 있다는 것을 클라이언트에게 알려주고 싶을 때, 에러 핸들링을 다른 컴포넌트에 강제하고 위임하고 싶을 때try ... catch를 쓰고 싶지 않을 때재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "10": "테스트 의존성 관리로 높은 품질의 테스트 코드 유지하기양권성ㆍServer Developer2022. 6. 9테스트 코드는 애플리케이션 코드 못지 않게 높은 품질을 유지해야 합니다.낮은 품질(이해하기 어려운 코드, 여기저기 깨져있는 테스트)의 테스트는 유지보수가 어렵고 기술부채에 못지 않은 부채로 다가옵니다.그래서 테스트 코드의 높은 품질을 유지하기 위해 다양한 Builder, Helper 클래스들이 나오게 되고, 테스트 전용으로 의존성을 추가하기도 합니다. 하지만 이 또한 관리의 대상이며 제대로 관리하지 않으면 중복 코드와 얼기설기 얽힌 의존성 지옥을 맛보게 됩니다.이 포스트에서는 Gradle의 java-test-fixtures 플러그인을 사용하여 위 문제를 해결하는 방법에 대해 설명합니다.TL;DRGradle의 java-test-fixtures 플러그인을 사용하면 테스트용으로 작성한 Builder, Helper 클래스 등등을 다른 모듈과 공유할 수 있습니다.추가적으로 해당 모듈의 테스트 전용 의존성까지 전파시킬 수 있어 각 모듈마다 불필요한 테스트 전용 의존성들을 일일이 추가할 필요가 사라집니다.프로젝트 구조예제를 이해하기 쉽게 하기 위해 프로젝트 구조(멀티 모듈)를 가정하고 이야기를 진행하겠습니다.domain 모듈: 핵심 비즈니스 로직에만 관심이 있는 모듈, 외부(써드파티 라이브러리, DB, HTTP 등등)에 의존하지 않고 온전히 비즈니스 로직에만 관심을 갖고 있는 모듈로써 어떠한 의존성도 가지지 않습니다.db 모듈: 데이터의 CRUD(저장, 조회, 수정, 삭제)에만 관심이 있는 모듈, 클라이언트의 요구사항을 처리하기 위해 domain 모듈에 의존(implementation)하고 있습니다.이미지 출처: Gradle Docs// db 모듈의 build.gradle.kts\ndependencies {\n    implementation(project(\":domain\"))\n    // 기타 디펜던시들...\n}application 모듈: 클라이언트의 요청을 받아 처리하는 모듈, 클라이언트의 요구사항을 처리하기 위해 domain 모듈에 의존(implementation)하고 있으며, application 모듈에 main 함수가 존재하기 때문에 데이터 조작(저장, 조회 등등)을 위해 db 모듈에도 의존(runtimeOnly)하고 있습니다.이미지 출처: Gradle Docs// application 모듈의 build.gradle.kts\ndependencies {\n    implementation(project(\":domain\"))\n    runtimeOnly(project(\":db\"))\n    // 기타 디펜던시들...\n}테스트 전용으로 작성한 클래스를 다른 모듈에게 노출시키기domain 모듈에 아래와 같은 객체가 있다고 가정해보겠습니다.class Order(\n    val id: String,\n    val description: String,\n    val amount: Long\n)테스트에서 위 클래스를 사용해야할 때 객체를 생성하려고 생각하면 매우 번거로워집니다. (공감이 되지 않는다면 파라미터가 10개 정도 된다고 생각해보면 됩니다.)이 때 모든 파라미터에 기본값을 넣는 절충안도 존재하는데, 객체의 필수값이 기본값으로 채워진 채 객체가 생성되면 불안정하게 동작할 수 있습니다. 누군가의 실수로 프로덕션에서 객체의 필수값 중 일부가 기본값으로 생성된다면 의도치 않은 동작을 하게 될 수도 있기 때문입니다.따라서 테스트에서 사용할 목적으로 디폴트 값이 들어간 빌더 객체를 만들게 됩니다.참고로 IntelliJ IDEA에서 코틀린 클래스의 빌더를 만들어주는 플러그인은 kotlin-builder-generator를 사용하면 손 쉽게 만들 수 있습니다.data class OrderBuilder(\n    val id: String = \"\",\n    val description: String = \"\",\n    val amount: Long = 0L\n) {\n    fun build(): Order {\n        return Order(\n            id = id,\n            description = description,\n            amount = amount\n        )\n    }\n}하지만 빌더는 테스트에서만 사용해야하기 때문에 domain/src/test 디렉토리 밑에 생성해야합니다. test가 아닌 main 디렉토리 밑에 존재하게 되면 프로덕션 코드에서 누가 해당 빌더로 온전치 않은 상태의 객체를 생성하고 사용하는 실수를 할 수 있기 때문입니다.이런 Builder나 Helper 같이 테스트 전용으로 만든 클래스들을 해당 클래스가 존재하는 모듈(domain 모듈)이 아닌 해당 모듈을 의존하고 있는 다른 모듈(domain 모듈에 의존하고 있는 application, db 모듈)의 테스트에서 사용하고 싶다는 니즈가 생겼다고 가정해보겠습니다.하지만 application과 db 모듈에서 domain 모듈에 의존하고 있다고 할지라도 각 모듈의 테스트에서는 OrderBuilder를 import 할 수 없습니다.build된 jar 파일의 압축을 해제했을 때 나오는 결과물을 보면 main 디렉토리 밑에 있는 Order 클래스는 포함하고 있지만, test 디렉토리 밑에 있는 OrderBuilder 클래스는 포함하고 있지 않기 때문입니다.어떻게 생각해보면 당연한 결과입니다.domain 모듈을 테스트하는데 필요한 정보들은 프로덕션 코드에서는 필요가 없고, 그렇기 때문에 굳이 불필요하게 테스트 전용 클래스들까지 포함시킬 필요는 없기 때문입니다.이제 문제를 해결하기 위한 간단한 방법 두 가지를 떠올리게 됩니다.각 모듈의 test 디렉토리에 빌더를 복사/붙여넣기 합니다. 하지만 이는 코드의 중복을 유발하며 Order 클래스의 변경사항이 생겼을 때 각 모듈에 존재하는 OrderBuilder 클래스를 각각 수정해야한다는 번거로움이 존재합니다.Builder/Helper를 모아놓은 별도의 test-data 같은 테스트 전용 모듈을 만들고, 각 모듈에서 test-data 클래스에 의존(testImplementation)하게 만듭니다.// application/db 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    testImplementation(project(\":test-data\"))\n}하지만 이는 실제 소스코드(Order는 domain 모듈에 존재)와 거리가 멀어지게 만들어(OrderBuilder는 test-data 모듈에 존재) 응집도가 떨어지는 모듈이 나오게 됩니다.또한 테스트 전용임에도 불구하고 test-data 모듈의 클래스들을 외부에 노출시켜야하기 때문에 test 디렉토리가 아닌 main 디렉토리에 둬야 하는 점도 약간의 혼란(’main 디렉토리에 있으니까 프로덕션 레벨에서 사용하는 건가…?’ 하는 정도의)을 유발할 수 있습니다.둘 다 좋은 방법은 아니라는 생각이 듭니다. 이 문제를 해결하기 위한 빛과 소금과 같은 존재가 있습니다.구세주: java-test-fixtures 플러그인Gradle에는 이런 문제를 해결하고자 java-test-fixtures 플러그인이 존재합니다.우선 외부에 노출시키고자 하는 Builder나 Helper 클래스가 존재하는 domain 모듈의 build.gradle.kts 파일에 플러그인을 추가해주고 프로젝트를 reload 하면 됩니다.// domain 모듈의 build.gradle.kts\nplugins {\n    // 기타 플러그인들...\n    `java-test-fixtures`\n}java-test-fixtures 플러그인이 적용된 모듈에서 디렉토리를 생성하려고 하면 IntelliJ IDEA에서는 testFixtures 디렉토리가 자동완성 됩니다.그럼 아까 생성했던 OrderBuilder 클래스는 test가 아닌 testFixtures 디렉토리로 이동시켜준 후 build를 했을 때 수행되는 Gradle Task들을 보게 되면 testFixture 관련된 task가 추가된 걸 알 수 있습니다../gradlew :domain:build\n\n...\n> Task :domain:compileTestFixturesKotlin\n> Task :domain:compileTestFixturesJava NO-SOURCE\n> Task :domain:processTestFixturesResources NO-SOURCE\n> Task :domain:testFixturesClasses UP-TO-DATE\n> Task :domain:testFixturesJar\n\n그리고 빌드된 결과물을 보면 test-fixtures.jar가 추가된 걸 볼 수 있습니다.plain.jar는 plain에, test-fixtures.jar는 test에 각각 풀었는데 OrderBuilder는 test에 존재하는 걸 보니 test-fixtures.jar에 존재한다는 걸 알 수 있습니다.여기서 또 java-test-fixtures 플러그인의 장점이 나오게 되는데 다른 모듈에서 불필요하게 여기는 클래스들(test 디렉토리에 있는 @Test 어노테이션이 붙은 테스트 코드들 등등)은 노출되지 않고, 필요한 클래스들(testFixtures 디렉토리에 있는 Helper나 Builder 클래스 등등)만 노출된다는 점입니다.하지만 이렇게 했다고 해서 아직 application이나 db 모듈에서 OrderBuilder를 import 할 수 있는 건 아닙니다. application과 db 모듈에서는 plain.jar에 의존하고 있는 것이지, test-fixtures.jar에 의존하고 있는 건 아니기 때문입니다.따라서 application과 db 모듈에서 test-fixtures.jar에 의존하도록 각 모듈의 build.gradle.kts에 추가해줘야합니다.// application/db 모듈의 build.gradle.kts\ndependencies {\n    implementation(project(\":domain\"))\n    testImplementation(testFixtures(project(\":domain\")))\n    // 기타 디펜던시들...\n}위와 같이 의존성을 추가해줘야 비로소 application과 db 모듈의 테스트 코드에서도 domain 모듈의 testFixtures에 존재하는 OrderBuilder를 사용할 수 있게 됩니다.이해하기 쉽게 모듈 간의 디렉토리 관계를 좀 더 세분화해서 표현해보았습니다.테스트 전용으로 추가한 의존성을 다른 모듈에게 노출시키기db 모듈의 통합테스트를 위해 인메모리 DB인 H2를 테스트 전용으로 의존성을 추가했다고 가정해보겠습니다.이미지 출처: Gradle Docs// db 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    testRuntimeOnly(\"com.h2database:h2\")\n}이 상태에서 db 모듈의 통합테스트를 돌리게 되면 H2 DB를 사용하여 실제 DB와 격리된 환경에서 테스트가 돌아가는 것을 볼 수 있습니다.그리고 application 모듈은 아래와 같이 db 모듈에 의존하고 있기 때문에 통합테스트를 작성할 때도 인메모리 DB를 쓸 것이라 희망하게 되는데 실제로 테스트를 짜고 돌려보면 그렇지 않습니다.// application 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    runtimeOnly(project(\":db\"))\n}gradle 모듈의 디펜던시를 보게 되면 db 모듈의 testRuntimeClasspath에는 H2가 존재하지만, application 모듈의 testRuntimeClasspath에 존재하는 db 모듈에는 H2가 존재하지 않기 때문입니다.이 때도 application 모듈의 build.gradle.kts에 H2를 의존성으로 추가하는 방법이 있겠지만 관심사 문제가 있습니다. application 모듈의 관심사는 ‘어떻게 클라이언트와 커뮤니케이션해서 요구사항을 만족시킬 것인가?’이지 세부적인 내용(’저장소는 무엇을 쓸까? 데이터는 어디서 저장하고 어떻게 불러올까?’ 같은)은 관심사가 아닙니다. 따라서 H2를 직접적으로 의존성을 추가하는 순간 관심사 분리가 제대로 되지 않게 됩니다.이 문제를 해결하기 위해 또 우리의 구세주 java-test-fixtures 플러그인이 필요합니다.testFixturesComplieClasspath와 testFixturesRuntimeClasspath우선 외부에 테스트 전용 의존성(H2)을 노출시키고 싶은 db 모듈에 java-test-fixtures 플러그인을 추가하고, testRuntimeOnly로 추가했던 H2 의존성을 testFixturesRuntimeOnly로 변경해줘야 합니다.// db 모듈의 build.gradle.kts\nplugins {\n    // 기타 플러그인들...\n    `java-test-fixtures`\n}\n\ndependencies {\n    // 기타 디펜던시들...\n    testFixturesRuntimeOnly(\"com.h2database:h2\")\n}그리고 나서 다시 db 모듈의 디펜더시를 보면 기존에 보지 못했던 testFixturesCompileClasspath와 testFixturesRuntimeClasspath가 추가된 게 보입니다.사실 두 가지 클래스패스는 java-test-fixtures 플러그인을 추가하기만 해도 추가되는 클래스패스입니다.여기서 눈여겨봐야할 것은 기존에는 testRuntimeClasspath에만 존재하던 H2 의존성이 testFixturesRuntimeClasspath에도 추가된 점입니다.이에 대한 해답은 java-test-fixtures 플러그인 문서를 보다보면 아래와 같은 내용에 나오게 됩니다.Test fixtures are configured so that: • they can see the main source set classes • test sources can see the test fixtures classes두 번째로 나와있는 테스트 소스(test 디렉토리에 있는 내용들)에서 test fixture(testFixtures 디렉토리에 있는 내용들)에 있는 내용을 참조(can see)할 수 있도록 구성된다는 내용이 핵심입니다.따라서 testFixturesRuntimeOnly로만 추가(testFixturesRuntimeClassPath)했지만 testRuntimeOnly로도 추가된 것과 동일한 효과(testRuntimeClasspath에 추가된 효과)를 같이 보게 됩니다. 따라서 db 모듈의 통합테스트를 돌렸을 때는 여전히 H2 DB를 사용하게 됩니다.하지만 H2를 db 모듈에 testFixturesRuntimeClasspath에 추가했지만, 여전히 application 모듈의 testRuntimeClasspath를 보면 아직도 db 모듈에는 H2 의존성이 추가되지 않은 모습을 볼 수 있습니다.그 이유는 application 모듈의 build.gradle.kts를 보면 알 수 있습니다.// application 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    runtimeOnly(project(\":db\"))\n}이미지 출처: Gradle Docs바로 정답은 runtimeOnly 키워드에 있습니다.runtimeOnly로 추가한 디펜던시는 testRuntimeClasspath에도 추가됩니다. (물론 runtimeClasspath에도 추가됩니다.)하지만 testRuntimeClasspath에 추가된 의존성은 외부 모듈에 노출되지 않는다는 특성이 있습니다.따라서 우리는 db 모듈의 testRuntimeClasspath가 아닌 testFixturesRuntimeClasspath에 추가된 의존성들에 주목해야하며 해당 의존성들이 추가되도록 application 모듈의 build.gradle.kts를 수정해야 합니다.// application 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    runtimeOnly(project(\":db\"))\n    testRuntimeOnly(testFixtures(project(\":db\")))\n}마지막 부분이 db 모듈의 testFixturesRuntimeClasspath에 있는 의존성을 testRuntimeOnly로 추가(testRuntimeClasspath에 추가)하는 내용입니다.이제 application 모듈의 testRuntimeClasspath에도 db 모듈의 testFixutresRuntimeClasspath에 있는 H2 의존성이 추가된 걸 볼 수 있습니다.이 상태에서 application 모듈의 통합테스트를 돌리더라도 H2 DB를 사용하는 걸 볼 수 있습니다.결론테스트 코드는 실제 프로덕션에 영향을 미치지 않으므로 신경을 덜 쓰기 마련입니다. 그러다보면 중복이 난무하고 관심사 분리도 제대로 되지 않고 의존성 지옥에 빠지기 십상입니다. 하지만 테스트 코드는 우리의 소프트웨어를 좀 더 나은 설계로 유도하며 안정감도 주기 때문에 품질을 관리해야하는 소프트웨어임에는 분명합니다.혹시 해당 포스트를 보고 ‘어, 그거 그렇게 하는 거 아닌데…’라는 생각이 들었다면 토스페이먼츠에 와서 신나게 토론할 준비가 되어있으니 언제든 환영합니다!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "11": "CommonJS와 ESM에 모두 대응하는 라이브러리 개발하기: exports field장호승ㆍFrontend Developer2022. 10. 4토스 프론트엔드 챕터에서는 개발 생산성을 극대화하기 위해 코드를 지속적으로 라이브러리로 만들고 있습니다. 그 결과 지금은 100개가 넘는 라이브러리를 운영하고 있습니다.Node.js 12부터 ECMAScript Modules라는 새로운 Module System이 추가되면서, 기존의 CommonJS라는 Module System까지, 라이브러리는 두 가지 Module System을 지원해야 하게 되었습니다.토스팀에서는 그것을 package.json의 exports field를 통해 지원하고 있습니다. 각각의 모듈 시스템과 exports field에 대해 자세히 알아봅시다.Node.js에는 CommonJS, ECMAScript Modules(이하 CJS, ESM)라는 두 가지 모듈 시스템이 존재합니다.CommonJS (CJS)// add.js\nmodule.exports.add = (x, y) => x + y;\n\n// main.js\nconst { add } = require('./add');\n\nadd(1, 2);ECMAScript Modules (ESM)// add.js\nexport function add(x, y) {\n  return x + y\n}\n\n// main.js\nimport { add } from './add.js';\n\nadd(1, 2);CJS는 require / module.exports 를 사용하고, ESM은 import / export 문을 사용합니다.CJS module loader는 동기적으로 작동하고, ESM module loader는 비동기적으로 작동합니다.ESM은 Top-level Await을 지원하기 때문에 비동기적으로 동작합니다.따라서 ESM에서 CJS를 import 할 수는 있지만, CJS에서 ESM을 require 할 수는 없습니다. 왜냐하면 CJS는 Top-level Await을 지원하지 않기 때문입니다.이 외에도 두 Module System은 기본적으로 동작이 다릅니다.따라서 두 Module System은 서로 호환되기 어렵습니다.왜 두 Module System을 지원해야해요?서로 호환되기 어려운 두 Module System을 지원해야하는 이유는 뭘까요? 그냥 하나로 통일하면 안될까요? 토스팀에서는 왜 그것을 중요하게 생각할까요?토스팀에서는 Server-side Rendering(이하 SSR)을 적극적으로 사용하고 있기 때문에, Node.js의 CJS를 지원하는 것이 중요했습니다.그리고 Module System의 지원은 브라우저 환경에서의 퍼포먼스와도 관련이 있습니다. 브라우저 환경에서는 페이지 렌더링을 빠르게 하는 것이 중요한데, 이 때 JavaScript는 로딩되어 실행되는 동안 페이지 렌더링을 중단시키는 리소스들 중 하나 입니다.따라서 JavaScript 번들의 사이즈를 줄여서 렌더링이 중단되는 시간을 최소화 하는 것이 중요합니다. 이를 위해 필요한 것이 바로 Tree-shaking입니다. Tree-shaking이란 필요하지 않은 코드와 사용되지 않는 코드를 삭제하여 JavaScript 번들의 크기를 가볍게 만드는 것을 말합니다.이 때, CJS는 Tree-shaking이 어렵고, ESM은 쉽게 가능합니다.왜냐하면 CJS는 기본적으로 require / module.exports 를 동적으로 하는 것에 아무런 제약이 없습니다.// require\nconst utilName = /* 동적인 값 */\nconst util = require(`./utils/${utilName}`);\n\n// module.exports\nfunction foo() {\n  if (/* 동적인 조건 */) {\n    module.exports = /* ... */;\n  }\n}\nfoo();따라서 CJS는 빌드 타임에 정적 분석을 적용하기가 어렵고, 런타임에서만 모듈 관계를 파악할 수 있습니다.하지만 ESM은 정적인 구조로 모듈끼리 의존하도록 강제합니다. import path에 동적인 값을 사용할 수 없고, export는 항상 최상위 스코프에서만 사용할 수 있습니다.import util from `./utils/${utilName}.js`; // 불가능\n\nimport { add } from \"./utils/math.js\"; // 가능\n\nfunction foo() {\n  export const value = \"foo\"; // 불가능\n}\n\nexport const value = \"foo\"; // 가능따라서 ESM은 빌드 단계에서 정적 분석을 통해 모듈 간의 의존 관계를 파악할 수 있고, Tree-shaking을 쉽게 할 수 있습니다.위와 같은 배경으로 토스팀에서는 CJS/ESM 모두 지원하는 라이브러리를 운영하게 되었습니다.파일이 CJS인지 ESM인지 어떻게 알아요?Module System이 두 개가 존재하며 둘 다 지원해야할 필요성은 알겠는데, .js 파일이 CJS인지 ESM인지 어떻게 알 수 있을까요? package.json의 type field 또는 확장자를 보고 알 수 있습니다..js 파일의 Module System은 package.json의 type field에 따라 결정됩니다.type field의 기본값은 \"commonjs\" 이고, 이 때 .js 는 CJS로 해석됩니다.다른 하나는 \"module\" 입니다. 이 때 .js 는 ESM으로 해석됩니다..cjs 는 항상 CJS로 해석됩니다..mjs 는 항상 ESM으로 해석됩니다.TypeScript도 4.7부터 tsconfig.json 의 moduleResolution 이 nodenext 또는 node16 으로 설정된 경우, 위 규칙이 똑같이 적용됩니다.type field가 \"commonjs\" 인 경우, .ts 는 CJS로 해석됩니다.type field가 \"module\" 인 경우, .ts 는 ESM으로 해석됩니다..cts 는 항상 CJS로 해석됩니다..mts 는 항상 ESM으로 해석됩니다.CJS와 ESM의 차이, 패키지의 기본 Module System을 설정하는 방법과 확장자 모두 알아봤는데, 그래서 어떻게 하면 하나의 패키지가 CJS/ESM을 동시에 매끄럽게 제공할 수 있을까요?정답은 exports field입니다. exports field는 무슨 문제를 해결해줄까요? 어떤 역할을 할까요?패키지 entry point 지정기본적으로는 package.json의 main field와 같은 역할을 합니다. 패키지의 entry point를 지정할 수 있습니다.subpath exports 지원기존에는 filesystem 기반으로 동작했기 때문에, 패키지 내부의 임의의 JS 파일에 접근할 수 있었고, 또한 실제 filesystem 상의 위치와 import path를 다르게 둘 수 없었습니다.// 디렉토리 구조\n/modules\n  a.js\n  b.js\n  c.js\nindex.jsrequire(\"package/a\"); // 불가능\nrequire(\"package/modules/a\"); // 가능이 때, exports field를 사용해 subpath exports를 사용하면, 명시된 subpath 외에는 사용할 수 없고, filesystem 상의 위치와 import path를 다르게 지정할 수 있습니다.// CJS 패키지\n{\n  \"name\": \"cjs-package\",\n  \"exports\": {\n    \".\": \"./index.js\",\n    \"./a\": \"./modules/a.js\",\n  },\n}// ./a.js가 아니라\n// ./modules/a.js를 불러온다.\nrequire(\"cjs-package/a\");\n\n// 에러\n// ./b는 exports field에 명시하지 않은 subpath이다.\nrequire(\"cjs-package/b\");conditional exports 지원기존에는 filesystem 기반으로 동작했기 때문에, Dual CJS/ESM 패키지를 자연스럽게 운영하기가 어려웠습니다.exports field를 사용하면, 똑같은 import path에 대해 특정 조건에 따라 다른 모듈을 제공할 수 있습니다.{\n  \"name\": \"cjs-package\",\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.cjs\",\n      \"import\": \"./esm/index.mjs\"\n    }\n  }\n}// CJS 환경\n// ./dist/index.cjs를 불러온다.\nconst pkg = require(\"cjs-package\");\n\n// ESM 환경\n// ./esm/index.mjs를 불러온다.\nimport pkg from \"cjs-package\";올바른 exports fieldDual CJS/ESM 패키지의 exports field를 올바르게 작성하기 위해 주의해야할 점을 알아봅시다.상대 경로로 표시하기exports field는 모두 . 으로 시작하는 상대 경로로 작성되어야 합니다.// X\n{\n  \"exports\": {\n    \"sub-module\": \"dist/modules/sub-module.js\"\n  }\n}\n\n// O\n{\n  \"exports\": {\n    \".\": \"./dist/index.js\",\n    \"./sub-module\": \"./dist/modules/sub-module.js\"\n  }\n}Module System에 따라 올바른 확장자 사용하기conditional exports를 사용할 때, 패키지가 따르는 Module System에 따라, 즉 package.json의 type field에 따라 올바른 JS 확장자를 사용해야 합니다.CJS 패키지일 때// ESM은 .mjs로 명시해야함\n{\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.js\",\n      \"import\": \"./dist/index.mjs\"\n    }\n  }\n}ESM 패키지일 때// CJS는 .cjs로 명시해야함\n{\n  \"type\": \"module\"\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.cjs\",\n      \"import\": \"./dist/index.js\"\n    }\n  }\n}\n\n이 규칙을 지키지 않고 전부 .js 확장자를 사용했을 때는 어떤 일이 발생할까요? 아래와 같이 상황을 가정하겠습니다.cjs-package 는 CJS 패키지이다.type field가 \"commonjs\" 이기 때문이다../dist/index.js 는 CJS 문법(require / module.exports)으로 작성된 모듈이다../esm/index.js 는 ESM 문법(import / export)으로 작성된 모듈이다.{\n  \"name\": \"cjs-package\",\n  \"type\": \"commonjs\",\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.js\",\n      \"import\": \"./esm/index.js\"\n    }\n  }\n}CJS 환경에서 cjs-package 를 require 했을 땐 잘 동작합니다. ./dist/index.js 는 CJS 모듈이고, 확장자가 .js 이므로, 가장 가까운 package.json의 type field를 따라 CJS Module Loader가 사용될 것이기 때문입니다.// 잘 동작한다.\n// ./dist/index.js를  CommonJS Module Loader로 불러온다.\nconst pkg = require(\"cjs-package\");하지만 ESM 환경에서 cjs-package 를 import 했을 땐 에러가 발생합니다. ./esm/index.js 는 ESM 모듈이지만, 확장자가 .js 이므로 가장 가까운 package.json의 type field를 따라 CJS Module Loader가 사용됩니다.ESM 문법으로 작성된 JavaScript를 CJS Module Loader로 읽기 때문에 당연히 에러가 발생합니다.(예시: import 문은 ESM에서만 사용 가능하다는 에러가 발생)// 에러가 발생한다.\n// ./esm/index.js를 CJS Module Loader로 읽었다.\nimport * as pkg from \"cjs-package\";TypeScript 지원하기TypeScript에서 module import시, 항상 Type Definition을 찾게 되는데요. 기존에는 filesystem 기반으로 Type Definition을 탐색했습니다.// ./sub-module.d.ts를 찾는다.\nimport subModule from \"package/sub-module\";\n\n하지만 TypeScript 4.7부터 moduleResolution 옵션에 node16 과 nodenext 가 정식으로 추가되었고, node16 과 nodenext 는 filesystem 기반이 아닌 exports field로부터 Type Definition을 탐색합니다. 또한, CJS TypeScript( .cts )와 ESM TypeScript( .mts )를 구분합니다.TypeScript는 conditional import의 조건 중 types 를 참조하며, 이 때 JavaScript와 마찬가지로 package.json의 type field에 따라 알맞은 확장자 ( .cts / .mts )를 사용해야 합니다.CJS 패키지// ESM TS는 mts로 명시해야함\n{\n  \"exports\": {\n    \".\": {\n      \"require\": {\n        \"types\": \"./index.d.ts\",\n        \"default\": \"./index.js\"\n      },\n      \"import\": {\n        \"types\": \"./index.d.mts\",\n        \"default\": \"./index.mjs\"\n      }\n    }\n  }\n}ESM 패키지// CJS TS는 cts로 명시해야함\n{\n  \"type\": \"module\",\n  \"exports\": {\n    \".\": {\n      \"require\": {\n        \"types\": \"./index.d.cts\",\n        \"default\": \"./index.cjs\"\n      },\n      \"import\": {\n        \"types\": \"./index.d.ts\",\n        \"default\": \"./index.js\"\n      }\n    }\n  }\n}그럼 TypeScript의 경우에는 위 규칙을 지키지 않으면 어떻게 될까요? 아래와 같이 상황을 가정하겠습니다.esm-package 는 ESM 패키지이다.type field가 \"module\" 이기 때문이다..cts (CJS TypeScript)에서 esm-package 를 사용한다.{\n  \"name\": \"esm-package\",\n  \"type\": \"module\",\n  \"exports\": {\n    \".\": {\n      \"types\": \"./index.d.ts\",\n      \"require\": \"./index.cjs\",\n      \"import\": \"./index.js\"\n    }\n  }\n}이 때 .cts (CJS TypeScript)에서 esm-package 를 require하면 타입 에러가 발생합니다.esm-package 는 Type Definition을 ./index.d.ts 만 지원합니다. 즉, ESM/CJS TypeScript 모두 ./index.d.ts 를 바라보게 됩니다.이 때, esm-package 는 ESM 패키지이기 때문에 index.d.ts 는 ESM TypeScript로써 해석됩니다.따라서 esm-package 는 CJS TypeScript 입장에서 Pure ESM Module이고, CJS는 ESM을 불러올 수 없기 때문에 esm-package 가 순수 ESM으로만 확인된다는 타입 에러가 발생합니다.// index.cts\n\n// Type Error: esm-package는 동기적으로 가져올 수 없는 ES 모듈로만 확인됩니다.\n// CJS TypeScript를 위한 .d.cts를 지원하지 않았기 때문에 발생하는 에러\nimport * as esmPkg from \"esm-package\";\n\n최근 토스팀 내부 라이브러리들은 위처럼 올바르게 exports field를 작성하여 배포되고 있습니다. CJS/ESM JavaScript는 물론 TypeScript 지원까지 잘 되있습니다.JavaScript/TypeScript 생태계는 계속해서 발전하고 있지만, TypeScript까지 잘 지원하는 라이브러리는 정말 유명한 라이브러리들 중에서도 찾아보기가 많이 힘듭니다.그렇다면 우리가 그 시작점이 되면 어떨까요? 토스팀에서는 이런 기술적인 문제를 함께 풀어가고 싶으신 분들을 언제나 환영합니다. 함께 좋은 생태계를 만들어 나가고 싶어요.Node.js의 CJS/ESM에 대해CJSESMDetermining Module Systemexports field에 대해package.json export fieldSubpath exportsConditional exportsTypeScript의 CJS/ESM 지원에 대해4.7 릴리즈 노트재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "12": "토스증권 QA Team을 소개합니다곽수정ㆍQA Manager2022. 10. 25안녕하세요. 저는 3년 전 토스의 첫 QA Manager로 입사해서 현재 토스증권 QA팀에서 일하고 있는 곽수정입니다. 길다면 긴 3년이란 시간 동안 즐겁고 재밌게 일할 수 있었던 토스와 토스증권의 QA팀을 소개해보려고 해요.QA가 없었던 토스팀, 왜 첫 번째 QA Manager를 채용하게 되었을까요?초창기의 토스팀은 사용자에게 빠른 제품/서비스를 제공하는데 많은 노력을 기울이고 있었어요. 제품의 퀄리티 역시 PO, 개발자 등 제품을 만드는 ‘메이커(Maker)’들이 간단한 테스트를 통해 관리하고 있었죠. 버그가 발생하더라도 누구보다 빠르게 대응할 수 있다는 강한 자신감과 팀원 개개인의 오너십이 있었기 때문에 가능한 일이었다고 생각해요.이러한 동력에 힘입어 토스는 시장에서 빠른 속도감을 무기로 존재감을 높여갔어요. 그만큼 우리가 제공하는 제품과 서비스의 수도 증가했죠. 이에 따라 메이커들이 제품을 만들면서, 동시에 높은 수준의 제품 안정성을 체크하고 유지하는 데 한계가 생기기 시작했어요. 이 시기에 토스팀에서도 QA 담당자의 필요성이 논의되기 시작했지만, QA가 진행하는 품질 테스트가 우리의 속도에 영향을 주지는 않을지 우려가 공존하기도 했답니다.시장에서의 빠른 속도만큼이나, 고객들에게 안정적이고 높은 퀄리티의 서비스를 제공하는 것도 중요하다고 믿었던 토스팀은 2019년 첫 번째 QA Manager를 채용하게 됩니다. 그게 바로 저고요!그럼 토스팀의 QA는 어떻게 일할까요?QA가 없던 회사에서 QA가 일하는 방식을 만들어 나가는 것은 아주 어려움이 많았어요. 일반적인 IT 회사에서는 서비스 출시 전에 테스트하는 것이 너무 당연한데요. 처음 토스에 입사했을 땐 이것도 알려야 할 만큼 어려움이 있었답니다.이전 회사의 경우, 기획팀에서 기획서를 작성한 후에 개발/QA팀에 공유해서 서비스 출시 일정을 결정했어요. 그래서 QA팀에서는 QA 중에 발견한 버그로 인해 출시 일정이 뒤로 밀리거나 개발이 늦어지지 않도록 Sanity Test(BVT)라는 절차를 만들어서 QA 가용 리소스를 확보하는 장치를 만들기도 했고요. 오히려 이러한 장치가 개발자와 QA 모두에게 업무적인 비효율을 초래해서 서비스 출시일이 미뤄지는 경우가 많다는 점이 아쉬웠어요.하지만 토스의 방식은 달라요.QA Manager는 다양한 툴을 통해 서비스와 관련된 크고 작은 변경 사항들을 미리 확인할 수 있어요. 접근 권한 또한 열려 있어서 개발자의 작업 상태를 확인할 수 있고요. 자체적으로 QA계획을 수립해서 작은 단위부터 미리 테스트를 시작할 수도 있고, 통합테스트(Integration Test) 및 모니터링과 같은 QA 단계도 직접 조율할 수 있어요.QA Manager가 테스트를 진행하지만, 살충제 패러독스(Pesticide Paradox)에 빠지지 않도록 개발자도 배포를 위한 Regression Test Case를 별도로 수행하고 있어요. 매주 랜덤으로 수행자를 지정한 뒤 테스트가 완료되어야만 앱 심사 등록 요청이 가능하도록 하고 있어요. 서비스를 만들어가는 담당자 모두가 안정적인, 높은 품질의 서비스를 제공하고자 하는 마음이 크기 때문에 가능한 방식이라고 생각해요.토스증권이 서비스를 처음 출시했을 땐, 어떻게 QA를 했을까요?토스증권 출범을 준비하던 시기에는 토스증권 소속 QA Manager가 전무한 상태였어요. 당시 토스 소속이었던 제가 파견을 나가 QA 업무를 지원해주고 있었죠.조직 내 QA 매니저가 없는 상태에서 새로운 분을 모셔오는건 정말 어려운 일이었어요. 그래서 외주업체에서 QA 테스트 전문가분들을 모셔와 MTS(모바일 트레이딩 시스템) 출시를 위한 테스트를 진행했어요.다시 말하면, 새로운 법인에서 메인 서비스를 출시하는데 단 3명의 QA 인력이 그 모든 품질 테스트를 담당했다는 것이죠.👀 “3명이라고요?? 그게 가능한 일인가요?”당연히, 저 혼자만 QA를 진행할 수는 없었어요.많은 팀원분들이 적극적으로 도움을 주셔서 가능했어요.토스증권이 서비스를 성공적으로 런칭할 수 있도록 같은 마음으로 바라봐주는 든든한 팀원분들이 없었다면 힘들었을 것 같아요. 당시 토스증권 팀원 총 70명 정도를 대상으로 클로즈 베타 테스트를 1~2회 정도 진행하였고, 토스커뮤니티의 타 계열사 팀원분들께도 사전 신청을 받아 150명을 대상으로 하는 추가 테스트도 진행했어요.이러한 과정을 통해 Edge Case를 발견하고 고객의 피드백도 미리 체험해볼 수 있었습니다. 제품에 진심인 팀원분들이 꼼꼼하게 피드백을 주시다 보니 서비스를 빠르게 개선해서 출시할 수 있었어요.(토스증권 서비스 출시 과정과 관련된 에피소드는 다음 포스팅에서 자세히 공개할께요)토스증권 QA팀은 어떤 업무를 하나요?홀로 QA Manager로 근무하던 때에는 리소스가 부족하니 고객 접점이 높은 프론트 서비스를 기준으로 QA 범위를 정했었어요.이제는 QA팀이 되어서 함께하는 든든한 동료분들이 있습니다. 각각 제품을 만드는 Silo/Team에 소속되어 프로덕트를 만드는 전 과정을 함께하고 있어요.QA가 일할 때 필요한 공통 정책을 정의하기도 하고 프로덕트의 QA 커버리지 확대를 위해 노력하고 있기도 합니다. QA Manager는 본인이 담당하고 있는 서비스를 대상으로 팀원들에게 탐색적 테스팅이나 Bug Bash를 제안하기도 해요. 주기적으로 QA Study도 진행하면서 자기 계발도 함께해요.최근에는 Test Automation Engineer를 채용하기도 했어요. UI Automation Testing 설계뿐만 아니라 QA팀에서 필요한 도구를 개발하는 업무까지 수행하고 계세요.토스증권 QA팀은 테스트 커버리지를 계속해서 넓혀가고 있습니다. 장기적으로는 QA 품질 서비스 지표를 만들어서 변하지 않는 북극성 같은 목표를 만들고자 해요.토스증권에서 QA로 근무하면서 가장 만족하는 점은 무엇인가요?제가 재직했던 여러 회사에서는 QA의 권한이 곧 QA 품질 보고서(Sign Off)라고 생각하는 경우가 많았어요. QA의 중요성을 주장하기 위해 보고서를 작성하느라 야근하는 일도 잦았죠. 품질팀에서는 버그나 장애에 대한 책임을 피하고자 테스트를 아웃소싱에 맡기는 경우도 많았습니다. QA에서 발견한 버그의 Priority, Severity를 고려하지 않고 품질 확보라는 명목으로 서비스 출시일 자체를 미뤄버리는 경우도 있었어요.서비스 출시일이 계속 뒤로 밀리고 있는 게 과연 회사를 위한 일이 맞는지는 아직도 공감되지 않아요.토스증권은 이러한 비효율적인 부분을 가장 지양합니다. 테스트케이스 수행 결과를 취합하고, Sign Off 결과에 대한 근거를 제공하기 위해 추가 이터레이션, Regression Test 일정을 확보하기 위해 문서 작업을 하는 등의 비효율이 없어요.서비스 출시를 할 때 QA가 허들이 되는 것이 아니라, 안정적인, 좋은 품질의 서비스를 제공하여 고객 만족을 추구한다는 점에서 토스증권 QA Manager로 근무하고 있는 것이 만족스러워요.토스증권에서는 어떠한 사람을 찾고 있나요? 🙋‍♂️🙋‍♀️토스증권과 함께 성장하는 QA Manager가 되기 위해서는 다음과 같은 부분이 중요하다고 말씀드리고 싶어요.QA에 대한 열정과 긍정적인 마음가짐을 바탕으로 토스증권 서비스의 품질을 향상시키기 위해 함께 고민할 수 있는, 고민하는 것을 즐기는 분QA 업무를 하시는 분들은 대부분 팀이 세팅되어있고 안정적으로 Iteration하고 있는 회사를 선호하시는 것 같아요. 새롭게 설립되는 QA팀의 경우, QA가 회사에서 꼭 필요한 역할이라는 것을 내부적으로 설득하는 것이 어렵기 때문일 수 있겠죠.토스증권은 팀원들이 QA의 중요성을 잘 알고 있고 QA팀이 설정해둔 목표 또한 명확합니다. 빠르게 성장하고 있는 서비스와 함께 QA 커버리지를 확대하기 위해 함께할 동료분들을 찾고 있어요.토스증권에서 메이커와 같은 시선으로 제품의 전 과정을 함께하는 QA Manager가 되어보세요!토스증권 QA Manager 채용공고 👉 바로가기토스증권 Test Automation Engineer 채용공고 👉 바로가기재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "13": "TypeScript 타입 시스템 뜯어보기: 타입 호환성김병묵ㆍNode.js Developer2022. 10. 26토스 Node.js 챕터에서는 높은 코드 가독성과 품질을 위해 TypeScript의 타입 시스템을 적극적으로 활용하고 있고 이에 대한 이해도를 높이기 위해 스터디를 꾸준히 진행하고 있습니다. TypeScript의 타입 시스템에 대해 공부해보던 중 알게된 흥미로운 몇가지 토픽들을 소개하려 합니다. 그 중 한가지로 이번글에서는 “타입 호환성 (type compatibility)”에 대해 알아보고자 합니다.TypeScript 공식문서 타입 호환성에 관한 글을 보면 아래와 같이 소개하고 있습니다.TypeScript의 타입 호환성은 구조적 서브타이핑(structural subtyping)을 기반으로 합니다. 구조적 타이핑이란 오직 멤버만으로 타입을 관계시키는 방식입니다. 명목적 타이핑(nominal typing)과는 대조적입니다. TypeScript의 구조적 타입 시스템의 기본 규칙은 y가 최소한 x와 동일한 멤버를 가지고 있다면 x와 y는 호환된다는 것입니다.위 내용에 대해 하나씩 이해해봅시다. 우선 강한 타입 시스템을 통해 높은 가독성과 코드 품질을 지향하는 TypeScript가 왜 타입 호환성을 지원하는 것일까요? 이 경우 타입 안정성에 문제가 생기게 되는 것은 아닐까요? 아래 예시를 통해 타입 호환성이 왜 필요한지 살펴보겠습니다.위와 같이 음식 Food 타입의 객체를 인자로 받아 간단한 칼로리 계산 공식으로 주어진 음식의 칼로리를 구하는 calculateCalorie 함수가 있습니다. 타입과 함수는 아래와 같이 구현되어 있습니다.type Food = {\n  /** 각 영양소에 대한 gram 중량값 */\n  protein: number;\n  carbohydrates: number;\n  fat: number;\n}\n\nfunction calculateCalorie(food: Food){\n  return food.protein * 4\n    + food.carbohydrates * 4\n    + food.fat * 9\n}한편, 개발자가 코드를 작성하는 과정에서 (의도했거나 혹은 실수로) calculateCalorie 함수 인자에 여러가지 타입의 객체를 전달해본다고 가정해봅시다. 이 경우 TypeScript 타입 시스템은 프로그램이 타입 오류를 일으킬 가능성을 검사하게 됩니다.위 3가지 케이스에 대해 Type Checker가 어떻게 판단하는 것이 좋을까요?개발자가 정의한 Food 타입과 동일한 타입인 경우 (1번) 오류 없음이 명확하며, Computer 타입과 같이 다른 타입이며 칼로리 계산이 불가능한 경우 (2번) 오류로 판단하는 것이 명확합니다. 하지만, 햄버거를 의미하며 음식의 한 종류인 Burger 타입이 전달되는 경우 (3번) 어떻게 판단하는 것이 맞을까요?type Burger = Food & {\n  /** 햄버거 브랜드 이름 */\n  burgerBrand: string;\n}심지어 Burger 타입이 위와 같이 Food 타입을 상속하며 칼로리 계산에 필요한 모든 프로퍼티를 포함하고 있어 런타임 상에서 정상적으로 동작한다면 이를 타입 오류라고 판단하는게 올바른 걸까요?이처럼 실제로 정상적으로 동작할 수 있는 올바른 코드라면 타입 시스템은 개발자의 의도에 맞게 유연하게 대응하여 타입 호환성을 지원하는 것이 더 좋을 수 있습니다. 이러한 유연성을 위해 TypeScript 타입 시스템은 부분적으로 타입 호환을 지원하고 있습니다.한편 위에 예시에서 Computer 타입 사례처럼 타입오류로 판단하는 것이 명확한 경우가 있으며, 타입 안정성을 해치면서까지 유연함을 제공하는 것은 바람직하지 못합니다. 이를 위해서는 어떠한 경우에 호환을 허용할 것인지에 대한 명확한 규칙이 필요합니다. 이러한 규칙 중 프로그래밍 언어들에서 널리 활용되는 방식으로 명목적 서브타이핑(nominal subtyping)과 구조적 서브타이핑(structural subtpying)이 있습니다.명목적 서브타이핑은 아래와 같이 타입 정의 시에 상속 관계임을 명확히 명시한 경우에만 타입 호환을 허용하는 것입니다. 이 방법을 통해 타입 오류가 발생할 가능성을 배제하고, 개발자의 명확한 의도를 반영할 수 있습니다./** 상속 관계 명시 */\ntype Burger = Food & {\n  burgerBrand: string;\n}\n\nconst burger: Burger = {\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n}\n\nconst calorie = calculateCalorie(burger)\n/** 타입검사결과 : 오류없음 (OK) */\n\n한편, 구조적 서브타이핑은 아래와 같이 상속 관계가 명시되어 있지 않더라도 객체의 프로퍼티를 기반으로 사용처에서 사용함에 문제가 없다면 타입 호환을 허용하는 방식입니다. 아래 예시를 보면 비록 상속 관계임을 명시하지는 않았지만 burger 변수는 Food 타입의 프로퍼티를 모두 포함하고 있고 따라서calculateCalorie 함수 실행과정에서 오류가 발생하지 않습니다.const burger = {\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n}\n\nconst calorie = calculateCalorie(burger)\n/** 타입검사결과 : 오류없음 (OK) */구조적 서브타이핑 방식은 타입 시스템이 객체의 프로퍼티를 체크하는 과정을 수행해주므로써, 명목적 서브타이핑과 동일한 효과를 내면서도 개발자가 상속 관계를 명시해주어야 하는 수고를 덜어주게 됩니다. 참고로, 구조적 서브타이핑은 “만약 어떤 새가 오리처럼 걷고, 헤엄치고, 꽥꽥거리는 소리를 낸다면 나는 그 새를 오리라고 부를 것이다.” 라는 의미에서 덕 타이핑 (duck typing) 이라고도 합니다.TypeScript Type Checker는 구조적 서브타이핑을 기반으로 타입 호환을 판단합니다.TypeScript는 구조적 서브타이핑을 지원하며, 명목적 서브타이핑만 지원하는 C#, Java 등의 언어는 명시적으로 상속 관계를 명시해주어야 타입 호환이 가능합니다.💡  한편, 여기서부터 좀 더 본격적인 이야기를 다루어 보겠습니다.위 구조적 서브타이핑 예시의 코드는 타입 호환성에 따라 타입 오류가 발생하지 않지만, 아래 코드의 경우 컴파일 과정에서 Argument is not assignable to parameter of type 'Food' 라는 타입 오류가 발생하게 됩니다. 글을 더 읽으시기에 앞서 실제로 TS Playground를 통해 오류를 확인해보시고 다양하게 테스트해보시는 것도 추천합니다.const calorie = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n})\n/** 타임검사결과 : 오류 (NOT OK)*/왜 위 코드는 타입 호환이 지원되지 않는 것일까요? 처음에 이 오류를 마주쳤을 때 이런저런 테스트를 해보며 함수에 값을 바로 인자로 전달하는 경우만 타입 호환이 지원되지 않는 것 같다고 유추하기는 했으나 조금 더 구체적인 규칙과 이렇게 예외가 발생하는 이유에 대해 이해해보고자 했습니다.결과적으로 TypeScript 컴파일러 코드 상의 구현로직과 위 이슈와 연관된 TypeScript Github PR을 통해 이해할 수 있었습니다. 이에 대해 알아보기 위해 우선 TypeScript 컴파일러가 동작하는 방식에 대해 간략히 살펴보겠습니다.TypeScript 컴파일러가 동작하는 방식에 관해 아래 영상에 자세히 소개되어 있으며, 이 중 몇가지 내용만 요약하여 살펴보겠습니다.https://www.youtube.com/watch?v=X8k_4tZ16qUTypeScript 컴파일러의 역할은 TypeScript 소스코드를 AST (Abstract Syntax Tree)로 변환한 뒤, 타입 검사를 수행하고, 그 후 JavaScript 소스코드로 변환하는 과정을 담당합니다.TypeScript 소스코드를 AST로 변환하는 과정은 parser.ts, scanner.ts , 타입 검사를 수행하는 과정은 binder.ts, checker.ts, AST를 JavaScript 소스코드로 변환하는 과정은 emitter.ts, transformer.ts 등의 파일이 담당하고 있습니다.실제로 TypeScript Github의 compiler 디렉토리에 가면 위 코드 파일이 어떤식으로 구현되어 있는지 확인해볼 수 있으며, 이번 글에서 다루고 있는 주제인 구조적 서브타이핑과 타입 호환에 관한 부분은 타입 검사와 가장 연관이 높은 checker.ts 파일의 hasExcessProperties() 함수에서 처리하고 있었습니다.아래는 checker.ts 코드 중 타입 호환의 예외가 발생하는 지점의 코드를 주요한 부분만 남기고 간소화한 것입니다. 주석과 함께 봐주시면 좋을 것 같습니다./** 함수 매개변수에 전달된 값이 FreshLiteral인 경우 true가 됩니다. */\nconst isPerformingExcessPropertyChecks =\n    getObjectFlags(source) & ObjectFlags.FreshLiteral;\n\nif (isPerformingExcessPropertyChecks) {\n    /** 이 경우 아래 로직이 실행되는데,\n     * hasExcessProperties() 함수는\n     * excess property가 있는 경우 에러를 반환하게 됩니다.\n     * 즉, property가 정확히 일치하는 경우만 허용하는 것으로\n     * 타입 호환을 허용하지 않는 것과 같은 의미입니다. */\n    if (hasExcessProperties(source as FreshObjectLiteralType)) {\n        reportError();\n    }\n}\n/**\n * FreshLiteral이 아닌 경우 위 분기를 skip하게 되며,\n * 타입 호환을 허용하게 됩니다. */지면상 다소 간소화한 코드만 남겨두었지만, 함수에 인자로 들어온 값이 FreshLiteral 인지 아닌지 여부에 따라 조건분기가 발생하여 타입 호환 허용 여부가 결정된다는 것을 확인할 수 있었습니다.그렇다면 Fresh Literal 이란 무엇이며, 왜 이 경우에는 타입 호환의 예외가 발생하도록 되어 있는 것일까요?TypeScript는 구조적 서브타이핑에 기반한 타입 호환의 예외 조건과 관련하여 신선도 (Freshness) 라는 개념을 제공합니다. 모든 object literal은 초기에 “fresh” 하다고 간주되며, 타입 단언 (type assertion) 을 하거나, 타입 추론에 의해 object literal의 타입이 확장되면 “freshness”가 사라지게 됩니다. 특정한 변수에 object literal을 할당하는 경우 이 2가지 중 한가지가 발생하게 되므로 “freshness”가 사라지게 되며, 함수에 인자로 object literal을 바로 전달하는 경우에는 “fresh”한 상태로 전달됩니다.한편, TypeScript Github PR (2015년 7월) 의 논의에 따르면, fresh object인 경우에는 예외적으로 타입 호환을 허용하지 않기로 했음을 확인할 수 있습니다. 그러한 이유에 대해 살펴보겠습니다./** 부작용 1\n * 코드를 읽는 다른 개발자가 calculateCalorie 함수가\n * burgerBrand를 사용한다고 오해할 수 있음 */\nconst calorie1 = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n})\n\n/** 부작용 2\n * birgerBrand 라는 오타가 발생하더라도\n * excess property이기 때문에 호환에 의해 오류가\n * 발견되지 않음 */\nconst calorie2 = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  birgerBrand: '버거킹'\n})구조적 서브타이핑에 기반한 타입 호환은 유연함을 제공한다는 이점이 있지만, 위 코드 사례와 같이 코드를 읽는 다른 개발자의 입장에서 함수가 실제 다루는 것보다 더 많은 데이터를 받아들인다는 오해를 불러일으킬 수 있고, 프로퍼티 키에 대한 오타가 발생하더라도 오류가 확인되지 않는 부작용이 있습니다.한편, fresh object를 함수에 인자로 전달한 경우, 이는 특정한 변수에 할당되지 않았으므로 어차피 해당 함수에서만 사용되고 다른 곳에서 사용되지 않습니다. 이 경우 유연함에 대한 이점보다는 부작용을 발생시킬 가능성이 높으므로 굳이 구조적 서브타이핑을 지원해야할 이유가 없습니다.TypeScript Type Checker는 구조적 서브타이핑을 기반으로 타입 호환을 판단하되,Freshness에 따라 예외를 둡니다.이처럼 타입 호환성은 유연함이라는 이점을 제공하지만 그로 인해 부작용이 발생할 수 있으므로, 이에 대한 절충안으로 타입 호환을 제공해서 얻는 이점이 거의 없는 fresh object에 대해서는 호환성을 지원하지 않기로 논의되어 TypeScript 컴파일러 코드에 반영된 것을 확인해볼 수 있었습니다.한편, 그럼에도 개발자가 fresh object에 대해서 타입 호환을 허용하고자 한다면 아래와 같이 함수 매개변수 타입에 index signature를 포함시켜두어 명시적으로 타입 호환을 허용시키는 것이 가능합니다. 또는 tsconfig 상에 suppressExcessPropertyErrors 를 true로 설정하는 방식도 가능합니다. (이 또한 동일한 PR 논의에 정의되어 있습니다.)type Food = {\n  protein: number;\n  carbohydrates: number;\n  fat: number;\n  [x: string]: any                  /** index signature */\n}\n\nconst calorie = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n})\n/** 타임검사결과 : 오류없음 (OK) */\n\n또한 반대로 모든 경우에 대해 타입 호환을 허용하지 않도록 강제하는 것도 가능한데 이를 위해 사용할 수 있는 기법이 Branded type (또는 Branding type) 입니다. 아래와 같이 의도적으로 __brand 와 같은 프로퍼티를 추가시켜, 개발자가 함수의 매개변수로 정의한 타입 외에는 호환이 될 수 없도록 강제하는 기법입니다. 온도(섭씨, 화씨)나 화폐단위(원, 달러, 유로)와 같이 같이 number 타입이지만 서로 다를 의미를 가질 수 있어 명시적인 구분이 필요할 때 사용해볼 수 있습니다.type Brand<K, T> = K & { __brand: T};\ntype Food = Brand<{\n  protein: number;\n  carbohydrates: number;\n  fat: number;\n}, 'Food'>\n\nconst burger = {\n  protein: 100,\n  carbohydrates: 100,\n  fat: 100,\n  burgerBrand: '버거킹'\n}\n\ncalculateCalorie(burger)\n/** 타임검사결과 : 오류 (NOT OK) */\n\n앞선 글을 통해 이해한 타입 호환의 이점과 부작용에 대한 이해를 바탕으로 개발자는 자신의 프로젝트를 진행하는 과정에서 필요에 맞게 index signature, tsconfig > suppressExcessPropertyErrors, branded type 등을 통해 타입 호환성의 범위를 선택하여 개발하는 것이 가능할 것입니다.TypeScript Type Checker는 내부적인 규칙에 따라 타입 호환을 판단하지만,개발자가 필요에 따라 선택하는 것이 가능합니다.이번글의 내용을 모두 요약하면 아래와 같습니다.타입 검사의 안정성과 유연함 사이에서 절충안으로 도입된 개념이 타입 호환성입니다. 그리고 타입 호환성을 지원하는 방법과 관련하여 개발자에게 명시적 선언을 어디까지 요구할 것인지에 대한 선택지가 존재합니다.TypeScript는 구조적 서브타이핑에 기반한 타입 호환을 통해 개발자의 명시적 선언을 줄여주는 한편 이로 인한 부작용을 개선하고자 freshness에 기반한 예외조건을 두었고, Index Signature와 Branded type 등의 방식을 통해 개발자가 명시적으로 선택할 수 있는 선택지를 만들어두었습니다.프로그래밍 언어마다 타입 검사가 동작하는 방식이 다르며 이는 해당 언어를 개발한 커뮤니티의 논의와 의사결정에 따라 선택된 결과라고 볼 수 있습니다. 본 주제 외에도 TypeScript 컴파일러 코드와 Github PR을 살펴보면 흥미로운 논의와 토픽들을 확인해볼 수 있습니다.토스 Node.js 챕터는 토스의 다양한 제품과 라이브러리 개발을 위해 팀원들의 지속적인 성장이 중요하다고 믿으며, 이를 위해 꾸준히 공부하고 공유하는 자리를 가지고 있으니 많은 관심 부탁드립니다.토스 Node.js Chapter 채용 공고 👉 바로가기감사합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "14": "NestJS 환경에 맞는 Custom Decorator 만들기송현지ㆍNode.js Developer2022. 11. 22데코레이터는 비즈니스와 상관 없는 로직들을 숨기면서 기능을 변경하거나 확장할 수 있게 합니다. 또한 여러 클래스에서 반복되는 공통 관심사가 있을 때 데코레이터를 사용하면 중복된 코드를 줄이고 코드를 모듈 단위로 관리하는 효과를 거둘 수 있습니다.이런 이유로 저희 Node.js Chapter에서도 데코레이터를 적극 활용하고 있습니다. 하지만 NestJS에서는 데코레이터를 만들 때 다음과 같은 질문들이 있었습니다.데코레이터에서 Provider를 사용해야할 때 어떻게 Provider에 접근할 수 있을까?메타데이터를 쓰는 NestJS 데코레이터를 일반 데코레이터와 사용해도 괜찮을까?NestJS에서 데코레이터를 만들기 위해서는 NestJS의 DI와 메타 프로그래밍 환경 등을 고려해야 합니다. 그래서 이 글을 통해 NestJS에서는 어떻게 데코레이터를 만드는지 살펴보고, 앞의 두 질문들을 고려하여 NestJS 환경에 맞는 데코레이터를 만들어보려고 합니다.들어가기 전에, 만약 데코레이터나 메타데이터가 생소하시다면 아래 문서들을 읽어보시는 걸 추천드립니다.Typescript Decoratorreflect-metadata@Injectable()\nclass TestService {\n  @Cacheable('key')\n  test() {\n      // 비즈니스 로직\n  }\n}TestService가 있을 때, 캐싱 로직을 Cacheable 데코레이터를 사용해 비즈니스 로직과 분리하려고 합니다.Cacheable 데코레이터에서 CacheManager라는 Provider를 사용하려면 어떻게 접근해야 할까요?@Module{\n  imports: [CacheModule.register(...)]\n  providers: [TestService]\n}\nclass TestModule {}CacheManager Provider를 export하는 CacheModule을 import 해봅시다.function Cacheable(key: string, ttl: number) {\n  return function (target: any, _key: string, descriptor: PropertyDescriptor) {\n    const methodRef = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n    console.log(this) // TestService {}\n\n    // TypeError: Cannot read properties of undefined (reading 'get')\n    const value = await this.cache.get(key);\n    if (value) {\n      return value;\n    }\n\n    const result = await methodRef.call(this, ...args);\n    await this.cache.set(key, result, ttl);\n    console.log(result)\n    return result;\n    };\n  };\n}TestModule에서 CacheModule을 import하고 있긴 하지만 TestService에서 CacheManager 를 주입하지 않는 이상 Cacheable에서 CacheManager에 접근할 방법이 없습니다. Cacheable 데코레이터를 사용하려면 클래스에 항상 CacheManager를 주입해주어야 하는 불편함이 있습니다.게다가 CacheManager를 넣어준다고 해도 멤버 이름을 cache 로 강제해야 합니다. 가능한 방법이지만 휴먼 에러가 발생할 수 있어 좋은 방법은 아닙니다.그렇다면 NestJS 메서드 데코레이터는 어떻게 되어있을까요?NestJS가 데코레이터를 등록하는 과정은 ‘마킹 - 조회 - 등록’로 크게 세 단계로 나뉩니다. Cron 메서드 데코레이터를 예로 들어보겠습니다.마킹 - SetMetadata라는 함수로 특정 메서드에 CRON 심볼을 메타데이터 키로 등록합니다.조회 - 모듈이 초기화되는 시점에 DiscoveryServiced와 MetadataScanner로 모든 Provider 클래스를 순회하며 CRON 심볼을 메타데이터로 가지고 있는 메서드들을 찾습니다.등록 - 메서드를 찾았으면 해당 메서드를 크론 잡으로 등록합니다.NestJS에서 제공하는 SetMetadata와 DiscoverService, 그리고 MetadataScanner를 사용하면, 특정 클래스나 메서드만 필터링하여 IoC 내 다른 Provider를 사용해 원하는 로직들을 적용할 수 있습니다.SetMetadataSetMetadata는 타겟(클래스, 메서드)에 메타데이터를 마킹하는 데코레이터를 반환하는 함수입니다. NestJS의 코드를 보면 아래와 같습니다. setMetadata 코드export const SetMetadata = <K = string, V = any>(\n  metadataKey: K,\n  metadataValue: V,\n): CustomDecorator<K> => {\n  const decoratorFactory = (target: object, key?: any, descriptor?: any) => {\n    // method or class에 메타데이터 등록\n    Reflect.defineMetadata(metadataKey, metadataValue, class or method);\n    return target;\n  };\n  decoratorFactory.KEY = metadataKey;\n  return decoratorFactory;\n};Reflect.defineMetadata(metadataKey, metadataValue, class or method);SetMetadata 함수 내부에서는 *Reflect.defineMetadata 메서드를 통해 타겟 객체에 metadataKey를 키, metadataValue를 값으로 하는 내부 슬롯을 정의합니다. ([[Metadata]] )*Reflect 는 reflect-metadata 라이브러리가 설치되어있는 경우 사용할 수 있습니다. 메타데이터를 정의하거나 조회하는 데 사용합니다.SetMetadata(KEY, value) -> CustomDecorator;SetMetadata의 리턴값은 클래스, 메서드 데코레이터로 사용 가능합니다. 해당 데코레이터로 타겟 클래스나 메서드에 대한 메타데이터를 설정할 수 있습니다.const SOMETHING = Symbol('SOMETHING')\n\nfunction CustomDecorator(key: string | symbol) {\n  // SetMetadata(SOMETHING, key)와 다른 데코레이터를 합성할 수 있습니다.\n  return applyDecorators(SetMetadata(SOMETHING, key), AnotherDecorator)\n}\n\n@CustomDecorator('KEY1')\nclass DecoratedClass {}DecoratedClass에 SOMETHING 심볼을 메타데이터 키, 'KEY1'을 메타데이터 값으로 등록합니다.DiscoveryServiceNestJS는 DiscoveryModule 을 제공합니다. DiscoveryModule의 DiscoveryService에서는 내부적으로 modulesContainer를 사용하여 모든 모듈의 Controller와 Provider 클래스를 조회할 수 있습니다.DiscoverService를 사용하여 모든 Provider 클래스를 순회하며, SetMetadata로 등록했던 메타데이터 키로 특정 Provider를 필터링할 수 있게 됩니다.DiscoveryService 코드@Injectable()\nexport class DiscoveryService {\n  constructor(private readonly modulesContainer: ModulesContainer) {}\n\n  getProviders(\n    options: DiscoveryOptions = {},\n    modules: Module[] = this.getModules(options),\n  ): InstanceWrapper[] {\n    return modules.flatMap(item => [...item.providers.values()]);\n  }\n\n  // ...생략\n}CustomDecorator 가 붙은 메서드를 찾는 과정을 예로 들어보겠습니다. 메타데이터 키는 CUSTOM_DECORATOR  심볼이고, 메타데이터 값은 test-value 입니다.export const CUSTOM_DECORATOR = Symbol(\"CUSTOM_DECORATOR\");\nexport const CustomDecorator = SetMetadata(CUSTOM_DECORATOR, 'test-value');\n\n@CustomDecorator\n@Injectable()\nclass TestService {\n  test() {}\n}아래의 explorerService.find(CUSTOM_DECORATOR) 메서드를 실행하면 어떻게 될까요?import { Injectable } from '@nestjs/common';\nimport { DiscoveryService, MetadataScanner, Reflector } from '@nestjs/core';\n\n@Injectable()\nexport class ExplorerService {\n  constructor(\n    private readonly discoveryService: DiscoveryService,\n  ) {}\n\n  find(metadataKey: string | symbol) {\n    const providers = this.discoveryService.getProviders();\n\n    return providers\n      .filter((wrapper) => wrapper.isDependencyTreeStatic())\n      .filter(({ metatype, instance }) => {\n        if (!instance || !metatype) {\n          return false;\n        }\n        return Reflect.getMetadata(metadataKey, metatype);\n      })\n      .map(({ instance }) => instance);\n  }\n}첫번째 필터: filter((wrapper) => wrapper.isDependencyTreeStatic())request scope가 아닌 싱글톤 프로바이더만 필터링합니다.두번째 필터: Reflect.getMetadata(metadataKey, metatype)해당 필터는 메타데이터가 등록된 클래스만 필터링합니다.metatype 은 class TestService 와 같이 해당 Provider의 클래스를 의미합니다.Reflect.getMetadata(metadataKey, metatype) 은 metatype(클래스)에 metadataKey로 등록된 메타데이터의 값을 가져옵니다. TestService 클래스의 경우 메타데이터 키는 CUSTOM_DECORATOR 이고 값은 test-value 입니다.만약 등록된 메타데이터가 없으면 undefined를 반환하고 해당 Provider는 필터링됩니다.MetadataScanner앞의 DiscoverService의 예시에서는 데코레이팅된 메서드를 가진 인스턴스에 접근하는 데 그쳤습니다. 실제 데코레이팅된 메서드에 접근하기 위해서는 DiscoveryModule에서 제공하는 MetadataScanner 를 사용해야 합니다.MetadataScanner 코드export class MetadataScanner {\n  public scanFromPrototype<T extends Injectable, R = any>(\n    instance: T,\n    prototype: object,\n    callback: (name: string) => R,\n  ): R[] {\n    const methodNames = new Set(this.getAllFilteredMethodNames(prototype));\n    return iterate(methodNames)\n      .map(callback)\n      .filter(metadata => !isNil(metadata))\n      .toArray();\n  }\n\n  *getAllFilteredMethodNames(prototype: object): IterableIterator<string> {\n    // prototype에 등록된 method 이름들을 가져온다.\n\nscanFromPrototype 는 getAllFilteredMethodNames 메서드로 인스턴스의 모든 메서드 이름들을 가져와 인자로 받은 callback을 실행시킵니다. 이 중에서 메타데이터가 있는 메서드만 필터링합니다.scanFromPrototype 의 callback 파라미터에서 인스턴스 메서드에 접근할 수 있습니다. 이제 메서드에 접근해 데코레이팅 함수로 덮어씌울 수 있습니다.SetMetadata, DiscoveryService, MetadataScanner 모든 재료들이 모였으니 Provider에 접근 가능한 메서드 데코레이터를 만들어봅시다.Cacheable 데코레이터메서드에 CACHEABLE 심볼을 메타데이터 키로, ttl을 메타데이터 값으로 설정합니다.export const CACHEABLE = Symbol('CACHEABLE');\nexport const Cacheable = (ttl: number) => SetMetadata(CACHEABLE, ttl);\n\n@Injectable()\nclass TargetClass {\n  @Cacheable(0)\n  test() {}\n}CacheDecoratorRegister 클래스@Injectable()\nexport class CacheDecoratorRegister implements OnModuleInit {\n  constructor(\n    private readonly discoveryService: DiscoveryService,\n    private readonly metadataScanner: MetadataScanner,\n    private readonly reflector: Reflector,\n    private readonly cache: Cache,\n  ) {}\n\n  onModuleInit() {\n    return this.discoveryService\n      .getProviders() // #1. 모든 provider 조회\n      .filter((wrapper) => wrapper.isDependencyTreeStatic())\n      .filter(({ instance }) => instance && Object.getPrototypeOf(instance))\n      .forEach(({ instance }) => {\n        this.metadataScanner.scanFromPrototype(\n          instance,\n          Object.getPrototypeOf(instance),\n          (methodName) => {\n\t    // #2. 메타데이터 value\n            const ttl = this.reflector.get(CACHEABLE, instance[methodName]);\n            if (!ttl) {\n              return;\n            }\n\n            const methodRef = instance[methodName];\n\n            // #3. 기존 함수 데코레이팅\n            instance[methodName] = async function (...args: any[]) {\n              const name = `${instance.constructor.name}.${methodName}`;\n              const value = await this.cache.get(name, args);\n              if (value) {\n                return value;\n              }\n\n              const result = await methodRef.call(instance, ...args);\n              await this.cache.set(name, args, result, ttl);\n              return result;\n            };\n          },\n        );\n      });\n  }\n}해당 클래스를 모듈의 provider에 등록하면, onModuleInit 단계에서 @Cacheable로 데코레이팅된 메서드를 찾아 기존 메서드를 덮어씌웁니다.메서드 데코레이터를 만드는 과정은 다음과 같습니다.#1. 모든 Provider 클래스를 순회하며#2. 특정 메타데이터가 등록된 메서드를 찾아#3. 기존 메서드를 덮어씌웁니다.#3의 과정에서, CacheDecoratorRegister 생성자에 주입한 CacheManager를 사용할 수 있습니다.그런데 메서드 데코레이터를 만들 때마다 매번 이렇게 복잡한 과정을 거쳐야하는 걸까요? 저희 챕터에서는 메서드 데코레이터마다 반복되는 과정을 AopModule이라는 모듈로 해결했습니다.해당 모듈은 2022년 12월에 오픈소스로 공개되었습니다. 현재 npm에서 @toss/nestjs-aop 라이브러리를 다운 받아 사용해보실 수 있습니다.관련해서 NestJS 밋업에서 발표한 자료도 있으니 함께 참고하시면 좋을 듯 합니다. :)AopModuleAopModule이 데코레이터들을 등록하는 과정은 이렇습니다.간단히 설명하면Aspect 데코레이터가 붙은 클래스를 찾고 (CacheableDecorator)Cacheable 데코레이터가 붙은 함수를 찾아 (FooService.foo)1번 클래스의 wrap 함수로 2번의 함수를 감쌉니다. (CacheableDecorator.wrap)코드를 보며 좀 더 자세히 설명해볼게요.1. Aspect 데코레이터 사용Aspect 데코레이터import { applyDecorators, Injectable } from '@nestjs/common';\n\nexport const ASPECT = Symbol('ASPECT_CLASS');\n\nexport function Aspect() {\n  return applyDecorators(SetMetadata(ASPECT, 'ASPECT_CLASS'), Injectable);\n}데코레이터 사용@Aspect()\nexport class CacheLazyDecorator {}데코레이터 로직을 실행할 클래스에 ASPECT 라는 심볼을 메타데이터로 설정합니다.2. 데코레이터 생성export const CACHEABLE = Symbol('CACHEABLE');\nexport const Cacheable = (ttl: number) => SetMetadata(CACHEABLE, ttl);\n\n\nclass FooService {\n\t@Cacheable(1000)\n\tfoo() {}\n}특정 심볼(또는 문자열)을 메타데이터 키로 하여 SetMetadata로 원하는 데코레이터를 만듭니다.3. LazyDecorator 구현AopModule에 등록되는 모든 데코레이터들은 LazyDecorator 인터페이스를 구현해야 합니다. 데코레이팅 하는 시점을 모듈이 초기화되는 시점으로 미루기 때문에 LazyDecorator라고 합니다.LazyDecorator 인터페이스export interface LazyDecorator {\n  wrap(reflector: Reflector, instance: any, methodName: string): Decorator | undefined;\n}CacheLazyDecorator 구현@Aspect()\nexport class CacheLazyDecorator implements LazyDecorator {\n  constructor(@Inject(CACHE_MANAGER) private readonly cache: CacheManager) {}\n\n  wrap(reflector: Reflector, instance: any, methodName: string) {\n    const ttl = reflector.get(CACHEABLE, instance[methodName]);\n    if (!ttl) {\n      return;\n    }\n\n    const methodRef = instance[methodName];\n    const name = `${instance.constructor.name}.${methodName}`;\n    return async (...args: any[]) => {\n      const value = await this.cache.get(name);\n      if (value) {\n        return value;\n      }\n\n      const result = await methodRef.call(instance, ...args);\n      this.cache.set(name, result, ttl);\n      return result;\n    };\n  }\n}접근하고자 하는 Provider는 이제 생성자에 주입하여 사용할 수 있습니다.4. AutoAspectExecutoronModuleInit 단계에서 AopModule의 AutoAspectExecutor 가 ASPECT가 붙은 데코레이터 클래스들의 wrap 함수를 실행시키며 기존 메서드를 덮어씌웁니다.AutoAspectExecutor 코드@Injectable()\nexport class AutoAspectExecutor implements OnModuleInit {\n  constructor(\n    private readonly discoveryService: DiscoveryService,\n    private readonly metadataScanner: MetadataScanner,\n    private readonly reflector: Reflector,\n  ) {}\n\n  onModuleInit() {\n    const providers = this.discoveryService.getProviders();\n    const lazyDecorators = this.lookupLazyDecorators(providers);\n    if (lazyDecorators.length === 0) {\n      return;\n    }\n\n    providers\n      .filter((wrapper) => wrapper.isDependencyTreeStatic())\n      .filter(({ instance }) => instance && Object.getPrototypeOf(instance))\n      .forEach(({ instance }) => {\n        this.metadataScanner.scanFromPrototype(\n          instance,\n          Object.getPrototypeOf(instance),\n          (methodName) =>\n            lazyDecorators.forEach((lazyDecorator) => {\n              const wrappedMethod = lazyDecorator.wrap(this.reflector, instance, methodName);\n              if (wrappedMethod) {\n                instance[methodName] = wrappedMethod;\n              }\n            }),\n        );\n      });\n  }\n\n  private lookupLazyDecorators(providers: any[]): LazyDecorator[] {\n    // this.reflector.get(ASPECT, metatype) 결과값이 존재하는 providers만 필터링\n  }\n}Provider에 접근 가능한 데코레이터를 만드는 과정을 다시 요약하면 이렇습니다.SetMetadata로 필터링할 클래스에 메타데이터를 등록하고DiscoveryService로 모든 Provider를 조회하며등록된 Metadata로 특정 클래스나 메서드를 필터링하여 원하는 작업을 하면 됩니다.Provider에 접근이 필요없는 경우 일반 메서드 데코레이터를 구현하면 될 것입니다. 하지만 메타데이터를 사용하는 NestJS 데코레이터를 일반 데코레이터와 함께 사용해도 괜찮을까요?결론부터 말하자면 둘을 함께 사용하면 예상치 못한 버그가 발생할 수 있습니다.일반 메서드 데코레이터를 사용하면 안되는 이유메타데이터를 등록하는 다른 데코레이터와 함께 쓰이는 경우, 기존 메서드가 덮어씌워지면서 프로토타입에 등록된 메타데이터가 사라질 수 있습니다.export function OnError(handler: (e: Error) => void) {\n  return (target: object, key?: any, descriptor?: any) => {\n    const originMethod = descriptor.value;\n    descriptor.value = (...args: any[]) => {\n      try {\n        return originMethod.call(this, ...args);\n      } catch (error) {\n        handler(error);\n      }\n    };\n  };\n}OnError 데코레이터는 기존 메서드를 새로운 메서드로 덮어씌웁니다.아래 코드에서는 메타데이터를 등록하는 RegisterMetadata 데코레이터와 OnError 데코레이터를 함께 사용하고 있습니다. 데코레이터 선언 순서에 따라 기존에 등록된 메타데이터는 사라질 수 있습니다.아래 메서드 중에 Reflect.getMetadata를 했을 때 메타데이터가 사라지는 메서드는 무엇일까요?@Injectable()\nclass TestService {\n  @OnError(console.log)\n  @RegisterMetadata('value')\n  test() {\n    throw new Error('error');\n  }\n\n  @RegisterMetadata('value2')\n  @OnError(console.log)\n  test2() {\n    throw new Error('error');\n  }\n}정답은 test 메서드입니다. 실행 결과는 타입스크립트 플레이그라운드에서 직접 확인하실 수 있습니다.const testService = new TestService()\n\n// undefined\nconsole.log('test metadata', Reflect.getMetadata(REGISTER_METADATA, testService.test))\n// value2\nconsole.log('test2 metadata', Reflect.getMetadata(REGISTER_METADATA, testService.test2))왜 이렇게 되는 걸까요? 데코레이터의 실행 순서가 힌트입니다.g∘f(x) = g(f(x)) 와 같은 합성 함수가 있을 때 선언은 g가 f보다 먼저 되었지만 실행은 f 함수가 먼저 실행됩니다. 마찬가지로 데코레이터는 평가될 때는 선언된 순서대로 위에서 아래로, 실행될 때는 아래에서 위로 실행됩니다.RegisterMetadata에서 Reflector.defineMetadata가 먼저 실행되고 그 다음 OnError 데코레이터가 기존 함수를 덮어씌웁니다.덮어씌워지면서 기존에 메타데이터가 저장된 프로토타입과 끊기게 되고 test 메서드에서 메타데이터를 찾을 수 없게 됩니다.이런 사례도 있을 수 있습니다.@Injectable()\nclass TossScheduler {\n\n  @OnError(console.log)\n  @Cron('*/10 * * * *')\n  task() {\n    // do something\n  }\n}@nestjs/schedule 의 Cron 데코레이터 역시 CRON 심볼을 메타데이터로 등록합니다. 모듈이 초기화되는 시점에 해당 메타데이터가 등록된 메서드들을 조회하여 cron job을 등록합니다.하지만 OnError 데코레이터가 Cron 데코레이터 이후에 실행됨으로써 메타데이터가 사라지게 되고, NestJS에서는 task 메서드를 찾지 못해 cron job을 등록하지 못하게 됩니다.이렇듯 일반 메서드 데코레이터를 NestJS 환경에서 그냥 사용하게 되면 개발자의 실수에 의해 코드의 동작이 바뀔 수 있습니다. 데코레이터 실행 순서나 메타데이터 환경에 대해 알고 있지 못하다면 이런 류의 버그를 찾는 데는 시간이 오래 걸릴 지도 모릅니다.이를 방지하기 위해서는 메타데이터를 고려하여 데코레이터를 생성해야 합니다.메타데이터를 유지하는 데코레이터메타데이터를 유지하는 가장 naive한 방법은, 오버라이딩 되기 전에 메타데이터를 저장해둔 뒤 오버라이딩이 끝나면 메타데이터를 다시 등록해주는 것입니다.OnErrorPreserveMeta 코드export function OnErrorPreserveMeta(handler: (e: Error) => void) {\n  return (target: object, key?: any, descriptor?: any) => {\n    const originMethod = descriptor.value;\n\n    //  오버라이딩 되기 전의 메타데이터를 저장해놨다가\n    const metaKeys = Reflect.getOwnMetadataKeys(descriptor.value);\n    const metas = metaKeys.map((k) => [\n      k,\n      Reflect.getMetadata(k, descriptor.value),\n    ]);\n\n    descriptor.value = (...args: any[]) => {\n      try {\n        return originMethod.call(this, ...args);\n      } catch (error) {\n        handler(error);\n      }\n    };\n\n    // 오버라이딩 된 메서드에 대해 메타데이터 재등록\n    metas.forEach(([k, v]) => Reflect.defineMetadata(k, v, descriptor.value));\n  };\n}직관적이지만 매번 Decorator를 만들어줄 때마다 이런 과정을 거쳐야 하는 게 불편합니다. 이를 해결하는 좀 더 간단한 방법이 있습니다.프로토타입을 사용해 메타데이터 유지하기SetMetadata 파트에서 Reflect.defineMetadata 는 타겟 객체에 [[Metadata]] 라는 내부 슬롯을 정의한다고 말씀드렸습니다.내부 슬롯 또한 프로토타입의 내부 프로퍼티이니, 기존 프로토타입에 메타데이터 내부 슬롯이 저장되어있을 것입니다. 따라서 새롭게 정의한 메서드에 기존 프로토타입을 연결해주면 됩니다.변경된 OnErrorPreserveMeta 코드export function OnErrorPreserveMeta(handler: (e: Error) => void) {\n  return (target: object, key?: any, descriptor?: any) => {\n    const originMethod = descriptor.value;\n\n    const wrapper = (...args: any[]) => {\n      try {\n        return originMethod.call(this, ...args);\n      } catch (error) {\n        handler(error);\n      }\n    };\n\n    Object.setPrototypeOf(wrapper, originMethod); // 이 줄만 추가\n    descriptor.value = wrapper;\n  };\n}Object.setPrototypeOf(arg1, arg2) 은 arg1 객체의 프로토타입을 arg2로 설정합니다.기존 메서드를 덮어씌운 후 Object.setPrototypeOf(wrapper, originMethod)로 originMethod를 wrapper의 프로토타입으로 설정해주면 메타데이터가 유지됩니다.@Injectable()\nclass TestService {\n  @OnError(console.log)\n  @RegisterMetadata('value')\n  test() {\n    throw new Error('error');\n  }\n}\n\nconst testService = new TestService()\n\nconsole.log('test metadata', Reflect.getMetadata(REGISTER_METADATA, testService.test)) // 'value'메타데이터와 NestJS의 DiscoveryModule 을 사용하여 NestJS의 IoC 컨테이너에 접근할 수 있는 데코레이터, 그리고 메타데이터를 유지할 수 있는 데코레이터를 만들어보았습니다.메타데이터 태깅, DiscoveryModule, 프로토타입을 사용해 NestJS 환경에 맞는 데코레이터를 만들 수 있었습니다. 이 글을 통해 더욱 더 NestJS의 Aop 패턴에 맞는 프로그래밍을 하게 되었기를 바랍니다.또한 토스 Node.js 챕터는 토스의 다양한 제품과 라이브러리 개발을 위해 팀원들의 지속적인 성장이 중요하다고 믿으며, 이를 위해 코드 리뷰, 스터디와 엔지니어링 세미나 등을 통해 꾸준히 공부하고 공유하는 자리를 가지고 있으니 많은 관심 부탁드립니다.Referenceshttps://zuminternet.github.io/nestjs-custom-decorator/https://github.com/nestjs/nesthttps://github.com/nestjs/swagger/issues/217https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/setPrototypeOf재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "15": "토스증권 QA 문화 ‘통합테스트’를 아시나요? (feat. 해외주식)황채은ㆍQA Manager2022. 12. 12안녕하세요. 토스증권 QA팀에서 일하고 있는 QA Manager 황채은입니다.지난 글에서 잠깐 언급되었던 토스증권의 Closed Beta Test 기억하실까요? 🔗지난 포스팅 참고하기 링크오늘은 해외 증권 서비스 출시를 위해 토스증권 임직원 대상으로 진행한 Closed Beta Test 에 대해 소개하려고 합니다.토스증권 서비스는 어떤 테스트 과정을 통해 출시되고 있을까요?토스증권에서는 기능별로 Silo가 만들어져있고, 그 Silo에는 PO, Developer, PD, DA, QA가 함께 서비스를 만들고 있는데요.PO(Product Owner), 개발자(Front/Server/Core), PD(Product Designer), DA(Data Analyst), QA(Quality Assurance)개발이 완료된 서비스들은 1차로 개발자 단위 테스트를 진행 후 QA Manager는 알파 환경에서 계획한 테스트를 진행하고, 이슈 수정이 모두 완료되면 QA 환경에서 최종 확인된 기능에 대해 서비스 Release를 Silo 별로 진행하고 있습니다. 이런 과정은 일반적인 IT 회사의 QA Cycle과 비슷하게 진행하고 있어요.하지만, 토스증권에서는 내부 임직원들 대상으로 테스트 참여를 유도하여 미처 발견하지 못한 Edge Case 들과 보다 다양한 사용자의 피드백을 미리 얻고자  Closed Beta Test를 진행하기도 한답니다. 실제 제가 작년에 진행했던 토스증권의 해외 주식 서비스 런칭 경험을 소개해 드리려고 해요.토스증권에서 진행한 내부 임직원 테스트는 아래의 다양한 테스트 방식의 개념을 조금씩 섞어서 진행했어요.QA라면 모두가 알고 있는 탐색적 테스팅의 개념, Dog Fooding, Bug Bash를 모두 활용하였는데요. 그전에 개념들을 한번 언급하고 지나갈게요!Closed Beta Test란?비공개 베타 테스트로, 서비스를 정식으로 오픈하기 전에 프로그램의 버그를 찾거나 사용성, 요구사항 충족 등을 검증하기 위해 개발자와 관련되지 않은 사용자에 의해 진행되는 테스트탐색적 테스팅이란?테스트 대상 제품을 사용하면서 제품의 정보 습득과 동시에 테스트를 설계하고 실행하는 방식으로 탐색적 테스팅의 주요 구성요소는 Test Charter, Test Note, Time Boxing, Debrief가 있음Dog Fooding 이란?서비스 출시 전 사용자의 입장에서 내부 인원이나 개발자가 직접 사용하며 제품을 개선하는 것Bug Bash란?제품이 최종 릴리즈되기 전 다양한 직군의 인원들이 제품을 사용하며 아직 남아있을 수 있는 버그를 찾아내는 활동으로 짧은 시간에 많은 인원이 진행하기 때문에 상대적으로 빠르게 버그를 찾을 수 있음토스증권의 Closed Beta Test는 어떻게 진행되었을까요?업무시간에 임직원 대상으로 진행하는 만큼, 효율적으로 진행하기 위해 여러 준비과정이 필요했는데요.개개인이 자율적으로 참석 가능한 임직원의 신청을 받았는데, 개발자를 제외한 임직원 중에서 70% 이상이 참여할 정도로 많은 관심을 가져주셨어요.서비스에 대해 여러 피드백을 받기 위해 직군에 관계없이 참여가 가능한 분들은 모두 신청을 받았는데요. 그러다보니, 알파 환경 자체를 처음 접해보는 임직원분들도 있어 사용성에 어려움이 많아 사전 테스트 환경 준비가 중요했어요. 마치, 실제 서비스가 런칭되었을 때와 동일하게 사용하는 경험을 주기 위해 알파 테스트 환경이지만 운영과 동일한 환경에서 볼 수 있도록 테스트 환경을 마련하였는데, 모두의 시간을 할애하는 만큼 테스트 환경을 위해 InfraOps, Network Engineer, Devops 팀에서 테스트 환경 구축에 많은 도움을 주셨어요.신청자 대다수가 토스증권 해외 주식에 대한 기본적인 Spec을 전혀 모르기 때문에 테스트 시나리오를 제공해야했어요. 그래서, 탐색적 테스팅의 Test Charter 개념을 섞어서 Test Scenario를 Use Case 형식으로 작성했는데요. 서비스의 주요 퍼널을 통과할 수 있게 작성하고 QA Team의 Peer Review를 통해 시나리오를 준비하였어요.시나리오는 총 36건 작성을 하였고, 해외 증권 거래소 시간에 맞게 테스트를 진행해야 해서, Use Case를 테스트할 수 있는 Timeline을 설정했어요. 해외 증권 거래소의 테스트 거래소와 연동을 하여 실제 주식이 체결되는 것처럼 보이게 환경을 만들어 두었기 때문에, 먼저 증권 거래소 운영 시간에 맞추고 주식의 매수/매도 유형에 맞춰 시나리오를 만들었는데요. 그래서 해외 주식 서비스는 밤 11시가 넘은 심야에도 계속 진행되어야 했어요.해당 Timeline에 맞게 임직원 모두가 같은 시간대에 테스트를 하는 것을 유도하기 위해 캘린더 초대도 잊지 않았어요! (참석자 모두, 심야 시간인데도 불구하고 수락을 눌러주셨어요!)이렇게 준비된 테스트 형식을 모두가 이해하기 쉽게 “통합테스트” 라는 용어로 정리하였어요!이제 마지막으로 테스트 진행 관련된 사항을 어떻게 공유할지, 어떻게 Debrief 할지를 정해야 했는데요.토스증권은 업무를 보다 빠르고 효율적으로 진행할 수 있도록 만들어진 다양한 사내 툴들이 있어요.대표적으로 Slack에서 JIRA(Bug Tracking System)를 바로 연동하여 이슈를 생성하고 완료까지 처리할 수 있는 (Bot)이 있어요. 또 Slack의 Workflow에서 Emoji를 남기면, 해당 게시글이 다른 채널로 전달되는 기능을 활용해서 보다 편하게 이슈를 모아볼 수 있었어요.글로 설명하려니 이해가 잘 안되시죠?그러면, 이제 저와 함께 통합테스트를 함께 진행해 볼까요?완료 후 미션에 DONE Emoji를 남기면 Thread에 완료 표기미션 중 이슈로 인해 진행하지 못하는 경우 SOS Emoji를 남기면 Thread에 표기이슈에 Emoji를 남기면 별도 채널로 이동되고, 해당 채널에 이슈 등록이런 미션의 형태로 진행하니, 임직원들도 게임에 참여하듯이 (미션 클리어! 효과음이 들리시나요?) 즐겁고 적극적으로 진행해 주셨어요. 또한 테스트를 진행하는 동안 이전에 사용하였던 해외증권 서비스의 경험을 비롯한 다양한 의견들을 제시해 주셨어요.그렇게 통합테스트 기간 동안 약 100개가 넘는 피드백이 등록되었는데요. 👀전달된 의견들은 실제로 Bug나 결함보다는 사용자로서의 의견에 더 가까운 내용들이 많았어요. 제시해 주신 여러 의견들을 통해 오픈 전 서비스의 사용성 및 품질을 더 높일 수 있었다고 생각해요.3일이라는 짧다면 짧고 길다면 긴 기간 동안 모두가 즐겁게 테스트에 참여해 주셨어요.서비스 출시 전, 통합테스트는 어떤 의미였을까요?임직원 대상으로 진행한 통합테스트는 QA팀에서 QA Plan에 맞게 여러 테스트 Iteration을 수행하고, 개발 Side의 단위 테스트, 성능 테스트가 진행이 되었다 하더라도 우리가 만든 서비스가 만든 의도에 맞게 동작하는지에 대해 미리 임직원 대상으로 실험할 수 있었어요. 우리가 만들었던 기능 중에서 우려했던 부분 역시 다시 드러나면서 개선이 더 필요한 부분에 대해 Maker 들과 공감대 형성에 가장 큰 역할이 되었습니다.또한, 우리가 사용자에게 의도한 대로 동작하는 부분이 제대로 동작하는지까지 미리 알 수 있었고 그 장치들을 더 효과 있게 바꾸는 계기도 되었어요.토스증권 QA팀에서는 정량적인 서비스 품질뿐만 아니라, 회사에서 품질을 생각하는 문화에 대해서도 모두가 참여하도록 유도하고 서비스 품질에 대한 정성적인 기준에 대해 다 같이 고민하는 시간을 만들어가는 과정에도 주도적으로 참여해야 한다고 생각합니다.그래서 bugbash, closed beta testing, 탐색적 테스팅 등과 같이, QA라면 흔히 알고 있는 이런 여러 테스트 방식 중에서 필요한 부분으로 토스증권만의 ‘통합테스트’ 라는 컨셉을 만들었어요. 또한, 임직원 70% 이상이 참여하도록 만들었고 그 결과 또한 성황리에 마무리할 수 있었어요.이후로 임직원과 함께하는 통합테스트는 토스증권의 QA 문화로 자리매김할 수 있었고, 지금도 큰 서비스가 출시될 때마다 통합테스트를 주기적으로 진행하고 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "16": "똑똑하게 브라우저 Polyfill 관리하기박서진ㆍFrontend Developer2023. 1. 21토스 앱은 넓은 범위의 기기를 지원하면서도 현대적인 JavaScript를 이용해서 개발되고 있습니다. 그렇지만 최신 JavaScript를 오래된 브라우저 위에서 실행하기 위해서는 “Polyfill” 문제를 해결해야 하는데요.이번 아티클에서는 Polyfill 문제가 무엇인지 알아보고, 토스에서 어떻게 똑똑하게 다루고 있는지 살펴보려고 합니다.Polyfill이란?오래된 버전의 브라우저에서는 현재 JavaScript가 당연하게 사용하고 있는 Promise나 Set 객체가 없는 경우가 있습니다. 편리한 Array.prototype.at() API는 Chrome 92 이상에서만 지원되기도 합니다.예를 들어서, 아래와 같은 코드는 최신 브라우저에서는 잘 동작하지만, 오래된 브라우저에서는 실패합니다. 객체나 메서드에 대한 구현이 없기 때문이죠.[1, 2, 3].at(-1);\n\nPromise.resolve(1);\n\nnew Set(1, 2, 3);이런 문제를 해결하기 위해서는 오래된 브라우저에서 없는 구현을 채워주어야 합니다. 이렇게 구현을 채워주는 스크립트를 Polyfill이라고 합니다. 대부분의 Polyfill은 아래와 같이 이미 브라우저에 포함되어 있는지 체크하고, 없으면 값을 채워주는 형태로 동작합니다.Array.prototype.at = Array.prototype.at ?? /* Array.prototype.at에 대한 자체 구현 */;\n\n위 스크립트를 실행한 이후에는, 오래된 브라우저에서도 안전하게 [1, 2, 3].at(-1) 코드를 실행할 수 있습니다.표준적으로 사용되는 Polyfill들은 core-js 리포지토리에 모여 있습니다. 아래 코드를 실행하면 대부분의 ECMAScript 표준 객체와 메서드를 오래된 브라우저에서도 사용할 수 있게 됩니다.import 'core-js/actual';Polyfill의 문제위와 같이 코드를 작성하면 폭넓은 브라우저를 지원할 수 있다는 장점이 있지만 문제가 하나 생깁니다. 불러와야 하는 JavaScript 코드가 많아진다는 점입니다. 실행해야 하는 Polyfill 스크립트가 많아질수록 사용자가 경험하는 웹 서비스의 성능은 나빠집니다.특히, 위와 같이 설정하면 최신 버전의 브라우저에서는 대부분의 ECMAScript 표준 객체와 메서드가 포함되어 있음에도 불구하고 불필요한 Polyfill 스크립트를 내려받아야 합니다. 꼭 필요한 Polyfill 스크립트만 선택적으로 불러올 수 있는 방법은 없을까요?첫 번째 방법: @babel/preset-env 사용하기이 문제를 해결하기 위해 사용할 수 있는 첫 번째 방법은 @babel/preset-env Smart Preset을 사용하는 것입니다. 이 Smart Preset은 이미 정의된 브라우저 목록에 따라서 자동으로 필요 없는 Polyfill을 제거해 줍니다.예를 들어서, 웹 페이지가 Internet Explorer 11을 지원해야 한다면 아래와 같이 babel.config.js 를 설정할 수 있습니다.module.exports = {\n  presets: [\n    ['@babel/preset-env', { targets: { ie: 11 } }],\n  ],\n  /* 그 외의 설정 */\n};\n\n이후에 동일하게 core-js/actual 을 import 하더라도 Internet Explorer 11에 필요한 Polyfill 목록만 포함되는 것을 확인할 수 있습니다. 총 221개의 Polyfill이 포함됩니다.// 입력 코드\nimport 'core-js/actual';// 출력 코드\nrequire(\"core-js/modules/es.symbol.js\");\nrequire(\"core-js/modules/es.symbol.description.js\");\nrequire(\"core-js/modules/es.symbol.async-iterator.js\");\nrequire(\"core-js/modules/es.symbol.has-instance.js\");\nrequire(\"core-js/modules/es.symbol.is-concat-spreadable.js\");\nrequire(\"core-js/modules/es.symbol.iterator.js\");\n// ... 계속 (총 221개의 Polyfill)Babel playgroundInternet Explorer 11을 지원 브라우저 목록에서 제외하면 훨씬 적은 25개의 Polyfill이 포함됩니다.module.exports = {\n  presets: [\n    ['@babel/preset-env', { targets: 'defaults, not ie 11' }],\n  ],\n  /* 그 외의 설정 */\n};// 입력 코드\nimport 'core-js/actual';// 출력 코드\nrequire(\"core-js/modules/es.error.cause.js\");\nrequire(\"core-js/modules/es.aggregate-error.cause.js\");\nrequire(\"core-js/modules/es.array.at.js\");\nrequire(\"core-js/modules/es.array.includes.js\");\nrequire(\"core-js/modules/es.object.has-own.js\");\nrequire(\"core-js/modules/es.regexp.flags.js\");\nrequire(\"core-js/modules/es.string.at-alternative.js\");\nrequire(\"core-js/modules/es.typed-array.at.js\");\nrequire(\"core-js/modules/esnext.array.find-last.js\");\n// ... 계속 (총 25개의 Polyfill)\n\nBabel playground이렇게 @babel/preset-env에 브라우저 지원 범위를 설정하면 Polyfill을 안정적으로 포함하면서 스크립트의 크기를 감축할 수 있습니다.두 번째 방법: User-agent에 따라 동적으로 스크립트 생성하기Babel을 올바르게 설정함으로써 포함되는 Polyfill 스크립트의 크기를 줄일 수 있지만, 최신 버전의 브라우저에서 불필요한 스크립트를 내려받게 되는 문제는 동일합니다. 예를 들어서, Chrome 최신 버전은 문제없이 [1, 2, 3].at(-1) 을 실행할 수 있지만, 관련한 Polyfill 스크립트를 내려받습니다.이 문제를 해결하는 또다른 방법은 브라우저의 User-agent에 따라서 동적으로 Polyfill 스크립트를 생성하는 것입니다.예를 들어서, Financial Times에서 관리하고 있는 polyfill.io 서비스에서는 https://polyfill.io/v3/polyfill.min.js 라고 하는 경로로 동적인 Polyfill 스크립트를 제공합니다.최신 버전의 Chrome에서 해당 경로에 접속하면, 아무 Polyfill 스크립트도 내려오지 않는다는 것을 알 수 있습니다.$ curl -XGET \"https://polyfill.io/v3/polyfill.min.js\" \\\n   -H \"User-Agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Mobile Safari/537.36\" \\\n   -v\n/* 빈 스크립트 */반대로, Internet Explorer 11에서 실행하면 많은 양의 Polyfill 스크립트가 내려온다는 것을 알 수 있습니다.$ curl -XGET \"https://polyfill.io/v3/polyfill.min.js\" \\\n   -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko\" \\\n   -v\n(function(self, undefined) {!function(t){t.DocumentFragment=function n(){return document.createDocumentFragment()\n\n이렇게 User-agent에 따라 동적으로 Polyfill 스크립트를 생성하면 최신 브라우저에서는 아무 Polyfill도 내려주지 않고, 오래된 브라우저에서는 필요한 Polyfill 만 내려줄 수 있게 됩니다. ✨  브라우저가 꼭 필요한 Polyfill 스크립트만 내려받을 수 있는 것이죠.자체 Polyfill 서비스 구축하기토스에서는 polyfill.io 서비스를 그대로 사용할 수도 있었지만, Financial Times가 제공하는 Polyfill 중 일부가 ECMAScript 표준대로 작동하지 않아 오류가 발생한 경험이 있어서 자체적으로 구현했습니다.core-js 와 core-js-compat, browserslist-useragent 라이브러리를 사용하면 손쉽게 동적인 Polyfill을 제공하는 Node.js 서버를 만들 수 있었습니다.먼저, User-agent에 따라서 필요한 core-js polyfill 목록을 계산하기 위해서 아래와 같은 getCoreJSPolyfillList 함수를 작성할 수 있습니다.import { resolveUserAgent } from 'browserslist-useragent';\nimport compat from 'core-js-compat';\n\n/**\n * userAgent에 따라 필요한 Polyfill의 목록을 반환합니다.\n * e.g. ['es.symbol', 'es.symbol.description', 'es.symbol.async-iterator']\n */\nfunction getCoreJSPolyfillList(userAgent: string) {\n  try {\n    const result = resolveUserAgent(userAgent);\n    const majorVersion = parseMajorVersion(result.version);\n\n    return compat({\n      targets: `${result.family} >= ${majorVersion}`,\n      version: coreJSVersion,\n    }).list;\n  } catch {\n    // 일반적이지 않은 User-Agent인 경우\n    return compat({\n      targets: 'IE >= 11',\n      version: coreJSVersion,\n    }).list;\n  }\n}\n\nfunction parseMajorVersion(versionString: string) {\n  const match = versionString.match(/^(\\\\d+)\\\\.*/);\n\n  if (match == null) {\n    return versionString;\n  }\n\n  return match[1];\n}\n\n이제 필요한 Polyfill 리스트를 하나의 스크립트로 만들면 됩니다. 토스에서는 esbuild 를 이용하여 core-js 스크립트를 하나로 이어붙이는 방법을 선택했습니다.import { build } from 'esbuild';\n\n/*\n * userAgent에 맞는 완성된 Polyfill 스크립트를 생성한다.\n */\nasync function buildPolyfillScript(userAgent: string) {\n  const script = getCoreJSPolyfillScript(userAgent);\n\n  const result = await build({\n    stdin: {\n      contents: script,\n      loader: 'js',\n    },\n    target: 'es5',\n    bundle: true,\n    minify: true,\n    write: false,\n  });\n\n  return result.outputFiles[0].contents;\n}\n\nfunction createCoreJSPolyfillScript(userAgent: string) {\n  return getCoreJSPolyfillList(userAgent)\n    /* 실험적인 esnext 기능은 제외합니다. */\n    .filter(x => !x.startsWith('esnext.'))\n    .map(item => `import \"core-js/modules/${item}\";`)\n    .join('\\\\n');\n};\n\n이제 이 함수를 Node.js 서버에 포함시키거나, Lambda@Edge, Compute@Edge 와 같은 Edge Runtime에 포함하면 손쉽게 나만의 Polyfill 서버를 띄울 수 있습니다.마치며토스팀에서는 자체 제작한 Polyfill 시스템을 이용하여 최신 JavaScript API는 마음껏 활용하면서도 오래된 버전의 브라우저도 빠짐없이 지원할 수 있었습니다.글을 마무리하면서, 글의 내용을 요약해보자면 아래와 같습니다.Polyfill이란 신규 JavaScript API를 오래된 버전의 브라우저에서도 사용할 수 있도록 하는 방법입니다. 그렇지만, Polyfill 스크립트가 많아지면 웹 성능이 나빠집니다.Babel의 @babel/preset-env 스마트 프리셋을 이용하여 포함할 Polyfill 스크립트의 범위를 지정할 수 있습니다. 다만, 이 경우에도 최신 브라우저는 오래된 브라우저를 위한 Polyfill을 내려받습니다.User-agent에 따라 동적으로 Polyfill 스크립트를 생성할 수 있습니다. 이로써 최신 브라우저에서 내려받는 Polyfill 스크립트를 거의 없게 만들 수 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "17": "선언적인 코드 작성하기박서진ㆍFrontend Developer2023. 3. 16선언적인 코드(Declarative Code)는 프론트엔드 개발을 하다 보면 자주 만나게 되는 개념입니다. 특히 React 생태계에서 웹 서비스를 개발하다 보면 선언적인 코드에 대해 고민하게 되는데요. 이번 아티클에서는 토스 프론트엔드 챕터에서 생각하는 선언적인 코드란 무엇인지, 그리고 실제로 어떻게 선언적인 코드를 작성하는지 공유해드리려고 합니다.선언적인 코드토스 프론트엔드 챕터에서는 선언적인 코드를 “추상화 레벨이 높아진 코드”로 생각하고 있습니다. 예를 들어서, 아래와 같이 주어진 배열의 합을 구하는 함수 sum 을 생각해봅시다.sum([1, 2, 3]);sum 함수는 아래와 같이 for 문으로 구현할 수 있습니다.function sum(nums: number[]) {\n  let result = 0;\n\n  for (const num of nums) {\n    result += num;\n  }\n\n  return result;\n}여기에서 sum 함수는 초기값이 0이고, 배열이 가지고 있는 각각의 원소를 순회하면서 결과값에 더하는 작업을 추상화합니다. 덕분에 sum 을 다루는 사람은 복잡한 제어 흐름을 이해할 필요 없이, “배열의 합을 구한다” 라고 하는 동작에 집중하여 함수를 사용할 수 있습니다.토스는 이렇게 동작에 집중하여 추상화된 sum 함수를 선언적인 코드로 생각하고 있습니다.여기에서 한 걸음 더 나아가서 sum 함수 내부의 for ... of 문을 살펴봅시다.for (const num of nums) {\n  /* 동작 ... */\n}이 제어 흐름도 선언적인 코드로 볼 수 있습니다. 배열이 가지고 있는 각각의 요소를 순회하는 동작을 추상화하고 있기 때문입니다.실제로 ECMAScript 표준에 따라서 for ... of 가 추상화하는 로직을 그대로 드러내면 아래와 같이 나타낼 수 있습니다.const iterator = nums[Symbol.iterator]();\n\nlet step;\n\nwhile (!(step = iterator.next()).done) {\n  const num = step;\n  /* 동작 ... */\n}위와 같이, for ... of 문은 Iterator를 생성하고, Iterator가 끝날 때까지 다음 요소를 차례차례 가져오는 작업을 “각각의 요소를 순회하는 작업”으로 추상화합니다. 이런 관점에서 봤을 때, for ... of 문은 선언적인 코드입니다.실제로는 생성된 Iterator를 삭제하는 동작도 존재하므로 보다 추상화되는 로직이 많습니다.코드의 관점을 벗어나면 보다 재미있는 예시를 생각할 수 있습니다.“왼쪽으로 10걸음 걸어라” 라고 하는 말을 생각합시다. 여기에서“왼쪽”은 “북쪽을 바라보았을 때 90도 돌아간 위치” 를 추상화한 것입니다.“90도”는 “한 번의 회전을 360등분한 각의 90배만큼 시초선에 대해 시계 반대 방향으로 돌아간 것” 을 추상화한 것입니다.“시계 방향” 의 정의는 “북반구에서 해시계의 바늘이 돌아가는 방향” 을 추상화한 것입니다.그래서 “왼쪽으로 10걸음 걸어라”는 사실 “북쪽을 바라보았을 때 한 번의 회전을 360등분한 각의 90배만큼 북반구에서 해시계의 바늘이 돌아가는 방향으로 돌아서, 동물이 육상에서 다리를 이용해 움직이는 가장 빠른 방법보다 느린, 신체를 한 지점에서 다른 지점으로 옮겨가는 행위를 10번 반복해라” 라는 말을 추상화한, 선언적인 말로 볼 수 있을 것입니다.좋은 선언적인 코드 작성하기위에서 선언적인 코드를 추상화 레벨이 높아진 코드로 살펴보았습니다. 그런데 선언적인 코드는 항상 좋은 것일까요? 토스에서는 추상화가 항상 좋은 것은 아닌 것처럼 선언적인 코드도 잘 쓰는 것이 중요하다고 생각합니다.토스에서는 좋은 코드를 판단하는 제1원칙을 “수정하기 쉬운 코드”라고 생각합니다. 비즈니스 요구사항은 항상 빠르게 변하기 때문에, 개발자가 기민하게 대응하는 것이 중요하기 때문입니다. 그러면 선언적인 코드가 언제 수정하기 쉽고, 언제 그렇지 않은지 살펴봅시다.먼저 아래와 같은 회원가입 폼 컴포넌트를 살펴봅시다.<SignUpForm\n  onSubmit={result => {\n    /* 회원가입 결과에 따라서 특정 동작 수행 ... */\n  }}\n/>위 컴포넌트는 회원가입 로직을 하나의 컴포넌트로 추상화했기 때문에 선언적인 컴포넌트로 볼 수 있습니다.이 코드는 수정하기 쉬울까요?먼저 회원가입 폼을 여러 곳에서 사용한다면 각각의 폼을 중복해서 개발할 필요 없이 한 번만 개발하면 되기 때문에 효율적일 것입니다. 또한 회원가입 폼에 변경이 생긴다고 하더라도, 한 곳에서만 바꾸면 다른 화면들에 모두 반영되기 때문에 빨리 수정할 수 있을 것입니다.수정하기 어려운 지점은 없을까요?화면마다 SignUpForm 이 조금씩 다르다면, 공통화된 것이 오히려 코드의 복잡함을 가져올 수도 있습니다. 예를 들어서, 어떤 페이지에서는 SNS 회원가입을 일반 이메일 회원가입보다 먼저 보여줘야 할 수 있습니다. 또, 다른 페이지에서는 텍스트나 스타일을 조금씩 다르게 보여줘야 할 수 있습니다.아래와 같이 SignUpForm 에서 바뀔 수 있는 부분이 많다면, 내부 구현과 인터페이스도 복잡해지고, 쓰는 쪽에서도 불편할 것입니다.<SignUpForm\n  signUpOrder={['sns', 'normal']\n  title=\"사이트에 어서 오세요\"\n  subtitle=\"먼저 회원가입을 해주세요.\"\n  primaryButtonColor={colors.blue}\n  secondaryButtonColor={colors.grey}\n  /* 많은 Prop 들 ... */\n  onCancel={/* ... */}\n  onSubmit={result => {\n    /* 회원가입 결과에 따라서 특정 동작 수행 ... */\n  }}\n\n이처럼 토스에서는 선언적인 코드가 항상 좋은 것이 아니라, 앞으로 제품이 어떻게 변화할지, 비즈니스 요구사항이 어떻게 되는지에 따라서 달라질 수 있다고 생각하고 있습니다. 앞으로 코드의 어떤 부분이 수정될지 예측하고, 이에 따라 적절한 선언 레벨을 따르는 코드를 작성할 필요가 있습니다.토스의 선언적 라이브러리그렇다면 다양한 상황에서 일반적으로 사용할 수 있는 좋은 선언적 코드는 없을까요? 이번 아티클은 토스 프론트엔드 챕터가 100개가 넘는 서비스들에서 자주 사용하고 있는 선언적 라이브러리에 대해서 소개하고 마무리하려고 합니다.useOverlay토스에서는 BottomSheet, Dialog, Toast와 같이 화면 위에 뜨는 오버레이를 띄워야 하는 상황이 많습니다. 토스는 이렇게 오버레이를 띄우는 동작을 추상화하여 useOverlay 라고 하는 Hook 을 사용합니다.const overlay = useOverlay();\n\n<button\n  onClick={() => {\n    overlay.open(({ isOpen, close }) => {\n      return (\n        <BottomSheet open={isOpen} onClose={close}>\n          나는 바텀시트야\n        </BottomSheet>\n      );\n    })\n  }}\n>\n  바텀시트 열기\n</button>예를 들어서, 위 코드에서는 바텀시트 열기 버튼을 누르면 나는 바텀시트야 라고 하는 바텀시트를 띄웁니다.useOverlay 가 없었더라면 아래와 같이 제어 흐름이 드러나는 코드를 작성했어야 할 것입니다.const [isSheetOpen, setIsSheetOpen] = useState(false);\n\n<button onClick={() => setIsSheetOpen(true)}>\n  바텀시트 열기\n</button>\n<BottomSheet open={isSheetOpen} onClose={() => setIsSheetOpen(false)}>\n  나는 바텀시트야\n</BottomSheet>\n\nuseOverlay에 대한 자세한 정보는 Slash libraries의 useOverlay Hook을 참고해주세요.ImpressionArea토스 앱에서는 어떤 영역이 보여졌는지/숨겨졌는지에 따라서 동작하는 로직이 많습니다. 예를 들어서, 사용자가 특정한 요소를 보면 폭죽을 터뜨리거나 토스트를 보여주는 식이죠. 토스에서는 ImpressionArea 라고 하는 컴포넌트로 이를 추상화하고 있습니다.<ImpressionArea onImpressionStart={() => { /* 보여졌을 때 실행 */ }}>\n  <div>내가 보여졌으면 onImpressionStart가 실행돼</div>\n</ImpressionArea>\n\nImpressionArea 가 없었더라면, 복잡한 IntersectionObserver API 를 사용하거나, 복잡한 Scroll 이벤트 핸들러 로직을 사용해야 했을 것입니다.ImpressionArea 에 대한 자세한 정보는 Slash libraries의 ImpressionArea 컴포넌트를 참고해주세요.LoggingClick토스에서는 데이터 주도 의사결정을 위해서 화면에 진입하는 사용자가 몇 명인지, 그 중 몇 명이 버튼을 누르는지를 기록하는 경우가 있습니다. 이런 누르는 동작에 대한 기록을 추상화하여 LoggingClick 컴포넌트를 사용하고 있습니다.<LoggingClick params={{ price }}>\n  <button onClick={buy}>사기</button>\n</LoggingClick>\n\n예를 들어서, 위 코드에서 사용자가 버튼을 누르면 “사기” 버튼에 대한 동작이 분석 시스템에 기록됩니다.LoggingClick 이 없었다면 아래와 같이 log 함수를 실행하는 것이 그대로 드러났어야 할 것입니다.<button\n  onClick={() => {\n    log({ title: '사기', price });\n    buy();\n  }}\n>\n  사기\n</button>\n\n재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "18": "tosspayments-restdocs: 선언형 문서 작성 라이브러리이준희ㆍServer Developer2023. 3. 22최소한의 코드로 문서 작성하기들어가며토스페이먼츠에서는 두 가지 장점 때문에 Spring REST Docs를 권장하고 있는데요. 첫 번째는 문서 작성 단계부터 API를 통합 테스트할 수 있다는 점, 두 번째는 인터페이스의 의도치 않은 변경을 감지할 수 있다는 점 때문입니다.하지만 Spring REST Docs에는 단점도 있습니다. 장황한 코드 때문에 가독성이 떨어지고, 코드 반복으로 인해 생산성이 떨어지는 아쉬움이 있었습니다. 이런 문제를 해소하기 위해 Kotlin DSL을 구현해서 반복적이고 지루한 Spring REST Docs 코드 작성에 필요한 노력을 줄이는 방법을 한규주님의 이전 글 'Kotlin으로 DSL 만들기: 반복적이고 지루한 REST Docs 벗어나기'에서 소개했었습니다.이번에는 나아가 더 높은 가독성, 더 최소화된 코드 중복, 세부 구현 및 의존성 은닉, 마지막으로 확장에 열려있는 특성을 갖춘 문서화 라이브러리 'tosspayments-restdocs'를 소개하고 개발 후기를 공유합니다.동일한 컨트롤러를 Spring REST Docs(좌측)와 tosspayments-restdocs(우측)로 문서화했을 때의 차이. 작성한 문서화 코드의 양이 크게 감소했다.다시 살펴보기: Spring REST Docs의 문제들Spring REST Docs 기반 문서화 코드의 아쉬운 점을 아래 코드를 통해 다시 한번 살펴볼게요.요청 필드 2개, 응답 필드 3개로 구성된 단순한 PUT 인터페이스지만, 장황하게 작성된 코드 때문에 전체 구조를 한눈에 파악하기 어렵습니다. 이런 구조의 코드는 처음 작성하는 비용이 많이 들 뿐만 아니라 유지보수 비용도 늘립니다. 위 코드의 문제점을 좀 더 구체적으로 살펴보겠습니다.1. 코드 중복먼저, 같은 내용이 반복적으로 명세에 포함되어 있습니다. 예를 들어 MockMvcResponse와 REST Docs Snippet을 만들기 위해 Path Variable, 그리고 Request Body Field 코드 중복이 발생했습니다.또한 Request Body값을 MockMvc에 전달하는 과정에서 샘플 데이터를 통해 필드의 타입 및 샘플이 추론 가능함에도 REST Docs Snippet에 불필요하게 다시 명세하고 있습니다.2. 불필요한 명령given(), prettyPrint(), then(), preprocessRequest(), preprocessResponse() 등의 메서드는 인터페이스 명세에 꼭 필요한 핵심이라 하기 어렵습니다. 문서 작성과 관계 없이 빌드에 필요하기 때문에 추가된 사항입니다.3. 인터페이스 명세 순서실제 HTTP 프로토콜에서는 Request Line → Request Header → Request Body → Response Line → Response Header → Response Body 순으로 페이로드가 만들어집니다.그러나 위의 예시 코드는 이러한 흐름에 맞춘 자연스러운 명세 작성 대신, MockMvc를 구성하는 기반 기술에 의존하는 명령을 나열하는데 집중하고 있습니다. 그래서 코드를 작성할 때나 읽을 때 모두 코드 블록을 왔다갔다 해야 하는 불편을 겪습니다.4. 기반 기술에 강한 의존기반 기술에 강하게 의존하고 있기 때문에 기반 기술에 브레이킹 체인지가 생기거나, 유지보수가 중단되면 대응이 어렵습니다.또, 이렇게 기반 기술에 많이 의존하게 되면 문서를 작성하는 모든 개발자가 기반 기술의 세부 항목을 학습해야 합니다. 가령 MultiPart API, Streaming API, Reactive API 등 다른 형태의 API를 문서화할 때마다 개발자는 사용할 MockMvc의 세부 기능을 각각 학습해야 합니다. 이런 방식은 문서화에 필요한 학습 비용을 높이고 지속적인 대응을 어렵게 만듭니다.위 코드를 tosspayments-restdocs 라이브러리를 사용해서 다시 작성한 코드 예시입니다.생성될 문서를 직관적으로 예측할 수 있고, 기반 기술을 완전히 추상화해서 코드가 절반 이하로 감소했습니다.tosspayments-restdocs를 적용한 결과이런 개선점을 만든 tosspayments-restdocs 라이브러리에는 선언형 프로그래밍, 타입 추론 개념이 녹아있는데요. 어떻게 적용됐는지 하나씩 설명해보겠습니다.선언형 프로그래밍https://developer.mozilla.org/en-US/docs/Glossary/Element선언적으로 코드를 작성하는 대표적인 사례는 HTML입니다. HTML에서는 표현하고자 하는 항목을 요소(Element)로 추상화하고, 요소의 태그(Tag), 속성(Attribute), 내용(Content)을 명세해서 최종 결과물을 만듭니다.같은 방식으로, tosspayments-restdocs에서는 API의 실제 형태(HTTP의 페이로드)를 있는 그대로 표현할 수 있도록 Documentation, Request Line, Request Header, Request Body, Response Body를 요소화했습니다. 이렇게 모든 문서화 항목을 요소로 만들었기 때문에 HTML을 작성하는 방식처럼 선언적으로 문서를 작성할 수 있습니다.선언적인 작성에는 이런 장점이 있습니다.명세의 내용에 집중하게 됩니다. 세부 기술 및 이를 위한 불필요한 명령 없이, 문서화 코드의 본질인 인터페이스 명세에 집중할 수 있습니다.꼭 필요한 정보만 명세할 수 있습니다. 꼭 필요한 항목만 요소의 속성으로 선언하고, 추가적인 속성들은 중괄호를 열어 표현하게 됩니다.읽기 쉬운 코드가 됩니다. 최종 결과물의 실제 형태가 그대로 녹아져 있으므로, 위에서 아래로 한번만 읽으면 결과를 파악할 수 있습니다.확장이 쉽습니다. 새로운 문서화 항목이 생기는 경우 요소를 새로 정의하면 됩니다. 기존 항목에 자식 항목이 새롭게 생기는 경우에도 마찬가지로 대응이 가능합니다.구현 방법에서 자유로워집니다. 각 요소를 어떻게, 무엇으로 렌더링 할 지를 모두 라이브러리에 위임해서 기반 기술의 변화나 결과물의 포맷 변화가 코드에 미치는 영향을 줄일 수 있습니다.선언형 프로그래밍 구현 - 함수와 확장 함수tosspayments-restdocs의 문서화 코드 진입점인 documentation 요소 함수를 살펴보겠습니다.documentation 함수 호출 예documentation 내부 구현(이해를 돕기 위해 단순화하였습니다)documentation은 문서의 이름(documentName)을 필수 속성으로, requestLine 등 세부 스펙 요소 추가적인 속성으로 취급하는 요소 함수로, 문서화의 시작(문서 스펙 정의)과 끝(문서 출력)을 담당합니다.앞서 꼭 필요한 항목만 요소의 속성으로 선언하고, 추가적인 속성들은 중괄호를 열어 표현한다고 했는데요. 이 개념을 코틀린으로 구현하면서 필수 속성은 함수의 파라미터로, 추가 속성은 람다 함수 파라미터로 표현했습니다. 요소의 모든 속성을 파라미터로 펼쳐두면 함수가 장황해지고 확장성이 떨어지기 때문입니다.추가 속성을 람다 확장 함수(Extension Function) 스코프 내에서 정의하도록 하면 스코프 내에서 일어나는 일들에는 함수가 관여하지 않습니다. 그래서 편의성을 확보하면서 확장성을 유지할 수 있고, 스코프를 계층화하여 도메인을 더욱 잘 표현할 수 있습니다.예를 들어 documentation 요소 함수의 람다 확장 함수(specCustomizer: DocumentSpec.() -> Unit) 스코프에서는 RequestLineSpec, RequestBodySpec, ResponseBodySpec 등을 세부 요소 스펙으로 품고 있는 DocumentSpec의 this가 주어집니다. 스코프 및 스코프에 this를 주입하는 이런 방식에는 다음과 같은 장점이 있습니다.스코프 단위로 특화된 기능을 제공할 수 있습니다. 예를 들어 requestLine 함수는 DocumentSpec의 세부 요소 스펙인 RequestLineSpec을 정의하기 위한 요소 함수입니다.구조화된 확장성을 갖습니다. 새로운 속성이 추가되었을 때 스코프 내에 함수나 프로퍼티를 추가하여 쉽게 대응할 수 있습니다. 또한 새로운 자식 요소를 갖게 되는 경우에도 동일한 방식으로 람다 확장 함수 스코프를 정의해 대응할 수 있습니다. documentation → requestBody → field 로 이어지는 nested scope 가 그 예입니다.타입 추론Spring REST Docs의 문제 중 하나는 필드의 타입을 REST Docs Snippet에 다시 명세하는 비효율적인 작성 방식입니다.Kotlin에서는 Inline Function 한정으로 Reified Type Parameter를 제공합니다. 타입 정보가 소거되는 일반적인 Generic Function의 Type Parameter와 달리, Reified Type Parameter의 경우에는 타입 정보가 소거되지 않아 라이브러리에서 접근할 수 있습니다.tosspayments-restdocs에서는 항상 문서화 요소가 샘플을 받게 강제하고, 샘플의 타입과 값을 내부 자료구조에 저장하도록 했습니다.sample이 reified T로 선언되어 타입 정보(T::class.java)에 접근이 가능합니다.타입 정보가 남아있다면 문서를 작성하는 개발자를 대신해 다양한 작업을 자동화 할 수 있습니다. tosspayments-restdocs에서는 타입 명세, 열거형 예시 작성, 포멧 명세 등에 타입 정보를 활용하고 잇습니다.타입 정보 활용 예(타입별 양식 자동생성) – 열거형은 엔트리 나열, 시간 타입은 타임 포멧을 반환타입 정보 활용 예(생성된 문서) – 열거형 타입으로부터 얻은 정보로 ResultType의 엔트리(SUCCESS, ERROR)가 자동생성 되었습니다.지금까지 문서를 최소한의 코드로 작성하면서 변화에도 더 유연하게 대처할 수 있는 tosspayments-restdocs 라이브러리와, 라이브러리에 녹인 생각들을 소개했습니다.tosspayments-restdocs는 토스페이먼츠의 다양한 팀에서 Spring REST Docs, kotlin-dsl-restdocs를 대신해 활용하며 생산성을 높이고 있습니다.개발자가 더욱 변하지 않고 가치있는 일에 집중할 수 있도록, 토스페이먼츠에서는 생산성을 높이기 위한 다양한 활동을 이어나가고 있습니다. 함께 고민하며 더욱 좋은 문화와 기술을 만들어나가면 좋겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "19": "ESLint와 AST로 코드 퀄리티 높이기전성ㆍFrontend Platform Engineer2023. 3. 31코딩 컨벤션을 일관적으로 유지하기일관적인 코딩 컨벤션을 가지면 코드를 읽기 쉬워지고, 안티패턴을 방지할 수 있습니다. 결과로 버그도 줄고, 코드를 쉽게 유지보수할 수 있죠.하지만 이것을 사람이 직접 적용하는 것은 한계가 있기 때문에, 여러 가지 정적 분석 도구를 활용하게 됩니다. JavaScript/TypeScript 코드베이스에서는 주로 ESLint를 통해 컨벤션과 맞지 않는 코드를 사전에 감지하게 되는데요. 이러한 정적 분석 도구를 이용하게 되면 코드 리뷰 등 사람이 직접 읽지 않아도 컨벤션과 다른 부분을 기계적으로 잡아낼 수 있습니다.이미 만들어진 규칙에서 오는 한계ESLint에서는 생태계 내 다양한 플러그인 등을 통해 많은 수의 자주 사용되는 코딩 컨벤션을 커버할 수 있습니다. 하지만 우리 회사의 컨벤션에 맞는 규칙이 없다면 어떨까요? 조직이 커지고 요구 사항이 변화하게 되면서 커뮤니티에서 만들어진 규칙만으로는 조직 내 사용례에 정확히 부합하지 않는 경우가 생깁니다. 사내 라이브러리 내 사용 방식에 대한 컨벤션을 정의하거나, 조직 내 컨벤션과 커뮤니티에서 통용되는 컨벤션이 다소 다를 수도 있죠.예를 들어 토스에서는 SSR을 통해 서버 사이드에서 React 렌더링을 한 뒤 애플리케이션에 제공해서 로딩 속도를 높이고 있는데요, 이로 인해 브라우저 환경에서 작동하는 코드가 서버 사이드에서 실행되면서 의도치 않은 버그를 유발하는 케이스가 있습니다. 이러한 경우 미리 브라우저에서 호환되지 않는 코드를 감지하여 사전에 오류를 예방할 수 있다면 큰 도움이 되겠죠.린터는 어떻게 규칙을 적용할까?이러한 제약 사항을 해결하기 위해서는 우리만의 ESLint 규칙을 정의할 수 있어야 합니다. 그런데 ESLint는 어떻게 코드에 대한 규칙을 만들고 적용하고 있을까요?예를 들어서, Production 환경에서 로그가 함부로 찍히지 않도록 console.log 사용을 제한하는 규칙을 만드는 상황을 생각합시다.간단하게 정규식으로 구현해보면 이런 형식이 될 것입니다.if (sourceCode.match(/console\\\\.log/) != null) {\n  console.log(\"console.log를 사용하면 안 돼요!\") \n}하지만 이 방법은 생각했던 만큼 잘 작동하지 않습니다.예를 들어서, 이 코드에서 console.log가 문자열 안에 있는지도 알 수 없습니다.const message = \"console.log()를 쓰지 마세요.\";\n\n<Button>console.log 로그 활성화</Button>주석 안에 있는지도 알 수 없죠.// console.log(…) 를 쓰지 마세요.이러한 작은 케이스들을 하나하나 대응할 수도 있지만, ESLint는 좀 더 강력한 방법을 사용합니다.AST에서 원하는 정보 찾아내기ESLint는 Abstract Syntax Tree(AST)를 이용해서 규칙을 정의하고 적용합니다.AST는 소스 코드를 읽어낸 뒤 각 코드에서 구문 정보를 정리하여 나타낸 트리 형태의 자료 구조입니다. 예를 들어서, console.log 함수 호출과, 문자열이나 주석 속의 console.log 를 구별할 수 있게 해 줍니다.AST의 상세한 구조는 파서마다 약간의 차이가 있지만, AST Explorer라는 도구를 사용하면 소스 코드를 넣었을 때 어떤 AST가 나오는 지를 쉽게 확인할 수 있습니다. 일례로 console.log() 을 acorn이라고 하는 파서에서 파싱을 시도하면 이런 AST를 얻을 수 있습니다.{\n  \"type\": \"ExpressionStatement\",\n  \"expression\": {\n    \"type\": \"CallExpression\",\n    \"callee\": {\n      \"type\": \"MemberExpression\",\n      \"object\": { \"type\": \"Identifier\", \"name\": \"console\" },\n      \"property\": { \"type\": \"Identifier\", \"name\": \"log\" }\n    },\n    \"arguments\": []\n  }\n}이와 다르게, 문자열에 포함되어 있는 console.log() 의 파싱을 시도하면 이런 AST를 얻을 수 있습니다.{ \n  \"type\": \"Literal\", \n  \"value\": \"console.log()\", \n  \"raw\": \"\\\"console.log()\\\"\" \n}함수를 호출하는 경우, CallExpression과 MemberExpression이 사용되고, 문자열 안에 있는 경우 Literal이 사용되는 것을 볼 수 있네요.여기서 얻은 정보를 바탕으로 acorn을 이용해 console.log를 감지하는 스크립트를 작성해볼 수 있습니다.import { Parser } from \"acorn\";\nimport { simple } from \"acorn-walk\";\n\nsimple(Parser.parse(sourceCode), {\n  CallExpression({ callee }) {\n    if (callee.object.name === \"console\" && callee.property.name === \"log\") {\n      console.log(\"console.log를 사용하면 안 돼요!\");\n    }\n  },\n});이렇게 acorn-walk 를 사용하면 CallExpression에 해당하는 console.log만 감지할 수 있습니다. 주석이나 문자열, 화이트스페이스에 관계없이 안전하게 소스코드를 분석할 수 있는 것이죠.ESLint에서 사용할 규칙 직접 정의하기ESLint는 espree라고 하는 파서를 통해 소스 코드를 파싱하고, 이 결과를 각 플러그인에서 순회하며 규칙을 실행합니다. 우리가 원하는 규칙을 직접 플러그인을 통해 정의하고, 실행할 수 있어요.Espree AST만 읽을 수 있다면 ESLint 규칙도 쉽게 만들 수 있습니다.토스에서는 소스 코드 내에서 HTTP 링크를 찾아 HTTPS 링크로 바꿔야 한다고 알려주는 ban-http 와 같은 규칙을 정의하고 있습니다. 이런 규칙을 어떻게 직접 정의할 수 있는지 알아볼까요?먼저 소스 코드 내 문자열이 Espree AST에서 어떻게 표현되는 지를 알아봐야 합니다. AST Explorer에서 상단의 파서 설정을 Espree로 변경해주면 이를 쉽게 알 수 있습니다.Literal 타입의 노드에서 value를 읽으면 문자열 내용을 알 수 있네요.{\n  \"type\": \"Literal\",\n  \"value\": \"http://toss.im\",\n  \"raw\": \"\\\"http://toss.im\\\"\"\n}이를 기반으로 아래와 같이 ESLint 규칙을 새로 정의할 수 있습니다.module.exports = {\n  meta: {\n    /* ... */\n  },\n  create: function (context) {\n    return {\n      Literal: function (node) {\n        if (typeof node.value !== \"string\") {\n          return;\n        }\n        if (node.value.indexOf(\"http://\") >= 0) {\n          context.report({ node, messageId: \"isHttpBanned\" });\n        }\n      },\n    };\n  },\n};위 코드는 Literal을 만났을 때, 그 Literal의 값이 “http://” 로 시작하는 문자열이면 에러를 리포트하는 코드입니다. 생각보다 복잡하지는 않죠?이렇게 작성된 규칙을 ESLint에 추가하면 개발자들이 개발 중 규칙에 맞지 않는 코드를 작성했을 때 이렇게 알려줄 수 있어요.토스에서 사용하는 여러가지 규칙들이를 바탕으로 토스에서는 여러 가지 ESLint 규칙을 만들어서 플러그인으로 배포하고, 이를 서비스에서 사용하여 코딩 컨벤션을 유지하고 있습니다. 몇 가지 사용하는 규칙들은 아래와 같은 규칙들이 있어요.토스 프론트엔드 챕터 내 맥락이 강한 규칙들사내 라이브러리 사용 시 deprecated된 API 사용 금지이전 토스 도메인 사용 금지외부 라이브러리 사용에 관련한 규칙들사용하지 않기로 한 패키지 사용 제한 (ban-axios, ban-lodash)훅 이름에서 한글 허용 (rules-of-hooks)SSR에서 사용 시 오류를 내는 라이브러리 사용 제한 (ban-ssr-unsafe-method)또한 ESLint 외에도 자체 제작한 도구를 통해 사용해 deperecated 된 API의 사용이나 중복된 코드를 감지하기도 해요.더 알아보기ESLint 플러그인을 만드는 방법과 ESLint API 자체에 대한 글은 ESLint 공식 문서에서 더 자세히 알 수 있어요.Create Plugins – ESLint – Pluggable JavaScript Linter재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기다른 글 둘러보기JSCodeShift로 기술 부채 청산하기2021. 05. 04",
    "20": "Spring Boot Actuator의 헬스체크 살펴보기양권성ㆍServer Developer2023. 4. 1뭐든 알고 쓰는 게 참 중요한 것 같습니다. 단순히 “지금은 잘 돌아가니까 문제 없다”는 접근은 문제가 발생하기 전까지는 문제를 방치하기 마련입니다.사용하는 기술이나 구조에 대해 끊임없이 질문을 던지고 탐구하는 과정은 토스팀 코어밸류 3.0 중 하나인 Question Every Assumption, 모든 기본 가정에 근원적 물음을 제기한다에도 부합하는 사례인것 같습니다. 이번 포스트에서는 제가 개발 과정에서 헬스 체크를 별다른 생각 없이 Spring Boot Actuator가 제공하는 기능을 사용하면서 겪은 이슈를 간략하게 설명해보겠습니다.들어가기에 앞서이 포스트는 작성 시점 기준에서 최신 Spring Boot GA(General Availability) 버전인 3.0.5 버전을 기준으로 설명합니다. 해당 버전의 하위/상위 버전에서는 기능이 미묘하게 다르게 동작할 수 있습니다. 2.x 버전에서도 큰 맥락에서는 동일한 동작을 보장하리라 추측되지만 본인이 사용하는 버전에 해당하는 자세한 내용을 찾아보시길 권장합니다.헬스 체크란?서비스의 고가용성(HA, High Availability), 고성능을 위한 부하 분산 등의 이유로 우리는 서버의 이중화(혹은 그 이상)를 하고, 앞에서 어떤 서버로 요청을 보낼지 라우팅 역할을 하는 로드 밸런서를 둡니다.로드 밸런서가 적절히 부하를 분산하여 A/B 서버 중 한 대에게 클라이언트의 요청을 보냅니다.하지만 아래와 같이 서버 한 대가 서비스 불가 상태라면 어떻게 해야할까요? 해당 서버에 요청이 들어가야할까요?혹은 대량의 트래픽이 들어올 것을 대비하는 등등의 이유로 서버를 증설해야 하는데 해당 서버가 관련된 소스코드를 로딩하고 있다면 어떻게 해야할까요? 이 때도 마찬가지로 해당 서버에 요청이 들어가야할까요?두 케이스 모두 해당 서버로 요청을 보내면 안 됩니다. 정상적인 서비스가 불가능해서 클라이언트의 요청을 수행할 수 없습니다. 장애를 유발하거나 해당 서버의 부하를 크게 증가시켜 오히려 장애를 더 심각하게 만들 수도 있습니다.따라서 로드 밸런서에서는 각 서버의 헬스 체크 API를 호출해서 해당 서버가 현재 서비스 가능한 상태인지 아닌지 주기적으로 점검합니다.헬스 체크 API 경로는 커스텀하게 설정 가능합니다.헬스 체크에서 서버에 문제가 발견되면 로드 밸런서는 해당 서버로 요청을 보내지 않게 됩니다.헬스 체크는 정상적으로 서비스가 가능한 서버에만 트래픽을 보내서 서비스의 고가용성을 확보하는 데 도움됩니다.Spring Boot Actuator의 헬스 체크Spring Boot Acutator를 의존성으로 추가하면 기본적으로 헬스 체크 엔드포인트가 활성화됩니다.Spring Boot 3.x 기준으로 헬스 체크 엔드포인트는 /actuator/health이고, 설정을 바꾸지 않아도 해당 엔드포인트로 접속하면 HTTP 200 상태 코드와 해당 서버의 상태가 Response Body로 응답됩니다.크롬 개발자 도구로 확인해본 Spring Boot Actuator의 헬스 체크 결과Spring Boot Actuator는 어떠 기준으로 서버의 헬스 체크를 할까요? 확인하려면 Health Information 문서를 살펴보면 됩니다. 해당 정보는 보안에 민감한 요소가 들어있을 수 있어서 퍼블릭하게 접근이 가능해서는 안 됩니다. 저는 로컬에서 간단하게 확인만 해보는 목적으로 application.yml(application.properties) 파일에 management.endpoint.health.show-details: always로 설정한 후에 다시 헬스 체크 결과를 확인했습니다.Auto-configured HealthIndicators(WebMVC 전용)와 Auto-configured ReactiveHealthIndicators(Webflux 전용)에 나열된 HealthIndicator(혹은 ReactiveHealthIndicator)는 Spring Boot Auto Configuration에 의해 자동으로 활성화되는데 관련된 의존성이 존재할 때만 활성화 되는 것들도 있습니다. 예를 들어, DataSourceHealthIndicator는 DataSourceHealthContributorAutoConfiguration에 의해 설정되는데 Spring Data JPA 같이 DataSource를 사용하는 의존성을 추가했을 때 활성화됩니다.그럼 코드레벨에서 각 (Reactive)HealthIndicator들이 어떻게 사용되는지 보겠습니다.먼저 /actuator/health에 접속한 뒤에 브레이크 포인트를 걸고 디버그 모드로 살펴보면 HealthEndpointSupport 클래스의 getAggregateContribution 메서드에서 각 HealthContributor(혹은 ReactiveHealthContributor)를 순회하면서 헬스 체크하는 코드를 보실 수 있습니다. (헬스 체크하는 코드에 있는 HealthIndicator 인터페이스는 HealthContributor 인터페이스를 상속받았습니다.)HealthEndpointSupport 클래스의 getCompositeHealth 메서드에서는 각 HealthIndicator로부터 수집한 상태를 바탕으로 현재 서버의 상태를 진단합니다.@Override\npublic Status getAggregateStatus(Set<Status> statuses) {\n    return statuses.stream().filter(this::contains).min(this.comparator).orElse(Status.UNKNOWN);\n}/**\n * {@link Comparator} used to order {@link Status}.\n */\nprivate class StatusComparator implements Comparator<Status> {\n\n    @Override\n    public int compare(Status s1, Status s2) {\n        List<String> order = SimpleStatusAggregator.this.order;\n        int i1 = order.indexOf(getUniformCode(s1.getCode()));\n        int i2 = order.indexOf(getUniformCode(s2.getCode()));\n        return (i1 < i2) ? -1 : (i1 != i2) ? 1 : s1.getCode().compareTo(s2.getCode());\n    }\n\n}SimpleStatusAggregator의 getAggregateStatus 메서드에서는 각 상태를 수집해서 하나의 Status로 반환하고 있는데 이 때 StatusComparator가 사용됩니다.defaultOrder.add(Status.DOWN.getCode());\ndefaultOrder.add(Status.OUT_OF_SERVICE.getCode());\ndefaultOrder.add(Status.UP.getCode());\ndefaultOrder.add(Status.UNKNOWN.getCode());\nDEFAULT_ORDER = Collections.unmodifiableList(getUniformCodes(defaultOrder.stream()));이 때 가장 중요한 것은 Status의 순서인데 SimpleStatusAggragtor의 static 생성자 블럭을 보게되면 위와 같은 순서로 추가하고 있고,public SimpleStatusAggregator() {\n    this.order = DEFAULT_ORDER;\n}별도의 순서를 주지 않은 기본 생성자는 defaultOrder에 추가한 순서를 사용하는 것을 볼 수 있습니다.getAggregateStatus는 Status 중에 가장 순서가 빠른(오름차순) 것 하나를 반환하게 되어있기 때문에 만약에 Down을 반환한 HealthIndicator가 하나라도 존재하면 서비스의 상태를 Down으로 생각해서 503을 반환하게 됩니다.헬스 체크에서 조심해야 하는 점Spring Boot Actuator 헬스 체크의 동작원리를 잘 모르고 사용하면 일어날 수 있는 문제를 설명하겠습니다.1. 의도치 않은 장애 발생각 서버에서는 서비스를 제공하는 서비스 DB와 데이터를 분석하는 로그 DB가 있다고 가정하겠습니다. 그리고 로그 DB에 적재하는 작업은 비동기로 별도의 스레드에서 처리하도록 작업을 해놨다고 가정하겠습니다. 로그 데이터 저장이 불가능하더라도 실시간 서비스에는 문제가 없도록 하기 위해서죠.이 때 만약 로그 DB에 작업을 해야해서 순단이 발생하거나 접속에 문제가 생긴다면 어떻게 될까요? 아래 정답을 확인하기 전에 1분 동안 한 번 생각해보시길 바랍니다.위에 Spring Boot Actuator의 헬스 체크는 여러 HealthIndicator가 수집한 상태를 토대로 서비스의 상태를 판단한다고 말씀드렸습니다. 그 순서를 차근차근 설명해보겠습니다.RoutingDataSourceHealthContributor에 의해 여러 DataSource의 헬스를 체크합니다.DataSourceHealthIndicator에 의해 서비스 DB의 상태를 체크했을 때는 UP이 반환됩니다.DataSourceHealthIndicator에 의해 로그 DB의 상태를 체크했을 때는 DOWN이 반환됩니다.수집한 상태들은 SimpleStatusAggregator에 의해 서비스 상태를 판단하게 되는데 아무런 순서 설정을 하지 않았으면 DOWN인 게 하나라도 있다면 DOWN이 반환됩니다.서비스의 상태가 DOWN(503)으로 판단됐기 때문에 로드 밸런서에서는 서버로 트래픽을 보내지 않게 됩니다.서비스 DB에 문제가 없음에도 불구하고 클라이언트의 요청은 처리되지 않고 장애가 발생합니다.우리는 분명 최대한 높은 가용성을 보장하기 위해 로그 DB의 장애가 전파되지 않도록 격리했음에도 불구하고 장애가 발생할 수 있습니다. 이를 해결하기 위해서는 아래와 같은 방법 등등이 있습니다.Spring Boot Actuator의 헬스 체크가 아닌 직접 헬스 체크 API를 구현할 수도 있습니다.HealthIndicator 중에 헬스 체크에 영향을 끼치지 않길 희망하는 것들은 비활성화 시키는 방법도 있습니다. (RDB를 예로 들자면 management.health.db.enabled: false(기본값 true)로 설정한다거나)문제가 되는 HealthIndicator 빈을 직접 생성해서 Auto Configuration의 동작을 오버라이딩 하는 방법 등등이 있습니다.다만 헬스 체크에 이런 저런 로직들이 들어간다는 것은 일반적으로 예측 가능하지 못할 수 있으므로 팀 내에 꼭 공유가 잘 되어야할 것입니다.2. 트러블 슈팅의 지연비슷한 상황으로, 예전에 API 서버에서 외부 의존성 중에 ES만 죽었는데, API 서버가 죽었다고 판단돼서 DOWN이 된적이 있었어요. 헬스 체크에서 detail 옵션을 키면, 상세하게 쭉 나오더라고요. 당시에 LB 통해서 접근이 안 됐는데, WAS는 개별로 접근했을 때는 문제가 없어 보여서 트러블 슈팅이 늦어졌었습니다.이는 실제 사내에서 비슷한 상황이 발생했을 때 트러블 슈팅이 지연된 사례입니다. Spring Boot Actuator 헬스체크의 동작원리를 정확히 이해했다면 ES(Elasticsearch) 서버가 죽었을 때 해당 서버의 헬스체크도 같이 죽게 된다는 걸 예측할 수 있습니다. (ElasticsearchRestClientHealthIndicator 혹은 ElasticsearchReactiveHealthIndicator가 ES 서버의 헬스체크를 해서 헬스체크 API 응답에 전체적으로 영향을 끼치기 때문에)하지만 헬스체크의 동작원리를 잘 모르면 우리가 장애를 격리했다고 생각한 시스템(위의 상황에서는 ES)에만 문제가 있는데 왜 장애가 발생하는지, 왜 도메인을 통해서 접근하면 접근이 안 되는지 상황 파악이 안 될 수 있습니다. 서버는 정상적으로 살아있고 부하도 없는 상황이라면 헬스 체크 API를 호출할 생각도 못 하고, 로드 밸런서의 버그인지부터 의심을 할 수도 있습니다. 이렇게 엉뚱한 포인트를 의심하게 되면 장애 상황은 계속 되고, 서버를 재시작해도 근본적인 문제를 해결(위 상황에서는 ES 서버의 복구)하기 전까지는 여전히 헬스 체크에 실패할테니 장시간 장애가 지속될 수도 있습니다.결국 각 서버 인스턴스마다 직접 헬스 체크 API를 호출해서 정상 응답을 받는지 확인해봐야하는데 여기까지 사고의 흐름이 다다르는데 너무 많은 시간 소요와 불필요한 리소스 낭비들을 초래하게 됩니다.마치며평상시에는 헬스 체크하면 그냥 200 OK만 응답하는 정말 심플한 API 수준으로만 생각하고 큰 신경도 쓰지 않았습니다. 근데 사소한 것에 한 번 데인 뒤로부터는 개발자가 왜 호기심이 많아야하는지 한 번 더 깨닫게 되었습니다. 그냥 단순히 돌아만가는 코드가 아닌 이 코드가 왜 그렇게 돌아가는지, 우리가 왜 이 기술을 선택하게 된 것인지, 끊임없이 고민하고 탐구하기 위해서는 강력한 호기심이 동기부여가 되기 때문입니다. 이러한 고민을 미리했다면 장애 상황을 미연에 방지할 수 있고, 장애 발생 이후에라도 이슈 분석을 통해 트러블 슈팅 능력도 크게 향상된다는 것을 다시 한번 깨닫게 되는 소중한 경험이었습니다.참고 링크Spring Boot Actuator DocsNHN Forward spring-boot-actuator documentation재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기다른 글 둘러보기Kotlin으로 DSL 만들기: 반복적이고 지루한 REST Docs 벗어나기2022. 04. 11에러 핸들링을 다른 클래스에게 위임하기 (Kotlin 100% 활용)2022. 05. 14tosspayments-restdocs: 선언형 문서 작성 라이브러리2023. 03. 22",
    "21": "Node.js url.parse() 취약점 컨트리뷰션표상영ㆍSecurity Researcher2023. 5. 12토스 보안기술팀(Security Tech)에서는 개발 서비스 외에도 서비스에서 사용하고 있는 프레임워크나 Third-party에 대한 취약점 연구도 수행하고 있습니다.이번 아티클은 Node.js의 Built-in API 중 하나인 url.parse() 의 Hostname Spoofing 취약점을 발견하고 안전한 코드로 패치될 수 있도록 컨트리뷰션 했던 과정을 다뤄보려 합니다.https://github.com/nodejs/node/pull/45011url.parse() 취약점 발생 원인Node.js의 url.parse()는 WHATWG URL API 가 아닌 자체적인 스펙으로 개발된 함수입니다.WHATWG URL API\nWHATWG는 Web Hypertext Application Technology Working Group의 약어로 국제 웹 표준화 그룹을 뜻하고, WHATWG URL API 는 국제 표준 스펙으로 URL(Uniform Resource Locator)을 다룰 수 있도록 제공되는 API를 말합니다.  WHATWG URL API 가 등장하기 전, URL 파싱 기능을 제공하기 위해 자체적으로 개발된 함수로 보이는데요. 표준 스펙이 아닌 자체적으로 해석하다보니 다른 파서(parser)들과 해석 결과가 달라지게 되고, 이 때문에 의도치 않은 코드흐름이 발생하게 됩니다.url.parse()에서는 hostname을 잘못된 방식으로 파싱하여 취약점이 발생하게 되었는데요. Node.js의 url라이브러리는 여기에서 확인할 수 있고, 취약점이 발생했던 부분은 아래의 getHostname() 함수입니다./* comment\n해당 취약점은 v19.1.0에서 패치되었습니다. \n아래 코드는 v19.1.0 이전 버전에서 확인할 수 있습니다.\n*/\nfunction getHostname(self, rest, hostname) {\n  for (let i = 0; i < hostname.length; ++i) {\n    const code = hostname.charCodeAt(i);\n    const isValid = (code >= CHAR_LOWERCASE_A && code <= CHAR_LOWERCASE_Z) ||\n                    code === CHAR_DOT ||\n                    (code >= CHAR_UPPERCASE_A && code <= CHAR_UPPERCASE_Z) ||\n                    (code >= CHAR_0 && code <= CHAR_9) ||\n                    code === CHAR_HYPHEN_MINUS ||\n                    code === CHAR_PLUS ||\n                    code === CHAR_UNDERSCORE ||\n                    code > 127;\n\n    // Invalid host character\n    if (!isValid) {\n      self.hostname = hostname.slice(0, i);\n      return `/${hostname.slice(i)}${rest}`;\n    }\n  }\n  return rest;\n}getHostname() 함수의 로직은 단순합니다. 반복문을 통해 전달된 값의 문자를 하나씩 가져온 뒤, 조건에 맞는 값을 구하는 로직인데요. 여기서 isValid 라는 특정 조건을 정의해두고, 현재 문자가 조건에 충족되지 않으면 해당 문자의 앞 인덱스까지 slice하여 그 문자열을 hostname으로 설정합니다. 그리고 그 뒤 문자들은 앞에 /를 붙여 경로(path)로 사용합니다.const isValid = (code >= CHAR_LOWERCASE_A && code <= CHAR_LOWERCASE_Z) ||\n                code === CHAR_DOT ||\n                (code >= CHAR_UPPERCASE_A && code <= CHAR_UPPERCASE_Z) ||\n                (code >= CHAR_0 && code <= CHAR_9) ||\n                code === CHAR_HYPHEN_MINUS ||\n                code === CHAR_PLUS ||\n                code === CHAR_UNDERSCORE ||\n                code > 127;isValid 의 조건을 정규식으로 표현해보면 /[a-zA-Z0-9\\.\\-\\+_]/u 와 같은데요. *ECMAScript기준 \n“hostname으로는 저 범위의 문자들만 올 수 있어!” 라고 설정해둔 것이죠.디버깅 코드getHostname() 함수에 디버깅 코드를 추가하고 확인해보면, 실제로 isValid 범위 밖 문자일 경우 hostname 파싱을 중단하고, 나머지 문자열은 앞에 / 를 붙여 경로(path)로 사용하는 것을 볼 수 있습니다.때문에, http://EVIL_DOMAIN*.toss.im 의 hostname이 EVIL_DOMAIN 이 되어버리고, Hostname Spoofing 취약점이 발생하게 됩니다.Hostname Spoofing\nHostname Spoofing은 시스템을 대상으로 Hostname을 속이는 해킹 기법을 말합니다.\nSpoofing은 ‘속이다’라는 사전적 의미를 갖고 있으며, 시스템을 대상으로 어떠한 정보를 속이는 해킹 기법을 Spoofing이라고 합니다.\nWHATWG URL API ↔ url.parse() 비교WHATWG URL API 의 hostname 파싱 결과를 보면 evil_domain*.toss.im 으로 Node.js와 다른 것을 확인할 수 있습니다.Reserved CharactersNode.js에서 이렇게 파싱하는 이유는 RFC(Request For Comments) 문서를 보면 알 수 있습니다. RFC 3986는 Standard URI Syntax 를 정의해둔 문서인데요.문서의 여러 항목 중 아래 2.2 Reserved Characters를 보면 그 이유를 찾을 수 있습니다.https://www.rfc-editor.org/rfc/rfc3986#section-2.2Reserved Characters 는 URI를 구성할 수 있는 문자 중 특수 목적을 갖고 사용할 문자들을 미리 예약해둔 것인데요. 예를 들면 port 구분자로 사용되는 :(colon) , path 구분자로 사용되는 /(slash) 가 예약된 문자인 것이죠. 따라서, *, !, $, :, # 와 같은 문자들은 hostname으로 사용할 수 없습니다.그 아래를 보면 사용해도 되는 Unreserved Characters 도 정의되어 있습니다.https://www.rfc-editor.org/rfc/rfc3986#section-2.3정의된 문자들을 보면 위에서 확인한 url.parse()의 isValid 조건과 비슷한 것을 알 수 있죠.취약점 악용 시나리오이러한 WHATWG URL API ↔ url.parse() 간 파싱 결과 차이는 서비스의 도메인 검증로직을 우회하는데 악용할 수 있습니다.// server.ts (exec command: ts-node server.ts)\n/* dependencise\n\texpress@^4.18.1\n\tts-node@^10.9.1\n\ttypescript@^4.3.2\n\tnode-fetch@2\n\t@types/node-fetch@^2.6.2\n*/\nimport express, { Request, Response, NextFunction } from 'express';\n\nconst node_fetch = require(\"node-fetch\");\nconst app = express();\n\napp.get(\"/image/resize\", async (req: Request, res: Response) => {\n\t// GET메소드로 url파라미터 입력 받음\n\tconst url = req.query.url as string; // [1]\n\t// WHATWG URL API를 이용해 hostname 파싱\n\tconst host = new URL(url).hostname; // [2]\n\n\t// 파싱한 hostname 검증 (example.com과 *.example.com일 경우에만 분기문 통과)\n\tif(host === \"toss.im\" || host.endsWith(\".toss.im\")) { // [3]\n\t\t// 검증된 hostname일 경우, node_fetch로 http request\n\t\tvar result = await node_fetch.default(url); // [4]\n\t\tvar requestUrl = result.url; // [5]\n\t\t// 파라미터로 입력된 url과 node_fetch로 실제 요청한 url 콘솔 출력\n\t\tconsole.log(`Input URL: ${url} / Request URL: ${requestUrl}`); // [6]\n\t// 그 외 경우는 reject\n\t} else {\n\t\tconsole.log(\"reject\");\n\t}\n});\n\napp.listen(4540, () => {\n});위 코드는 사용자에게 파라미터로 url을 입력받고, 입력된 url 검증 후 fetch하는 간단한 웹서버입니다.new URL(url).hostname 으로 hostname을 가져온 뒤, hostname이 toss.im 과 일치하거나 .toss.im 으로 끝나는지 확인합니다. 일반적으로 도메인을 검증할 때 사용하는 방식이죠.파라미터로 입력한 url과 node-fetch에서 실제로 요청한 url 콘솔 출력이 경우, 위 그림처럼 url파라미터 값을 https://google.com!.toss.im 로 입력하면 검증 로직이 우회되고 서버는 https://google.com/!.toss.im/ 으로 요청하게 되는데요. 그 이유는 node-fetch 라이브러리 코드를 보면 알 수 있습니다./* node-fetch v2.6.11 \n * [https://github.com/node-fetch/node-fetch/tree/v2.6.11]\n * request.js\n *\n * Request class contains server only options\n *\n * \n */\n\nimport Url from 'url'; // [1]\nimport Stream from 'stream';\nimport whatwgUrl from 'whatwg-url';\nimport Headers, { exportNodeCompatibleHeaders } from './headers.js';\nimport Body, { clone, extractContentType, getTotalBytes } from './body';\n\nconst INTERNALS = Symbol('Request internals');\nconst URL = Url.URL || whatwgUrl.URL;\n\n// fix an issue where \"format\", \"parse\" aren't a named export for node <10\nconst parse_url = Url.parse; // [2]\nconst format_url = Url.format;\n\n/**\n * Wrapper around `new URL` to handle arbitrary URLs\n *\n * @param  {string} urlStr\n * @return {void}\n */\nfunction parseURL(urlStr) {\n\t/*\n\t\tCheck whether the URL is absolute or not\n\n\t\tScheme: https://tools.ietf.org/html/rfc3986#section-3.1\n\t\tAbsolute URL: https://tools.ietf.org/html/rfc3986#section-4.3\n\t*/\n\tif (/^[a-zA-Z][a-zA-Z\\d+\\-.]*:/.exec(urlStr)) {\n\t\turlStr = new URL(urlStr).toString() // [3]\n\t}\n\n\t// Fallback to old implementation for arbitrary URLs\n\treturn parse_url(urlStr); // [4]\n}위 코드는 node-fetch 라이브러리에서 url을 파싱하는 코드 부분입니다. 중요한 부분은 주석으로 번호 표시를 해두었는데요.[1]: Node.js의 url 라이브러리를 가져옵니다.[2]: Node.js의 url.parse()함수를 parse_url 변수에 저장합니다.[3]: 파싱할 url이 정규식 조건과 일치하면 WHATWG URL API 로 파싱합니다.[4]: 그 외에 경우는 Node.js의 url.parse()함수로 파싱합니다.url 파라미터로 입력한 https://google.com!.toss.im 은 [3] 정규식 조건에 충족되지 않으니, [4] url.parse()로 hostname이 파싱되었고 Node.js의 잘못된 파싱 방식으로 인해 https://google.com/*.toss.im 으로 요청하게 된 것입니다.이처럼 서비스 서버의 검증 로직을 우회하고 공격자가 원하는 임의의 도메인으로 요청하도록 하는 공격기법을 SSRF(Server Side Request Forgery) 라고 하는데요. 공격자는 SSRF 공격을 통해 외부에 공개되어 있지 않은 서비스 내부 망에 접근하여 민감한 정보들을 탈취하거나, 관리자 기능들을 악용할 수 있습니다.취약점 패치 컨트리뷰션화이트해커 문화에는 취약점을 제보하고 그에 따른 보상을 지급하는 버그바운티(Bug Bounty) 라는 프로그램이 존재합니다. 보안에 중요한 가치를 두고 있는 기업들이 독립적으로 운영하기도 하고, 국가기관에서 운영하기도 하는데요.토스 버그바운티 챌린지토스에서도 작년에 자체적으로 토스 버그바운티 챌린지 (https://bugbounty.toss.im)를 진행한 바 있고, 국가 기관에서는 한국인터넷진흥원(KISA)이 국내 소프트웨어에 대한 취약점을 제보받고 있습니다.Node.js url.parse() 취약점 제보저 또한 버그바운티 프로그램을 통해 Node.js 측에 취약점을 제보하였고, 취약점을 알맞게 패치할 수 있는 방안들에 대해 논의하면서 컨트리뷰션을 진행하였는데요.기존에 Unreserved Characters 를 화이트리스트로 처리하는 방식 대신 Reserved Characters 를 블랙리스트로 처리하는 방식으로 변경하여 isValid 조건을 좀 더 엄격하게 가져가도록 패치하였습니다.https://github.com/nodejs/node/pull/45011 에서 패치된 코드를 확인해 볼 수 있고, 해당 Pull Request는 v19.1.0, v18.13.0 에서 적용되었습니다.https://nodejs.org/api/url.html#urlparseurlstring-parsequerystring-slashesdenotehost추가로 기존에 Legacy 상태였던 url.parse()함수를 Deprecated 로 변경하였는데요. --pending-deprecation 옵션을 사용하는 경우, 런타임에서 Deprecated 함수임을 경고하도록 패치되었습니다.이 글을 읽으신 분들도 Node.js를 사용하고 계시다면, 취약점이 존재하는 버전을 사용 중인지 확인해보시는 것을 권장드립니다. *취약점은 v19.1.0, v18.13.0 에서 패치되었습니다.그리고 저희 보안기술팀(Security Tech)에서 이와 같은 보안 연구를 같이 해나갈 동료분들을 찾고 있습니다. 관심이 있으시다면 언제든지 문을 두드려 주세요!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "22": "놀러오세요! 프론트엔드 다이빙 클럽진유림2023. 7. 21안녕하세요, 토스 프론트엔드 개발자 진유림입니다. 저는 개발을 처음 배울 때부터 커뮤니티 안에서 성장해왔는데요. 9XD, GDG, Facebook Developer Circle등 다양한 온/오프라인 커뮤니티에서 각양각색의 개발자를 만나며 IT업계에 대한 애정을 키우고, 지식은 나눌수록 커진다는 것을 깨달았어요.그러나 코로나 이후로는 오프라인 모임이 중단되었고, 개발 이야기를 나눌 수 있는 사람들이 회사 내로 한정되어 버린 것이 아쉬웠어요. 그래서 시작했습니다, 다양한 회사와 다양한 연차의 개발자들이 모여 노하우를 나누는 프론트엔드 커뮤니티, ‘프론트엔드 다이빙 클럽’을요.프론트엔드 다이빙 클럽?커뮤니티 이름으로부터 유추할 수 있듯이, ‘프론트엔드 다이빙 클럽’(이하 프다클)은 프론트엔드에 관한 깊은 이야기를 나눌 수 있는 공간입니다.격월로 소규모 오프라인 모임을 개최하며, 한 번 이상 참석한 사람들은 프라이빗 슬랙에 가입하여 온라인으로도 소통을 이어갈 수 있습니다.각 모임마다 주제가 바뀌고, 해당 주제에 관심이 많은 분들이 참가하여 다양하고 깊은 의견을 들을 수 있는 게 특징입니다.지난 세 번의 모임은 다음과 같은 주제로 진행되었습니다:웹뷰 위에서 서비스 개발하기프론트엔드 자동화프론트엔드 일하는 방식/문화 공유지금까지 네이버, 유니크굿, 무신사, 아임포트, 오늘의집, 뱅크샐러드, 29cm, 카카오페이, 당근마켓, 우아한형제들, 두나무 등 30개 이상의 회사로부터 1년차부터 20년차까지 다양한 연차의 프론트엔드 개발자 분들이 참석하여 의견을 공유하였습니다. 앞으로의 모임에서는 다음과 같은 주제를 기획하고 있습니다:프론트엔드 테스팅좋은 면접이란프론트엔드 커리어 패스이 주제들은 타 회사의 시각을 듣고 인사이트를 얼마나 많이 얻을 수 있는지를 기준으로 선택되고 있습니다.주제별 발표는 2세션 씩 진행하며, 다양한 회사의 관점을 짧지만 뾰족하게 접할 수 있는 시간으로 준비하고 있습니다. 발표자 신청은 모두가 할 수 있으니 관심 있는 분들의 신청 부탁 드려요(토스 프론트엔드 트위터https://twitter.com/TossFrontend에 공지).모임은 어떻게 진행되나요?금요일 7시, 역삼역 토스 오피스에서 진행되는 오프라인 모임은 다음과 같은 타임라인으로 진행됩니다.7:00 인트로설문조사로 응답해주신 관심 주제(e.g. 코드리뷰, 기술부채 관리 등)를 기반으로 미리 배치된 조(6명 규모)에 착석하와이 풍 음식 먹으며 조별 아이스 브레이킹7:30 첫 번째 발표세션 듣기 및 질의응답조별 토의 및 발표8:10 두 번째 발표세션 듣기 및 질의응답조별 토의 및 발표8:50 마무리 및 조 섞기 + 네트워킹발표를 듣고 그냥 돌아가는 것이 아니라 주제에 대해 활발히 토론하고 살아있는 인사이트를 얻을 수 있도록, 조별로 토론하는 시간을 많이 배치했습니다.주제에 대한 관심이 많은 분들이 참석하셔서 그런지, 아니면 네트워킹을 통해 친밀도가 높아져서 그런지 모든 발표에서 질문이 10~15개씩 나오는 것이 인상적이었습니다.참여자 후기발표자의 한 마디Q. 발표를 하며 얻어간 것이 있나요?A. 비슷한 고민을 회사 밖에서도 함께하는 동료 개발자들과 이야기하는 경험은 정말 귀하다고 느꼈어요. 무언가 만들거나 해결했지만 '이게 정말 좋은 방법일까'라는 고민이 남을 때가 있는데요. 프다클은 '이렇게 풀 수도 있구나', '이런 고민도 있구나'를 공유하면서 생각을 넓힐 수 있는 자리였습니다.참여자의 한 마디Q. 프다클에 매번 참여하는 이유는?A. 좋은 스피커분들의 발표를 다양한 분들과 같이 들으며 식견을 넓혀갈 수 있는 점, 다양한 회사의 프론트 개발자분들과 네트워킹을 할 수 있는 기회, 바빠서 자주 보지 못하는 지인과, 겸사겸사 좋은 자리에 함께 하는 것, 그리고 여러 멋진 분들을 보면서 더 자극받는 시간이 되기도!준비 위원회의 한 마디Q. 어떤 분들이 프다클에 참여하시면 좋을까요?A. 다이빙 클럽은 컨퍼런스 보다는 토론의 장에 가깝습니다. 지식과 경험을 공유하는 것에서 의미와 가치를 느끼시는 분들을 환영해요. 발표를 듣고 자신의 현업에서 적용하는 것 뿐 아니라, 관련해서 본인의 경험들을 적극적으로 나눠주시고 다른 분들의 경험을 경청해주시는 분들께서 참여해 주시기를 기대하고 있어요. 어떻게 참가할 수 있나요?프다클 커뮤니티는 초대 기반으로 발전해오고 있습니다. 매 모임마다 슬랙에 가입된 기존 멤버 대상으로 티켓을 20장씩 제공하고(선착순 10명, 추첨 10명), 이 분들이 지인을 동행하여 총 40명의 사람들이 모이게 됩니다.발표자 신청도 프다클에 참여할 수 있는 또 다른 방법입니다. 발표자 신청 폼은 토스 프론트엔드 트위터(https://twitter.com/TossFrontend) 에 올라갈 예정이니, 참여 원하시는 분들은 구독해주세요. 초대 기반인지라 커뮤니티 확장이 느리다는 아쉬움이 있긴 하지만, 그래도 해당 주제에 진정으로 관심이 있는 분들이 모이게 되어서 오프라인에서도, 온라인에서도 깊은 토론을 할 수 있다는 장점이 있습니다.프다클 합류를 희망하신다면, 구글폼을 통해 간단한 경력 정보와 함께 자기소개를 남겨주세요. 남겨주신 정보를 토대로, 매 회차 모임의 방향성에 따라 함께할 분을 선별하여 초대권을 보내드릴게요.올 해 안에는 더 큰 규모의 공개 모임을 진행할 계획도 있습니다. 기대해주세요!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "23": "레고처럼 조립하는 토스 앱이준석/송범근ㆍiOS Developer2023. 8. 22100만 줄.이게 뭐냐고요?바로, 토스 iOS 앱의 코드량입니다.토스팀은 사용자에게 가치를 전달하기 위해 끊임없이 서비스를 개발해왔어요. 지금 토스 앱 안에는 수백 개의 서비스가 들어있습니다. 그렇게 성장해오는 동안, 토스 iOS 앱도 Swift 100만 줄이 넘는 거대한 프로젝트로 자라났습니다.이 글을 읽고 계신 iOS 개발자분들에게 질문을 드려볼게요. 이렇게 프로젝트가 크고 복잡해지면 뭘 해야 할까요?바로 모듈 분리입니다!앱을 하나의 큰 Xcode 프로젝트로 관리하는 대신, 여러 개의 작은 모듈로 나눕니다. 그리고 모듈 간의 적절한 구조를 설계하는 거죠.코드 베이스가 커지면, 모듈 분리도 점점 더 중요해지죠. 그래서 토스 iOS 챕터도 모듈화에 대한 많은 고민을 했는데요. 이 글에서는, 저희가 어떻게 슈퍼 앱 토스의 모듈을 관리하고 있는지 살펴볼게요.먼저 기존 토스 앱의 구조를 알아봐야겠죠? 기존 토스 앱은 가장 일반적인 계층 구조로 이루어져 있었어요. 책임과 역할에 따라 계층을 나눠 해당 계층에는 그에 맞는 모듈들이 위치하고, 하위 계층에 있는 모듈은 상위 계층에 있는 모듈을 의존할 수 없는 형태죠.공통적으로 쓰이는 유틸리티 모듈들이 모여있는 Foundation 계층.수백 개의 서비스 모듈이 위치한 Feature 계층.최종적으로 사용자에게 제공될 앱이 위치한 App 계층.이렇게 계층적으로만 모듈을 관리하고 있었어요.일반적인 계층 구조도 꽤 오랜 시간 잘 작동했습니다. 계층 안에서의 모듈 분리도 되어있었고요. 하위 계층의 모듈이라면 필요에 따라 가져다 활용할 수 있었죠.하지만 수년간 토스에는 정말 많은 서비스가 생겼습니다. 다시 말해 Feature 계층에 모듈들이 엄청나게 늘어났죠.그러다 보니 문제가 발생했습니다.단순히 모듈 수가 많아서는 아니었어요. 많은 서비스가 생겨나면서, 모듈 간의 의존성이 폭발적으로 늘어나는 게 문제였죠.하나의 서비스가 Feature 레이어의 여러 모듈을 활용하는 경우가 많이 생겼습니다. 결과적으로 같은 계층 내에 의존 관계가 복잡해졌고요.그러자 순환 참조와 같은 문제가 발생하기도 했어요. 전반적인 모듈 구조도 이해하기에 너무 어려워졌습니다. 처음에는 계층 구조를 유지하면서 대응을 해봤어요. 공통 기능을 다시 묶어서 하위 계층의 모듈로 분리하거나, 새로운 계층을 추가하여 공통 모듈을 추가로 분리하는 방식도 시도했고요.그런데 이렇게 하다 보니 지나치게 많은 계층이 생겨났어요. 적절한 계층 및 모듈 분리의 기준을 세우기도 애매해지더라고요.이대로는 안 되겠다. 다른 방법이 필요하다. iOS 개발자 모두가 느끼기 시작했습니다.Microfeatures 아키텍처는 Tuist가 소개한 모듈 구조예요. (Tuist는 iOS 프로젝트 관리 툴입니다.)크게 보면, Microfeatures 아키텍처는 아래와 같이 5개의 요소로 구성되어 있어요.Feature(Source)Feature의 실제 기능이 구현된 코드가 위치한 모듈이에요.\nInterfaceFeature에서 제공하는 기능에 대한 외부 인터페이스와 모델을 제공하는 모듈이에요.\nTesting단위 테스트나 Example 앱에서 사용될 코드와 Mock 데이터 등을 제공하는 모듈이에요.\nTests단위 테스트, UI 테스트 등이 위치한 모듈이에요.\nExampleFeature의 기능을 간단히 체험해 볼 수 있는 작은 앱이에요.그래서 이 요소들을 가지고 하나의 서비스를 담당하던 모듈을 쪼개는 거예요.홈 서비스를 예로 들어볼까요?기존에 하나였던 Home 모듈을 이렇게 5개의 모듈로 나누게 되는 거예요.Home (Feature)HomeInterface (Interface)HomeTesting (Testing)HomeTests (Tests)HomeExample (Example)이것을 여러 서비스로 확장해 보면 이렇게 돼요.Interface 모듈에 있는 인터페이스를, Feature, Testing 모듈이 구현합니다. Tests 모듈에 테스트를 작성하고요.Example 앱을 구성할 때는 Feature 혹은 Testing 모듈 중 필요한 것을 골라서 사용해요.이때 자체적으로 개발한 DI(Dependency Injection) Container를 사용해요. Interface 모듈에 대한 Feature 모듈의 구현을 주입하는 역할을 맡고 있죠.Microfeatures 아키텍처 구조로 바뀌고 난 이후에는 뭐가 달라졌을까요?다른 서비스에서 Home 서비스의 코드를 필요로 할 때 Home 모듈을 직접 사용하지 않아요. 대신 HomeInterface 모듈을 사용하죠. HomeInterface에는 외부에 제공되는 인터페이스가 있어요. 이 Home의 기능을 사용할 때는 이 인터페이스를 씁니다.이렇게 하면 한 서비스가 다른 서비스의 코드를 사용하더라도, 같은 계층 (Feature ↔ Feature, Interface ↔ Interface) 내의 의존 관계가 생기지 않게 돼요.같은 계층 내에 의존 관계가 복잡해지는 문제를 해결했습니다.쉽고 멋있게 해결한 척했지만… 사실 Microfeatures 아키텍처를 도입하는 것은 꽤나 험난한 길이었어요.일단 작업량이 만만치 않았죠. 기존에 하나였던 모듈을 5개의 모듈로 분리해야 하는 작업인데, 기존의 모듈도 이미 수백 개였거든요.수많은 컨플릭이 발목을 잡았어요. 공통 모듈을 자주 건드릴 수밖에 없었고. 현재 작업 중인 모듈이 있고, 그 모듈이 의존하는 다른 모듈 분리 작업이 동시에 진행되었어요. 컨플릭이 자주 발생할 수밖에 없는 상황이었죠.현실적인 리소스의 문제도 있었고요. 토스에선 빠른 실험을 위해 ‘매주’ 앱 배포를 진행하는데요. 이런 환경에서 사일로 업무도 하고, 구조 개선을 위한 모듈 분리도 하는 건 쉽지 않은 일이었죠.iOS 챕터에서는 작업량을 줄이기 위해서, Tuist에서 제공하는 Stencil 템플릿과 Tuist Scaffold 기능을 최대한 활용했어요. 토스 앱의 모듈 구조에 알맞게 Tuist extension 을 적절히 구현했습니다. 다양한 Template 을 만들어서 단 1줄의 코드로 새로운 모듈을 생성할 수 있도록 했죠.하지만 무엇보다도 iOS 챕터 전체가 모두 적극적으로 참여했던 게 도입을 해낼 수 있었던 가장 큰 이유예요. 기존 구조의 문제를 겪고 있던 분들이 도입의 필요성에 많이 공감해 주셨고요. 너 나 할 것 없이 담당하고 있던 기능들에 Microfeatures 아키텍처를 적용해 나갔어요. 담당자가 퇴사를 했다든지, 공통적으로 쓰는 모듈이라든지 그런 애-매한 모듈들도 있었는데요. 다들 본인 것처럼 적극적으로 나서주셨죠.Microfeature 아키텍처의 장점은 이게 끝이 아닙니다.Example 앱이 있기 때문이죠!Example 앱은 필요한 기능만을 담은 별도의 미니 앱이라고 보시면 돼요. Microfeature 아키텍처가 도입된 후로, 개발할 때 Example 앱을 적극 활용하고 있어요. 실제로 송금 Example 앱, 자동이체 Example 앱, 본인확인 Example 앱 등등 다양한 Example 앱이 있습니다.Microfeature 아키텍처가 있다면 Example 앱을 만드는 건 어렵지 않아요. 구현을 확인해 보고 싶은 기능의 Feature 모듈을 사용하고요. 나머지 기능의 경우 Testing 모듈에 있는 Mock을 연결하면 되죠.아래 그림은 송금 Example 앱인데요. 실제 송금 결과 케이스들을 바로바로 볼 수 있도록 만들어놓았어요.특히 UI 개발을 할 때 무척 편리합니다. \n아까 말씀드린 것처럼, 토스 앱은 어마어마하게 큰 앱이거든요. Swift만 100만 줄이 넘고요. 모듈은 약 700개나 있고요. 그니까 빌드 시간이 엄청 길 수밖에 없어요.만약 토스 앱 전체를 빌드하고 나서, 수정한 화면이 어떻게 보이는지 확인해야 된다면? UI 수정 확인 한번 하려고 한참을 기다려야 합니다. 빌드 할 때마다 수십 초씩 걸린다는 건 상당히 불편한 일이잖아요. 해보신 분들은 이게 얼마나 생산성을 떨어뜨리는지 아실 거예요.Example 앱은 이럴 때 빛을 발합니다. Example 앱은 토스 전체 앱 빌드보다 훨씬 빠르거든요. (무려 5배) 전체 앱이 아닌 일부분만을 빌드하기 때문이죠.Example을 하다 보면, 협업을 하는데도 굉장히 편리해요.토스 디자인 시스템(TDS)을 만드는 디자인 플랫폼 팀의 예시를 들어드릴게요.TDS에는 AnimateTop라는 컴포넌트가 있습니다. 말 그대로 애니메이션 효과가 들어있는 제목이에요. 그런데 이걸 코드로만 보면, 실제 느낌은 알기 어렵잖아요.그럴 때 ShowCaseExample 앱을 사용해요. AnimateTop이 다양한 속성을 적용했을 때 어떻게 보이는지 실제로 확인해 볼 수 있죠.개발을 하고 나면, Example 앱을 사내에서 배포할 수 있어요.복잡한 화면이나 애니메이션을 개발하다 보면 디자이너 분과 빌드 된 앱을 같이 확인하는 경우가 꽤 많아요. 중간중간 실제 화면을 확인해보기 위해 다른 팀원에게 앱을 빌드하여 보여주는 경우도 흔히 있고요.그럴 때 Example 앱을 사용하면, “00님, 이거 와서 봐주세요, 어때요?” 매번 여쭤볼 필요가 없어요. Example 앱 배포했습니다! 알려드리면 직접 받아서 확인하면 되거든요. 직접 사용하시는 PO나 디자이너 분들도 정말 편하다고 하시더라고요.디자인 플랫폼 팀에서는 새로운 인터랙션, 컴포넌트 등을 만드는 일이 많은데요. 이 Example 앱을 적극 활용해서 개발부터 QA까지 하고 있습니다.🔔 요약 정리앱의 코드 베이스가 커지고 복잡해질수록, 모듈 분리와 관리가 중요해진다.기존 토스 iOS 앱은 일반적인 계층 구조로 나눠서 모듈을 관리했다.하지만 앱 내 서비스가 계속 많아지면서, 같은 계층 내의 의존 관계가 너무 복잡해졌다.토스 iOS 챕터는 이 문제를 해결하기 위해 Microfeature 아키텍처를 도입했다.Microfeature 아키텍처 도입은 작업량, 컨플릭, 리소스의 문제로 쉽지 않은 과제였지만, iOS 챕터 모두가 합심하여 결국 성공!Microfeature 아키텍처를 도입하면서 Example 앱도 적극적으로 쓰게 되었다.Example 앱은 전체 앱보다 빌드가 5배 빨라서 개발 생산성이 올라가고, 사내 배포가 가능해서 협업에도 도움이 된다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠금융사 최초의 Zero Trust 아키텍처 도입기2023. 09. 01은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)2023. 08. 31",
    "24": "은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)장세경/조서희ㆍCore Banking Developer2023. 8. 31토스뱅크는 기존의 공급자 중심의 뱅킹 서비스를 고객 중심으로 변화시키기 위해 많은 노력을 기울이고 있어요.그러나 기존의 전통적인 뱅킹 시스템을 구현하는 방식으로는 안정적인 고객 중심 뱅킹 서비스 제공에 여러 기술적 한계가 있었죠. 이번 아티클에서는 토스뱅크가 어떤 방식으로 기술적 한계를 극복했고, 어떤 기술로 고객 중심의 뱅킹 서비스를 제공해 드리고 있는지에 대해 소개해 드릴게요.현재 은행 시스템에 대한 소개채널계와 코어뱅킹(계정계)먼저, 일반적인 은행 시스템의 아키텍처에 대해 알아볼게요. 은행에는 크게 고객의 요청을 코어뱅킹 서버로 전달하는 채널계와 금원과 관련된 메인 비즈니스 로직을 처리하는 코어뱅킹(계정계)라고 하는 두 개의 서버를 중심으로 하는 아키텍처로 구성되어 있어요. 여기에 코어뱅킹 서버는 대부분의 은행에서 거대한 모놀리식 아키텍처로 구성되어 있죠.코어뱅킹 시스템 아키텍처 히스토리코어뱅킹 시스템이 모놀리식 아키텍처를 유지해온 이유는 은행 시스템의 변천사를 알면 그 힌트를 얻을 수 있는데요. 1970년대부터 은행의 계좌 데이터를 적절하게 가공하고 처리해야 하는 니즈가 생기면서, 1세대와 2세대 코어뱅킹 아키텍처가 생겨났고, 2000년대에 디지털 붐이 일면서 모바일 뱅킹, 웹 뱅킹, 텔레뱅킹 등 다양한 거래 요청을 한 곳에서 적절하게 처리해줄 수 있도록 현재의 모놀리식 코어뱅킹 아키텍처가 생겨나게 되었어요. 지난 20여 년간 코어뱅킹 아키텍처는 운영체제와 개발언어의 크고 작은 변화는 있었지만, 현재의 모바일 트렌드와는 맞지 않는 20년 전의 모놀리식 아키텍처를 대부분의 은행에서 사용하며, 현재의 거대한 모놀리식 형태로 몸집을 불려가고 있었죠.현재 토스뱅크의 채널계는 기존 토스의 DNA를 이어받아 모두 MSA 환경으로 구성되어 있어요. 반면에, 기존의 코어뱅킹 시스템은 Redis, Kafka 등의 모던한 기술을 사용하고는 있었지만, 여타 은행과 다름없이 채널계와의 통신을 위한 MCI, 대외연계를 위한 FEP, 대내 단위 시스템과의 연계를 위한 EAI가 코어뱅킹 서버에 강결합되어 있는 구조로 여타 은행과 다른 없는 거대한 모놀리식 시스템으로 구성되어 있었죠.그렇다면, 모놀리식 코어뱅킹 아키텍처가 어떤 한계가 있었기에 MSA로 전환했어야 했을까요? 모놀리식 코어뱅킹 시스템의 장점과 단점을 곱씹어보며, 그 이유를 살펴볼게요.물론 모놀리식 코어뱅킹 시스템도 장점이 있습니다. 모놀리식 코어뱅킹 시스템의 장점트랜잭션 관리의 용이성 : 로컬 DB 트랜잭션으로 여러 하위 도메인의 데이터를 ACID하게 변경할 수 있음.개발의 단순성 : 모든 코드가 단일 코드 베이스에 있으므로 개발하기가 단순함.보편성 : 대부분의 코어뱅킹 시스템이 모놀리식으로 구성되어 있으므로, 인력 수급과 개발이 용이함.그렇지만, 모놀리식으로 구성된 시스템은 트래픽이 갑자기 몰렸을 때, 특정 코어뱅킹 서비스만 스케일 아웃을 하는 전략을 가져갈 수 없어요.또한, 1개의 서버이기 때문에 장애가 발생한 서비스 외에 다른 서비스들의 영향도를 제한할 수 없어, 안정성이 부족하다는 단점도 있죠. 즉, 한 개의 컴포넌트에서 장애가 발생하면, 전 업무가 마비되는 구조로 이어질 수 있다는 건데요.예컨대, 토스뱅크가 카드 결제 시 결제 금액의 30%를 환급해주는 파격적인 이벤트를 모놀리식 시스템 구조에서 진행한다고 해볼게요.카드 서비스는 평소보다 훨씬 많은 트래픽이 들어올 것이고, 이 트래픽이 수용할 수 있는 임계점을 넘어서면, 이벤트를 진행하는 카드 서비스 뿐만 아니라 전혀 상관 없는 계좌 개설이나, 대출 약정 서비스들까지도 마비 될 거에요.미리 이벤트를 알고 있다고 하더라도, 카드 서비스만 스케일 아웃을 할 수 없기 때문에 전체 시스템의 가용성을 확보해두어야 하는 비효율도 발생할 것이고요.모놀리식 아키텍처의 서비스 영향도 제한이 어려운 이유에 대해 조금은 이해가 되셨나요?토스뱅크는 고객분들에게 가치를 제공해드리기 위해 하루에도 수차례씩 혁신적인 실험과 기능 추가를 위한 배포를 하고 있어요. 그러면서 Market Fit에 맞는 제품과 서비스들을 빠른 속도로 찾아가고 있고, 그만큼 토스뱅크를 애용해주시는 고객분들도 많이 늘어나고 있죠.하지만 토스뱅크의 서비스가 고객분들의 사랑을 받아 나날이 성장하는 만큼 기존의 모놀리식 아키텍처를 유지하면서 토스뱅크의 혁신적인 서비스들을 안정적으로 제공해드리기는 점점 어려워졌어요. 그래서 저희는 현재의 차세대 코어뱅킹 아키텍처를 대량 트래픽에 특화되어 있고, 각 업무별 서비스 영향도를 분리할 수 있는 MSA로 전환하기로 결정했습니다.그중에서도 저희는 토스뱅크 서비스 중에서 가장 트래픽이 많으면서, 토스뱅크의 대표 서비스 중 하나인 지금 이자 받기 서비스를 모놀리식 코어뱅킹 시스템에서 분리하여 MSA로 전환하기로 했답니다. 개발 방법기술 스택 선정먼저 기술 스택은 현재 토스뱅크 채널 서버에서 사용하고 있는 기술들을 대부분 채택했어요. Kubernetes위에 Spring boot, Kotlin, Jpa 등을 기반으로 개발했고, 비동기 메시지 처리와 캐싱은 Kafka, Redis를 사용하기로 결정했어요.그런데 개발하자마자 첫 번째 고민에 봉착했는데요. 현재 모놀리식으로 강결합되어있는 업무별 비즈니스 의존성을 어느 정도까지 느슨하게 가져갈 것이냐였어요.지금 이자 받기를 위해 필요한 도메인은 고객 정보 조회를 위한 고객, 금리조회를 위한 상품 그리고 이자의 회계 처리를 위한 회계 정보가 필요했어요. 이 모든 것을 하나의 마이크로 서버에서 처리하는 것은 MSA의 장점을 활용하지 못할 것이라 판단하여, 도메인 단위로 서비스를 나누기로 결정했어요.고객의 지금 이자 받기 요청은 고객 정보 조회를 거쳐, 금리 조회와 이자계산, 이자 송금, 회계처리를 한개의 트랜잭션으로 처리하고 있었는데요.새로운 코어뱅킹 아키텍처에서는 트랜잭션으로 엮이지 않아도 되는 도메인은 별도의 마이크로 서버로 구성했고, 각 서버의 API 호출을 통해 비즈니스 의존성을 느슨하게 가져가도록 구성했어요.그러면 이제 실제 이자지급 서버를 어떻게 개발했는지 알아볼게요. 동시성 제어먼저 은행 시스템의 안정성과 직결되는 부분인 동시성 제어입니다. 일단 적절하게 동시성 제어가 안되었을 때, 어떤 문제가 있을지 살펴볼까요? 0.01초 사이에 Transaction1을 통해 이자를 받았고, Transaction2를 통해 입금을 받았다고 가정해보면, Transaction1에서는 현재 잔액 기준인 100원에 지금 이자 받기를 한 100원을 더해 200원으로 갱신을 할 거예요.그리고 Transaction2에서는 Transaction1의 요청이 있었는지를 알 수 있는 방법이 없으므로, 처음에 조회한 100원의 잔액에 타행으로부터 입금받은 300원의 잔액을 더해 400원이라는 엉뚱한 금액으로 잔액을 갱신할 거예요. 이렇게 되면, 어떤 고객도 토스뱅크의 시스템을 신뢰하지 않겠죠.이렇듯 은행에서 고객 잔액의 갱신은 앱을 통한 거래는 물론이고, 타행을 통한 입금, ATM을 통한 이체, 자동이체 등으로 잔액를 갱신하는 트랜잭션의 채널이 매우 많아요. 그렇기 때문에 일반적으로 사용되는 Redis Global Lock 만으로는 은행 시스템 환경에서 동시성 제어 이슈는 해결하기가 어렵죠. 그래서 동시성 이슈를 해결하는 것이 코어뱅킹 개발에 있어서 필수 조건이라고 할 수 있습니다.저희는 이 문제를 Redis Global Lock과 더불어 DB Layer에서 동시성을 제어하기 위한 JPA의 @Lock 어노테이션을 통해 해결했어요.앞에 예시로 다시 돌아가 볼게요. Transaction2는 DB Layer에서 Lock으로 동시성을 제어하고 있기 때문에 Transaciton1이 끝날 때까지 대기합니다. 그리고, Transaction1의 commit이 끝난 이후의 변경된 잔액을 참조하겠죠. 그러면 잔액은 최초에 예상했던 500원으로 commit이 되고 트랜잭션의 동시성은 안전하게 보장됩니다.그런데 이 때, DB Lock을 사용할 때는 주의해야 하는 점이 있어요. Lock을 잡아야 하는 데이터를 명확히 식별하고, 갱신하는 데이터에 대해서만 Lock을 획득해야 데드락과 시스템 성능 저하를 예방할 수 있다는 점인데요.지금 이자 받기API의 경우 잔액을 갱신하는 이벤트가 메인 비즈니스 로직이기 때문에, 계좌 단위 현재 잔액 데이터에 대해서만 고유하게 Row Locking이 걸리도록 개발하여, 동시성을 보장하도록 구현했어요.또한, Transaction2의 동시성이 발생하였을 때, Transaction1을 끝날 때까지 기다릴 수 있도록 재시도할 수 있는 로직과 적절한 타임아웃을 적용해주어서 고객 관점에서 Lock이 걸렸는지도 모르게 안정적으로 이자를 받을 수 있게 구현했죠.성능 개선을 위한 비동기 처리두번째는 카프카를 활용한 비동기 트랜잭션 구현입니다. 기존 코어뱅킹 시스템에서는 1번의 이자를 지급받기 위해 20개의 테이블에 80번의 UPDATE, INSERT가 이루어지는 복잡한 구조였어요.그렇기 때문에 지금 이자 받기 서비스의 속도도 평균 300ms로 전체 코어뱅킹 서비스 중에서 느린 편에 속했죠. 이 정도면 정규화가 잘 되어 있는 데이터 모델과 정교하게 잘 설계된 인덱스 구조로도 빠른 응답 속도를 기대하기는 어려운 구조였어요. 그래서 기존 지금 이자 받기 트랜잭션에서 분리가 가능한 테이블은 카프카를 이용해 트랜잭션에서 분리했어요.트랜잭션 분리에 대한 기준은 고객의 잔액과 통장 데이터 관점에서 DB 쓰기 지연이 발생하였을 때, 실시간으로 문제가 발생하느냐? 로 접근하였고, 반드시 트랜잭션이 보장되어야 하는 데이터 모델과 즉시성을 요하지 않는 즉, 세금 처리와 같이 지금 이자 받기 트랜잭션과 묶이지 않아도 되는 데이터 모델의 DML은 트랜잭션을 분리했죠.구체적으로 살펴보면, 지금 이자 받기 서버에서 지금 이자 받기의 트랜잭션 종료와 동시에 세금 카프카 토픽에 메시지를 Produce하고, 비동기 처리 서버가 Consume해서 세금 DB에 저장하도록 구현했어요. 정상적인 상황이라면, 이자 DB와 세금 DB에도 준실시간으로 업데이트가 되었을 것이기 때문에 지금 이자 받기의 거래는 정상적으로 종료될 거에요.그렇지만, 카프카 메시지가 정상적으로 처리되지 않는 경우도 있기 때문에, dead letter queue를 이용해서 세금DB에 대한 트랜잭션을 안정적으로 보장할 수 있도록 했어요. 또, 재처리시 중복으로 세금이 업데이트 안되도록 API도 멱등하게 설계했죠.그 결과 세금 DML을 지금 이자 받기 트랜잭션에서 분리함으로써, 기존 80회의 DML이 이루어지던 지금 이자 받기 트랜잭션을 50회의 DML로 줄이는 개선 효과를 얻을 수 있었습니다.Redis를 활용한 캐싱 전략마지막으로는 Redis를 활용한 캐싱 전략입니다. 기존 코어뱅킹 시스템에서의 이자 계산은 RDB 기반의 일자별 거래내역DB를 조회해서 연산하는 방식으로 구현되어 있었어요.고객이 지금 이자 받기를 할 때마다, 계좌의 매일 매일 거래내역을 참조해서 이자 계산과 세금을 계산하는 구조이므로 성능적으로 오래 걸릴 수 밖에 없는 구조였죠. 그러나 고객은 하루에 1번 밖에 이자를 못받기 때문에 Redis를 활용하면, 하루에 1번만 DBIO를 발생시킬 수 있을 것이라 판단해서 Redis를 이용해 캐시를 활용하기로 했어요.기존의 이자금액은 고객이 계좌 상세탭에 접근할 때마다, 이자계산을 위한 DB I/O가 발생하고 있었는데요. 이를 고객이 하루 중 처음으로 계좌 상세탭에 접근할 때에만 DB에 접근하도록 구현했고, 이자예상조회의 결과를 Redis에 캐싱해 두도록 구현했어요. 그래서 고객이 하루에 2번 이상 계좌 상세탭에 접근할 경우에는 Redis에 미리 저장되어 있던 이자계산 결과를 리턴하도록 했죠. 그래서 불필요하게 DB 리소스가 낭비되는 것을 예방했습니다. 또한, Redis에 캐싱 된 이자 데이터의 만료일자도 하루로 두어서, 이자금액이 잘못 계산 되는 케이스도 원천적으로 방지했어요. 그래서 매일 자정 이후 고객이 계좌 상세탭에 처음 접근할 때만, 이자예상조회의 결과를 캐싱해서 이자 데이터의 정합성도 안정적으로 보장할 수 있었죠.기존 시스템을 안정적으로 전환하는 방법이자 지급 마이크로 서버에 이자 조회 거래, 지금 이자 받기 거래를 개발 완료했어요. 이제 기존 코어뱅킹(계정계)를 참조하던 서비스를 이자 지급 마이크로 서버를 바라보도록 전환해야 하죠. 시스템을 전환하기에 앞서, 이자 지급 마이크로 서버 API에 대한 검증이 필요했는데요. 어떤 검증 방식을 활용할 수 있을까요? 첫 번째 방법: 실시간 검증을 통한 건별 검증 방식첫 번째 방법인 온라인 검증 방식을 도식화한 그림입니다. 먼저, 앱에서 고객이 이자 조회 거래를 일으키면 채널계에서 MCI를 통한 기존 코어뱅킹 서버에 이자 조회 서비스를 호출하고, 동시에 이자 지급 마이크로 서버의 API를 호출했어요.코어뱅킹 서버에서 리턴된 이자 값과 이자 지급 마이크로 서버에서 리턴된 이자 값을 각각 리턴 받아, 두 이자 값이 불일치할 경우 토스뱅크 내부 모니터링 채널에 해당 내용을 알림으로 받도록 했어요. 채널에 알림이 오면 대상 및 로그를 확인하고 원인을 확인하여 이자 계산 로직을 수정해주는 과정을 거쳤어요. 두 번째 방법: 배치를 활용한 대량 검증 방식다음은 배치를 활용한 대량 검증 방식입니다. Staging 환경이란? 실제 운영환경과 동일하게 구성된 내부 API 테스트용 환경. Staging 환경에서 채널계 배치를 활용해 매일 대량의 검증 대상 목록을 추출했고, 온라인 검증 방식과 동일하게 코어뱅킹 서버와 이자 지급 마이크로 서버를 각각 호출해주었어요. 대상 목록에 대한 검증이 모두 끝나면, 이자 리턴 값이 불일치했던 건들에 대한 내용을 담아 내부 모니터링 채널에 알림으로 받았고, 로직 수정을 반복해주었습니다. 이렇게 저희는 두 가지 방식을 활용해서 이자 조회 거래에 대한 검증을 완료했습니다. 그런데 실제 이자를 지급받는 지금 이자 받기 거래의 경우 코어뱅킹 DB 원장에 잔액을 갱신하고 거래내역을 쌓고, 회계 처리를 해주는 등의 작업이 필요했기 때문에, 거래가 발생했을 때 실제 데이터가 정확하게 쌓이고 갱신되었는지 추가로 검증해야 했어요. 그래서 지금 이자 받기 거래의 데이터 정합성 검증을 위해, 상세한 도메인 기반의 테스트 시나리오를 작성했어요. 테스트 시나리오 작성을 통한 E2E 통합 테스트 수행하기토스뱅크 통장은 잔액을 구간별로 나누어 이자를 차등 지급하고 있는데요. 잔액 구간별로 나누어 차등 계산되어 이자가 지급되었는지 검증이 필요했어요.그리고 명의도용, 해킹 피해, 사망 등 토스뱅크 고객의 상태에 따른 검증이 필요했고, 계좌의 상태 및 출금/입금 정지 상태에 따른 검증이 필요했죠.해당 검증 케이스들을 고려해서 테스트 시나리오를 수립했고, 케이스 별로 테스트를 진행하여 이자 계산 및 실제 DB에 데이터가 정확하게 갱신되었는지를 확인하며 로직을 수정해주는 과정을 거쳤습니다. 이 과정을 통해, 이자 받기 거래에 대한 정합성 검증을 완료할 수 있었어요.순차 배포를 통한 안정적인 마이그레이션하기 이제 API에 대한 검증은 완료되었으니, 코어뱅킹을 바라보던 서비스를 fade out 시키고 이자 지급 마이크로 서버 API만을 바라보도록 전환해줘야 했어요. API를 전환할 때 대상 모수를 점차 늘려가며, 순차적으로 오픈했는데요. 먼저 토스뱅크 수신개발팀에 오픈하여 직접 이자 받기 거래를 일으키며 데이터 결과값을 검증했어요. 특이사항이 없는 것을 확인하여 토스뱅크 내부 팀원에게 오픈하였고, 모니터링을 진행했어요.다음으로는 일부 고객을 대상으로 오픈하고 점차 모수를 늘려가며 순차 오픈하여 전체 고객을 대상으로 전환을 완료하는 방식을 선택했습니다. 순차 배포 과정을 살펴보면, 코어뱅킹 서버를 바라보던 API 호출량과 이자 지급 마이크로 서버를 바라보던 API 호출량을 조절하여 이자 지급 마이크로 서버의 트래픽을 점차 늘려가는 형태로 진행했습니다.그렇게 저희는 순차 배포 방식을 채택 함으로써 기존에 운영하고 있던 시스템을 중단하지 않고도 안정적으로 시스템을 전환 할 수 있었어요.마지막으로 코어뱅킹 MSA 전환의 성과에 대해 공유 드리며, 이번 아티클을 마무리 해볼게요.코어뱅킹 MSA 전환의 성과코어뱅킹 시스템의 세대 전환오픈소스 기반의 개발 환경 변화에 따른 유연성 및 확장성 증가지금 이자 받기 거래의 성능 170배 개선계정게 서버로부터 독립적인 서버를 구축함으로써 안정성 증가지금 이자 받기 피크타임 트래픽에도 개별적으로 이자 지급 서버 스케일 아웃 가능도메인 단위로 분리하여 효율적인 MSA 코어뱅킹 시스템 구축빅뱅 배포 방식을 탈피하여 무중단 시스템 전환 가능재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠레고처럼 조립하는 토스 앱2023. 08. 22금융사 최초의 Zero Trust 아키텍처 도입기2023. 09. 01",
    "25": "금융사 최초의 Zero Trust 아키텍처 도입기정연우ㆍSecurity Engineer2023. 9. 1전통적인 환경의 보안 아키텍처는 방화벽과 같은 경계를 기준으로 신뢰와 비신뢰를 나누어서 운영이 되고 있었는데요. 신뢰 구간에서의 추가적인 보안 통제가 없으면서 신뢰의 크기가 커진다면 그만큼 보안의 Risk들도 증가하게 된다는 한계점들이 있었고, 다원화 된 Identity 관리, 보안 솔루션 관리, 재택근무 환경과 오피스 환경의 이원화된 환경을 관리함으로써 보안 가시성 확보 및 관리의 어려움, 팀원들의 업무의 불편함들을 해결하기 위해 토스에서는 제로트러스트 보안 아키텍처를 도입하게 되었는데요. 어떤 과정으로 도입하였고, 어떻게 운영 중인지 살펴보려고 합니다. Zero Trust란?고정된 네트워크 경계를 방어하는 것에서 사용자, 자산, 자원 중심의 방어로 변경하는 발전적인 사이버 보안 패러다임입니다.제로 트러스트는 물리적 위치, 네트워크 위치, 자산 소유권을 기준으로 자산 또는 사용자 계정에 부여된 암묵적인 신뢰는 없다고 가정합니다.Zero Trust를 도입하기 위한 고민Zero Trust 보안 아키텍처를 도입하기 위해서는 현재 우리가 가지고 있는 리스크들을 정리하고, 대체 혹은 보완하는 것에 대한 고민들이 필요한데요. 저희는 아래와 같은 고민을 하였습니다. Identity: 다원화된 인증 시스템 사용으로 인하여 계정, 권한 관리의 어려움으로 인해 보안 가시성이 낮아지는 부분을 통합으로 관리하여 보안 가시성을 확보Device: Active Directory, PMS(Patch Management System) MacOS의 OS 보안 패치, SW 관리, PC 보안 정책 확보의 어려움으로 인한 대체 수단 마련Network: 재택근무 시 네트워크 보안 통제의 어려움, SSL Inspection을 통한 네트워크 분석의 어려움들을 해결하는 대체 수단 마련을 통한 보안 가시성 확보 및 사내 네트워크 접근 시 추가적인 보안 검증을 통한 위협 예방 각 영역에서 통제를 다른 영역과 연계하는 통제 수단 마련 예시 1) Application 혹은 사내 네트워크 접근 시 Endpoint 정보를 식별하여, 접근 유무 결정예시 2) 사내 네트워크에 대한 접근 통제 시 Identity 정보를 통한 접근 통제 각 영역의 이벤트들을 통합 분석하여 보안 위협 식별 및 대응 Zero Trust 아키텍처 주요 컴포넌트Identity, Device, Network, Data, Application 각 영역에서의 보안성, 가시성을 확보하고, 각 영역 간 연계를 통한 추가적인 접근 통제 및 통합 보안 이벤트 분석을 통한 위협 식별 및 대응주요 기능IAM (Identity Access Management) Single Sign On : Application 로그인 통합(SAML, OIDC)RBAC(Role Based Access Control) : 인사 DB와 연계하여 직군, 팀 단위의 접근 제어ABAC(Attribute Based Access Control) : Role(권한)이 할당 되어있더라도, 회사에서 정의한, Network, 자산, 보안을 검증을 통한 접근제어Policy : Application에 접근할 때의 Factor(ID/PW, MFA 등) 정의, RBAC, ABAC 기반의 접근제어Security : Threat Intelligence 분석 / SIEM(Security Information Event Management) 연동을 통한 계정 탈취와 같은 보안 위협 예방 및 방지SASE(Secure Access Service Edge)Policy : SSL Inspection을 통한 암호환 된 Traffic 복호화 및 HTTPS Packet 분석\nURL Filter, Application Control을 통한 유해사이트, 비인가사이트 차단Data Protection :DLP(Data Loss Protection)을 통한 회사의 중요 데이터(내부자료, 개인정보) 유출 모니터링, 방지File Type Control: File 확장자 기반의 Upload / Download Contents 식별 및 차단CASB(Cloud Access Security Broker)를 통한 Cloud에 보관된 중요 데이터 유출 방지Threat Management : Malware: 악성코드, 랜섬웨어, 스파이웨어 등을 식별 및 차단ATP(Advanced Threat Protection): 네트워크 트래픽 검사를 통한 악성코드, 스팽, 피싱과 같은 위협 식별 및 차단Sandbox: 파일, URL, IP 등의 데이터를 분석하여 위협 식별과 악성코드 분석 및 격리ZTNA(Zero Trust Network Access)Role : IAM 연동을 통한 회사 인사 DB를 활용한 내부 네트워크에 접근할 수 있는 사용자, 그룹 정의Segments : 내부 네트워크에 있는 자산을 Application 단위로 정의(Domain, IP, Port 등Policy : Role, Segments를 사용한 접근제어, Role이 있는 사용자라도 EPP(Endpoint Protection Platform)와 연계하여 접근할 때마다 Device 보안성 검증UEM(Unified Endpoint Management)UEM(Unified Endpoint Management) Device Management : Device 보안, 데이터 보호, SW 등 식별 및 관리Asset Management : 하드웨어, 네트워크, 사용자 정보 수집, Device 분실 시 잠금 및 초기화Application Management : OS, 필수 S/W(브라우저, 메신저, 보안 프로그램, 인증서 등)을 자동 설치 및 패치 관리Policy : Device 보안 정책 준수(화면보호기, Disk 암호화 등)EPP(Endpoint Protection Platform)Anti-Malware : 악성코드, 랜섬웨어, 스파이웨어 등의 위협으로부터 보호EDR(Endpoint Detection & Response) : 실시간 악성코드,공격 탐지 및 대응\nDevice 행동(File System, Registry 변경, Process 실행 등) 분석을 통한 보안 위협 탐지 및 대응 탐지된 보안 위협 정보 분석을 통한 유사한 위협 예방 및 대응Device Compliance : Device에 대한 OS 보안 설정과 EPP의 보안 상태 평가, 분석, 취약점 및 위협 식별\nIAM, SASE & ZTNA와 연계를 통한 접근제어\n전환 절차 IAM 온보딩관리자가 사전에 팀원들에 정보를 받아서 IAM 계정을 생성을 하고,\n팀원들은 Password 설정 & MFA (Multi Factor Authentication)를 설정합니다.Active Directory 제거 & UEM Join더 이상 Active Directory와 PMS를 통해 Device의 보안 설정과 SW 패치 관리를 하지 않을거라 Device에 스크립트를 실행시켜서 AD Join을 풀고, UEM에 IAM을 통해 로그인을 합니다.\n로그인을 완료 하면, 자동으로 미리 설정해둔 Device의 보안 설정(예시: 화면보호기 설정, 디스크 암호화 등)을 적용하고, 더이상 사용하지 않는 Application 들은 삭제하고, 앞으로 사용해야하는 Application(웹브라우저, 메신저, EPP, SASE, ZTNA)는 자동으로 설치 됩니다. SASE & ZTNA Login 마지막으로 SASE & ZTNA Agent에 IAM 계정을 통해 로그인을 하면, Zero Trust 환경으로 전환 과정은 완료 됩니다. \n\nSASE와 ZTNA는 기존에 사용하던 방화벽 정책을 기반으로 Migration은 하지만, IP 기반 접근제어가 아닌, \nRBAC(Role Based Access Control) 기반 즉, 토스의 조직 정보를 기반으로 정책을 Migration 하였습니다. 도입 후Login저희가 사용중인 Application들을 IAM과 SSO 연동을 통해서 Application에 로그인 할 때에는 IAM 통에 인증을 완료 후 접근합니다. \n\nIAM 연동을 하기 전에는 ID/Password + OTP를 통해서만 검증을 했었다면, 이제는 신뢰된 Network 인지, 회사 자산인지, Device의 보안 수준은 준수되는지를 검증을 추가로 하여 접근을 합니다. 이 과정은 생체 인증으로 인증을 하는 과정에서 검증이 됩니다. 단, 검증하는 과정이 생겨도 로그인 속도가 지연되지 않습니다.NetworkSaaS Application이나 인터넷을 접근할 때에는 SASE가, On-Premise나 Public Cloud와 같은 Private 네트워크에 접근할 때에는 ZTNA가 동작을 하게 되는데요. \n\n회사에서 근무를 할 때와 재택근무를 할 때 위치에 큰 제약 없이 회사에 있을 때와 동일한 보안 환경으로 업무용 시스템 어디든 접속이 가능합니다. \n단, 업무용 시스템에 접속 할 때 접속하는 Device가 보안을 준수하고 있는지 빠르게 검증하고, 인터넷을 할 때는 피싱 사이트와 같은 곳으로부터 안전하게 회사의 자산과 데이터를 보호합니다.RBAC & ABAC 인사DB와 IAM을 연동하고, IAM과 Application, Network 등을 연동하여 팀원의 직무(Role)을 기반의 접근제어를 통한 접근제어 정책을 구성하여 보안정책의 가시성을 확보하고, 팀원의 퇴직 시에는 자동으로 할당된 권한이 회수되고, 직무의 변경이 있을 때 자동으로 권한이 회수되고, 변경된 직무에 맞는 권한으로 재 할당이 됩니다.마치며토스에서 Zero Trust 아키텍처 도입을 통해 보안 가시성 확보, 보안 관리의 효율화를 하면서도 팀원들의 업무 편의성도 향상되었습니다. 글을 마무리하면서, 글의 내용을 요약하자면 아래와 같습니다. Zero Trust란 고정된 네트워크 경계를 방어하는 것에서 사용자, 자산, 자원 중심의 방어로 변경하는 발전적인 사이버 보안 패러다임입니다. IAM, ZTNA, UEM, EPP, SIEM등을 연계하여 상호 보완적으로 Zero Trust 아키텍처를 구축하였습니다.Zero Trust 아키텍처를 도입 후 각 영역에서 보안 가시성 확보, 보안 관리의 효율화를 진행하면서도 팀원들의 업무 편의성도 향상되었습니다.DLP(Data Loss Protection)을 통한 회사의 중요 데이터(내부자료, 개인정보) 유출 모니터링, 방지\nFile Type Control: File 확장자 기반의 Upload / Download Contents 식별 및 차단\nCASB(Cloud Access Security Broker)를 통한 Cloud에 보관된 중요 데이터 유출 방지재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)2023. 08. 31레고처럼 조립하는 토스 앱2023. 08. 22",
    "26": "환경 고민없이 개발하기김동현ㆍFrontend Developer2023. 9. 1서론토스 프론트엔드 챕터는 유저가 경험하는 로딩 시간을 줄이기 위해 지속적으로 노력하고 있습니다. 특히 Slash 22를 통해 서버 사이드 렌더링(SSR)를 이용한 개선 사례를 소개드린 적이 있는데요. 이번 아티클에서는 Next.js 도입 과정에서 마주한 문제와 해결 방법에 대해 설명드리겠습니다.서버 사이드 렌더링 서버 사이드 렌더링(SSR)은 렌더링 작업 일부를 서버에 위임하는 방식으로, 브라우저에게 완성된 HTML을 전달합니다. 이를 통해 사용자는 빠르게 서비스를 이용할 수 있고, 해당 서비스는 검색 엔진 최적화(SEO)를 통해 더 많은 노출 기회를 얻을 수 있습니다.하지만 서버 사이드 렌더링(SSR)을 위해서는 별도의 서버 운영이 필요합니다.  프레임워크를 사용하는 경우, 서버 구축 및 운영 등의 문제에는 벗어날 수 있지만 렌더링 과정에 서버가 개입되면서 window is not defined 와 같은 생소한 에러를 경험하게 됩니다.단순하게 생각해보면 서버에서 제공한 HTML을 이용한 것뿐인데, 왜 이런 에러를 경험하게 되는걸까요? 서버 사이드 렌더링(SSR)환경에서 흔히 발생하는 에러와 그 해결법을 사례를 통해 살펴보겠습니다.Next.js 렌더링 과정예시로 살펴볼 애플리케이션은 쿼리 파라미터로 전달받은 유저의 이름을 화면에 출력합니다.function App() {\n\t// 쿼리 파라미터로 전달받은 유저의 이름을 얻어온다.\n\tconst name = new URL(location.href).searchParams.get(name);\t\n\t// 유저의 이름을 화면에 출력한다.\n\treturn <div>{name}</div>\n}코드를 살펴보면 문제없이 동작할 것 같지만 서버 사이드 렌더링(SSR) 환경에서 에러가 발생합니다. 어떤 부분이 에러를 일으키는걸까요? 에러 메시지를 보며, 원인을 찾아보겠습니다.1. 서버가 HTML을 생성한다.메세지를 살펴보면, 에러가 발생한 환경은 다음과 같은 특징을 가지고 있습니다.브라우저 객체인 location 이 존재하지 않는다. (location is not defined )💡 location은 브라우저 환경에서 제공되는 객체로, URL 관련 속성 및 메소드를 제공합니다페이지(HTML) 생성이 가능하다. ( This error happened while generating the page. )즉  1) 브라우저가 아니면서 동시에 2) 페이지(HTML) 생성이 가능하다는 사실을 통해 서버에서 발생한 에러임을 추측 해볼 수 있습니다.서버는 클라이언트에서 제공한 컴포넌트를 기반으로 HTML을 생성합니다. 이 때 만약 클라이언트 환경에만 존재하는 코드가 있다면 어떤 일이 일어날까요? 서버는 해당 코드의 작동 방식 을 이해할 수 없고, 이로 인해 에러가 발생하게 됩니다.처음 작성한 코드를 다시 돌아가보면 location 은 클라이언트 환경에만 존재하는 브라우저 객체입니다. 따라서, 해당 에러를 해결하기 위해서는 서버 환경에서 location 에 접근할 수 없도록 수정해야 합니다.function App() {\n\tconst name = (() => {\n\t\t/* 서버 환경인 경우, 객체에 접근하지 못하도록 수정 .. */\n\t\tif ( isServer() ) {\n\t\t\treturn null;\n\t\t}\n\t\t return new URL(location.href).searchParams.get(name);\t\n\t})();\n\t/* 유저의 이름을 화면에 출력한다 ..*/\n\treturn <div>{name}</div>\n}2. Hydration Mismatch서버 환경에서 브라우저 객체에 접근할 수 없도록 수정한 후, 새로운 에러가 발생하였습니다.위 에러를 해결하기 위해서는 Hydration 에 대한 이해가 필요합니다.서버에서 생성한 HTML은 단순 마크업이므로 사용자 인터랙션이 불가능합니다. 따라서 React는 이벤트 리스너, 상태 관리와 같은 클라이언트 로직을 전달받은 HTML과 통합하여 애플리케이션으로 작동할 수 있도록 합니다. 이 과정을 Hydration 이라 합니다.여기서 주의깊게 봐야할 점은 로직 연결 과정입니다. React는 요소(Element)와 로직 정보가 담긴 가상 DOM을 생성한 뒤, 이를 전달받은 HTML과 비교합니다. 따라서, 서버와 클라이언트의 렌더링 결과가 같은 경우에만 Hydration 을 수행할 수 있습니다.첫 번째 수정사항으로 서버에서 바라보는 name 변수의 값은 항상  null  입니다. 따라서 쿼리 파라미터가 존재하는 경우, 서버와 클라이언트는 각각 다른 결과물을 렌더링 하게 되면서 Hydration 을 수행할 수 없는 상태가 됩니다.// 서버\n<div>{null}</div>\n\n// 클라이언트\n<div>{'김토스'}</div>하나의 코드, 동일한 결과 Isomorphic 그렇다면 어떻게 문제를 해결할 수 있을까요? Hydration Mismatch 를 해결하기 위해서는, 서버와 클라이언트의 렌더링 결과물이 같아야합니다. 이를 위해 서버 환경에서 쿼리 파라미터에 접근할 수 있는 별도의 로직 작성이 필요합니다. function App() {\n\tconst name = (() => {\n\t\tif (isServer()) {\n\t    /* 서버 환경에서 쿼리 파라미터 접근 및 반환하는 별도 로직... */\n\t\t}\n\t\t return new URL(location.href).searchParams.get(name);\t\n\t});\n\treturn <div>{name}</div>\n}다행히도 Next.js는 개발자가 겪을 불편함을 줄여주고자 useRouter() 을 제공하고 있습니다.import { useRouter } from 'next/router';\n\nfunction App() {\n\tconst name = useRouter().query.name;\n\n\treturn <div>{name}</div>\n}useRouter() 를 사용하면 별도의 예외처리 없이도 서버, 클라이언트 어떤 환경에서든 동일한 결과 값을 보장 받을 수 있습니다.// 서버\n<div>{'김토스'}</div>\n\n// 클라이언트\n<div>{'김토스'}</div>이처럼 서버와 클라이언트 양측에 동일한 결과를 보장하는 코드를 isomorphic 하다고 표현합니다. 요구사항을 다시 살펴보면, 쿼리파라미터 값을 화면에 출력하는 매우 간단한 작업입니다. 그러나 서버 사이드 렌더링 환경에 대한 이해가 없었다면, 에러를 해결하는데 많은 시간을 소비했을 것입니다. 만약 처음부터 useRouter() 를 사용했다면 어땠을까요? isomorphic 한 코드는 나와 동료의 시간을 절약해줍니다. 일관된 결과를 서버와 클라이언트 양측에 보장하기 위해서는 관련 작업이 반드시 필요합니다. 따라서, 이러한 작업들을 추상화 해둔다면 불필요한 코드들을 감춰지고, 구현에만 집중할 수 있게 됩니다.토스의 isomorphic그렇다면 실제 서비스에 적용해볼 수 있는 실용적인 사례는 없을까요? 토스 프론트엔드 챕터에서 사용하고 있는 isomorphic 사례를 소개드리고 마무리 하겠습니다.SSRSuspense<Suspense /> 는 비동기 요청을 선언적으로 처리할 수 있도록 돕는 컴포넌트입니다.Promise가 대기 상태일 때(Pending) : <Loading />Promise가 완료됐을 때(Resolved): <APIRequestComponent />function App() {\n\treturn (\n\t\t<Suspense fallback={<Loading/>}>\n\t\t\t<APIRequestComponent />\n\t\t</Suspense>\n\t)\n}그러나 React 18 버전 미만에서는 <Suspense/>가 오직 클라이언트 환경에서만 정상 작동한다는 한계점이 있습니다. SSR 환경에서 안정적으로 작동할 수 있도록 <Suspense/>는 컴포넌트가 마운트  되기 전에는 fallback 컴포넌트를 렌더링합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "27": "프론트엔드 다이빙클럽에서 만나는 아고라: 다른 회사에선 테스트 코드 어떻게 짜요?서현석ㆍFrontend Developer2023. 10. 11안녕하세요. 토스증권 프론트엔드 개발자 서현석입니다.여러분은 ‘아고라(agora)’라는 단어를 들으면 어떤 뜻이 가장 먼저 떠오르시나요? 어원으로 거슬러 올라가면 그리스 도시국가(폴리스)에서 시민들이 자유롭게 토론을 벌이던 장소이자 고대 그리스어로는 ἀγορά, 즉 열린 공간, 광장을 뜻합니다. 아고라에서는 그리스의 정치, 경제, 사회 같은 주요 분야들에 대한 의제를 논의하고 결정할 뿐만 아니라, 사회적·문화적인 만남의 장소가 되어 인문학, 철학 그리고 예술에 대한 토론이 이루어졌다고 하죠.저는 프론트엔드 다이빙 클럽(이하 ‘프다클’)이 프론트엔드 커뮤니티의 아고라가 되어가길 기대합니다. 우리는 많은 개발 커뮤니티들을 접할 수 있지만, 일반적인 컨퍼런스나 세미나는 발표자가 중심이 되는 단방향의 정보 전달이 주를 이룹니다. 프다클에서도 발표자와 주제는 있지만, 참석자 분들 한 분 한 분이 작은 발표자가 되어 아고라의 자유 시민처럼 토론하기를 기대합니다.혹시 프론트엔드 다이빙 클럽에 대해 잘 모르신다면, 이 글을 확인해 주세요.프다클의 컨시어지가 되어 보았습니다.첫 모임 때에 비해 이번 회차에는 60명 정도의 참여자로 가장 많은 분들이 함께해 주셔서 인기를 실감할 수 있었습니다. 특히 이번 모집 신청은 약 30초 만에 빠르게 매진되기도 했는데요. 공개적으로 홍보하지 않고, 지인 기반 입소문으로 퍼져나가며 성장하는 커뮤니티다 보니 더 의미 있었던 것 같습니다.이번이 4번째를 맞이하게 된 프다클에서는 ‘프론트엔드 테스팅’을 주제로 진행되었습니다.지원해주신 발표자 분들 덕분에 준비된 2개의 세션은 아래와 같습니다.접근성이 주도하는 프론트엔드 자동화오늘도 테스트를 쓰기로 마음 먹는다. 그러나,어떤가요? 제목만 봐도 흥미 넘치는 주제일 것 같지 않나요? 저 같은 경우에도 평소에 테스트 코드 작성에 대한 기준이 모호하다 보니 같은 프론트엔드 챕터 내에서도, 어느 정도까지의 테스트 코드를 짜야 하는 지에 대한 토론을 한 경험이 있었습니다.접근성 주도의 테스팅을 짜는 방법은 발표자 분의 노하우 전수이기도 하면서, 직접 만든 라이브러리를 소개해주셔서 새로웠습니다. “프론트엔드에서 테스트 짠다고 하면 나를 비건처럼 바라본다”고 하셨던 말이 공감이 가서 웃음까지 놓치지 않았던 세션이었습니다.두번째 세션은 주입을 이용한 테스팅으로 의존성과 테스팅의 관계에 대해 생각할 수 있었습니다. 더 생산성 있는 테스트 코드 작성 방법의 대한 발표자 분의 고찰 기반으로 다양한 이야기의 섬토크가 오고 갔던 게 인상 깊었습니다.*섬토크란? 각 세션마다 해당 주제로 프다클에서 짜여진 각 조별로 토론할 수 있는 시간아무래도 프다클이 열리는 모임 시간이 출출한 저녁 시간이다 보니, 발표에 집중하지 못하지 않을까 걱정하는 저 같은 사람들이 있을 텐데요. 프다클에 참여하시면 양질의 핑거푸드가 마련되어있어 세션 시작 전과 종료 후 네트워킹 시간에 자유롭게 드실 수 있게 제공하고 있습니다.발표자에게만 마이크가 가는 것이 아니라 많은 분들이 서로 얼굴을 익히고 토론하는 시간이 있기 때문에, 기운 넘치게 이야기에 집중할 수 있었습니다. 많은 분들이 참여해주시는 만큼 취향에 따라 여러 종류로 드실 수 있게 비건 메뉴까지 준비해 두었으니 메뉴를 걱정하실 필요는 없습니다.올해 벌써 4번째를 맞이한 프다클이지만, 남은 기간 동안에도 많은 분들이 함께 할 수 있도록 준비하고 있습니다.처음과 지금을 비교해보면, 참여해주시는 분들의 피드백을 통해 매 회차 마다 성장해 가는 게 눈에 띄게 볼 수 있었는데요. 조금씩 삐걱거리던 부분들도 개선되고, 좋은 것들은 더 좋게 만들고자 하는 시도들이 있었습니다. 특별히 참여하신 분들에 한해 제공해 드리는 유니크한 굿즈들도 빼놓을 수 없을 것 같습니다. 매 회차 마다 조금씩 구성이 달라지기는 하지만 개인적으로 저는 커피사일로의 원두팩을 가장 좋아합니다.이외에도 매 회차 마다 제공되는 배지와 발표자에게 제공되는 스페셜 굿즈까지 준비되어 있으니 풍성한 프다클 커뮤니티가 될 수 밖에 없지 않을까요 ?만약 이 글을 읽고 관심이 생기셨다면 프다클에 함께 해주세요!프다클의 마스코트 수달이프다클 커뮤니티는 초대 기반으로 발전해오고 있습니다. 매 모임마다 슬랙에 가입된 기존 멤버 대상으로 티켓을 제공합니다. 기존에는 20장씩 제공해(선착순 10명, 추첨 10명), 이 분들이 지인을 동행하여 총 40명의 사람들이 모였지만, 더 많은 분들이 함께하실 수 있도록 30장으로 늘릴 계획도 가지고 있습니다.발표자 신청도 프다클에 참여할 수 있는 또 다른 방법입니다. 발표자 신청 폼은 토스 프론트엔드 트위터에 올라갈 예정이니, 참여 원하시는 분들은 구독해주세요.처음엔 초대 기반 커뮤니티가 잘 동작할지에 대한 확신이 없었지만, 거의 오픈과 동시에 마감되는 선착순 티켓팅을 보며 특정 주제에 대해 딥한 기술 네트워킹을 하고자 하는 분들이 정말 많이 계시다는 사실을 깨달을 수 있었습니다.프다클 합류를 희망하신다면, 구글폼을 통해 간단한 경력 정보와 함께 자기소개를 남겨주세요.\n남겨주신 정보를 토대로, 매 회차 모임의 방향성에 따라 함께할 분을 선별하여 초대권을 보내드릴게요.올 해의 마지막 프다클은 12월 중에 열릴 예정입니다. 많은 기대 부탁드려요!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠놀러오세요! 프론트엔드 다이빙 클럽2023. 07. 21",
    "28": "대규모 로그 처리도 OK! Elasticsearch 클러스터 개선기이준환ㆍData Engineer2023. 10. 12로그 수집 현황토스증권이 운영하는 서비스와 인프라에서는 매일 수많은 로그들이 생성되고 있고, 이를 Elasticsearch 클러스터로 수집하여 로그를 검색하고 분석하고 있습니다. 이러한 로그들은 약 100여 개의 로그 파이프라인을 통해 하루 기준으로 22테라 바이트, 약 170억 건의 로그를 인덱싱하고 있는데요, 서비스가 커질수록 수집되는 로그는 더욱 늘어나기 때문에 수평적으로 확장하고 안정적으로 운영하기 위해 지속적으로 클러스터의 개선이 필요합니다.실제로 SLASH23 발표를 준비하던 시기에는 피크 시간에 초당 60만의 인덱싱과 하루 56억 건 정도의 로그를 처리하였지만 수 개월이 지난 지금은 로그가 3배 이상 늘어나서 피크 시간에 초당 200만 이상의 인덱싱, 하루 기준으로 약 170억 건의 로그를 처리하고 있습니다.토스증권 Elasticsearch 클러스터는 온프레미스로 운영하고 있어 클러스터가 커질수록 상면 공간과 관리 부담이 있기 때문에 가능한 효율적으로 구성을 하는 것이 필요한데요, 거의 대부분의 로그 검색과 분석은 최근 보름 이내의 로그를 대상으로 하기에 Hot-warm 아키텍처를 도입하면 보다 효율적으로 운영할 수 있다고 판단하였고 Hot 노드보다 더 큰 디스크를 가진 Warm 노드를 구성한 후 생성된 지 오래된 인덱스는 Warm 노드로 이동하고, 일정 기간이 지나면 삭제하도록 하는 ILM (Index Lifecycle Management)을 구성하였습니다.이러한 구조를 채택하여 만약 최근 로그를 더 오래 보관하고 싶다면 Hot 노드 증설을 하고, 전체 로그를 더 오래 보관하고 싶다면 Warm 노드만 증설하는 등 목적에 따라 확장할 수 있도록 구성하였습니다.Elasticsearch 클러스터 안정화그런데 이처럼 수백 테라바이트 단위로 로그 수집 규모가 매우 커지면서 Elasticsearch 클러스터의 장애가 종종 발생하였는데요, 클러스터가 매우 느려져서 로그 인덱싱이 크게 지연되기도 하고 키바나에서 로그 검색을 할 때에도 검색 지연이 크게 발생했고 때로 데이터 노드가 내려가는 장애가 있었습니다.클러스터가 크게 느려졌을 때 노드들의 상태는 공통적인 패턴을 보였는데요, 매우 많은 fielddata 메모리를 사용하고 있었고 매우 긴 가비지 컬렉션이 빈번하게 발생하여 가비지 컬렉션이 끝날 때까지 클러스터가 일시적으로 멈추는 패턴이었습니다.매우 많은 fielddata 메모리 사용잘못 사용한 fielddata 옵션저희가 장애를 겪은 경우는 인덱스 매핑에서 일부 필드 설정에 fielddata 옵션을 잘못 사용한 경우였습니다.Elasticsearch 및 Lucene에 Doc value 스토리지가 도입되기 전에는 필드에 대해 aggregation과 정렬을 하기 위해서 필드의 값들을 fielddata라는 메모리 영역으로 올려서 fielddata에서 aggregation을 수행하고 정렬을 하였었는데요, 매우 큰 인덱스의 필드들을 메모리로 올려서 사용하다 보니 Elasticsearch의 JVM 메모리가 항상 부족해지고 매우 긴 가비지 컬렉션이 발생하여 이로 인해 클러스터가 일시적으로 멈추는 현상이 빈번했습니다. 그래서 Elasticsearch는 파일 시스템을 사용하고 컬럼 기반인 Doc value storage를 도입하여 aggregation과 정렬을 수행할 때 힙 메모리를 덜 사용하고 운영체제 레벨 캐시를 적극적으로 사용하는 방향으로 발전되어왔습니다.따라서 Doc value 스토리지를 지원하는 지금은 특별한 경우가 아니면 fielddata를 사용할 일이 없습니다. 인덱스의 크기가 작다면 문제가 되지 않겠지만 인덱스의 크기가 커지면 이는 금방 장애로 이어질 수 있습니다. 저희의 장애 당시 설정에서는 단지 7개의 필드에서 fielddata 옵션이 설정되어 있었는데 유입되는 로그의 양이 매우 많다 보니 이로 인해 데이터 노드의 힙 메모리가 금방 가득 차게 되고 가비지 컬렉션이 빈번하게 발생하였습니다. Doc value 스토리지를 지원하지 않는 text 타입을 aggregation 하려는 목적에서 fielddata를 사용할 수는 있지만 가급적 사용하지 말고 다른 방법으로 문제를 푸는 것이 좋습니다.인덱스 매핑또 하나의 Elasticsearch 클러스터의 안정성을 위협하는 것은 mapping explosion입니다. 유입되는 로그가 많아지고 규모가 커질수록 인덱스 매핑이 정말 중요해지는데요, Elasticsearch 클러스터가 느려지고 불안정한 경우 원인 분석을 하면 대부분 인덱스 매핑이 비효율적으로 정의되어 있는 경우가 많습니다.만약 관리하는 Elasticsearch 클러스터가 느려지고 불안정하다면 맨 먼저 인덱스 매핑을 살펴보는 것이 좋은데요, 인덱스에 너무 많은 필드들이 매핑되어 있는지와 fielddata 옵션을 사용하는 필드가 있는지, default dynamic mapping을 사용하고 있는지 점검해 보면 좋습니다.Elasticsearch는 명시적인 설정을 하지 않는다면 들어온 json의 형태 그대로 인덱싱을 하고 동적으로 인덱스 매핑을 업데이트하는데요,이런 특성으로 인해 입력으로 들어오는 json에 key가 매우 많다면 mapping explosion이 일어나고 마스터 노드가 클러스터 상태를 업데이트하고 관리하는 데 리소스를 매우 많이 사용하여 클러스터가 불안정하게 됩니다. 따라서 인덱싱할 데이터가 너무 많은 키를 가지지 않도록 해야 하지만 어쩔 수 없이 임의의 구조를 가진 데이터를 인덱싱해야 한다면 flattened type를 사용하거나 dynamic field를 false로 하는 것이 필요합니다. dynamic field 옵션을 false로 설정하면 명시적으로 매핑한 필드만 인덱싱되고 그 이외의 필드는 인덱싱되지 않기 때문입니다.\n{\n \"dynamic_templates\": [\n   {\n     \"strings_as_keyword\": {\n       \"mapping\": {\n         \"ignore_above\": 256,\n         \"type\": \"keyword\"\n       },\n       \"match_mapping_type\": \"string\"\n     }\n   }\n ]\n}{\n \"mappings\": {\n   \"dynamic\": false,\n   \"properties\": {\n     \"user\": {\n       \"properties\": {\n         \"name\": {\n           \"type\": \"keyword\"\n         }\n       }\n     }\n   }\n }\n}명시적인 매핑, flattened type, dynamic field를 끄는 것들은 결국 인덱스에 의도하지 않게 너무 많은 필드가 생성되지 않도록 하는 것이 목적입니다. 안정적인 Elasticsearch 클러스터를 운영하기 위해서 가장 중요한 점은 인덱스 필드를 제어할 수 있어야 합니다.적절한 샤드 개수그 외의 인덱스 설정에서 고려해야 하는 것은 인덱스의 샤드 개수입니다. 샤드 수를 결정하는 데에 정답은 없지만 토스증권에서는 헤비 인덱스의 경우 프라이머리 샤드의 개수는 Hot 노드의 수와 동일하게 하고 있습니다. 데이터 노드의 CPU 사용량이 여유가 있다면 프라이머리 샤드 개수를 더 늘려서 초당 인덱싱 처리량을 개선할 수 있어요. 다만 프라이머리 샤드를 더 늘린다면 샤드가 특정 노드에 쏠려서 핫스팟 노드가 될 수 있기 때문에 샤드의 수를 Hot 노드의 배수로 설정하는 것이 좋습니다.index refresh time은 60초로 설정해서 세그먼트 생성과 머지가 적게 발생하도록 하였고, flush_threshold_size는 기본값인 512MB의 두 배인 1GB로 설정하여 트랜스로그 플러시를 실행하는 빈도를 낮추도록 하였습니다. 마지막으로 슬로우 쿼리 로깅을 활성화하여 클러스터에 부하를 줄 수 있는 비용이 비싼 쿼리가 실행되는 것을 모니터링하고 있습니다.{\n \"index\": {\n   \"translog\": {\n     \"flush_threshold_size\": \"1024MB\"\n   },\n   \"refresh_interval\": \"60s\",\n   \"codec\": \"best_compression\",\n   \"search\": {\n     \"slowlog\": { … }\n   },\n   \"indexing\": {\n     \"slowlog\": { … }\n   }\n}Vector 로그 파이프라인 전환그 이후에 진행한 것은 로그 파이프라인 전환이었습니다. Elasticsearch로 로그를 인덱싱하는 로그 파이프라인은 현재까지 약 100여 개를 운영하고 있습니다. 로그 파이프라인은 Logstash를 사용하여 운영하였는데 Logstash는 범용적으로 사용할 수 있게 다양한 설정을 제공하지만 JVM 기반으로 만들어져있어 시스템 자원을 많이 사용하는 단점을 가지고 있습니다.Logstash 기반 파이프라인이 점점 늘어나서 쿠버네티스 클러스터에서 260GB 이상의 메모리를 사용하는 상황이 발생하였고 앞으로 로그 파이프라인은 더욱 늘어날 예정이기 때문에 Logstash를 대체할 수 있는 경량 로그 파이프라인으로 전환이 필요하였습니다.로그 파이프라인 전환을 위하여 여러 경량 로그 파이프라인 오픈 소스들을 검토하였고 그중 로그 가공과 정제를 코드로 유연하게 작성할 수 있는 vector를 선택하였습니다.Vector는 Datadog에서 공개한 Rust 기반 경량 log shipper이고 고성능과 메모리 효율을 목표로 하여 높은 워크 로드 환경에서 리소스 효율적으로 로그 파이프라인을 운영할 수 있습니다. 또한 다양한 source들과 sink들을 제공하고 있어서 목적에 맞게 로그 파이프라인을 구성하고 확장할 수 있는 장점이 있습니다.그리고 Logstash에서 아쉬웠던 부분이 로그 파이프라인 모니터링이었는데 Vector로 전환한 후 prometheus exporter로 로그 파이프라인 모니터링을 쉽게 구현할 수 있었고, 로그 파이프라인 모니터링을 통해 파이프라인에 문제가 생겼을 때 이를 빨리 파악하고 개선이 필요한 부분을 쉽게 알 수 있게 되었습니다.Vector로 로그 파이프라인을 전환한 후 시스템 자원을 많이 절약할 수 있었는데요, Logstash는 범용적으로 사용하기 좋은 장점이 있지만 JVM 기반으로 되어 있어 시스템 리소스를 제법 많이 사용하는 문제가 있었고 운영의 편의성을 위해 로그 파이프라인들을 각각 별도의 프로세스로 띄우고 있었기 때문에 Logstash 인스턴스가 많아지면 메모리 사용량이 많을 수밖에 없었는데 이 부분을 해소할 수 있었습니다. 기존에 약 260GB 정도 사용하고 있던 메모리 사용량이 10GB 수준으로 크게 줄었습니다. 약 96% 이상 메모리를 절약하는 성과가 있었습니다.Vector 전환 이후 시간이 흘러 로그 파이프라인이 더 늘어나고 유입되는 로그도 3배 이상 증가한 현재는 Vector 파이프라인들의 메모리 사용량은 약 23GB 정도로 측정되었습니다. 기존 Logstash 기반의 로그 파이프라인을 그대로 유지하고 있었다면 쿠버네티스 클러스터에서 수백 GB의 메모리를 사용하게 되었을 상황을 예방할 수 있었습니다.여러 데이터센터 간 클러스터링로그 파이프라인을 vector로 전환한 후 다음으로 진행한 것은 데이터센터 확장이었습니다. 데이터센터 이중화를 위하여 새로운 데이터 센터가 추가되어 새로운 데이터 센터에서 생성되는 로그를 수집하는 Elasticsearch 클러스터가 필요해졌는데요, 이렇게 커진 Elasticsearch 클러스터를 한 세트 더 구축하고 운영할 수도 있지만 더 나은 방법이 없을까 고민을 하였고 데이터센터 간에 하나의 Elasticsearch 클러스터를 구축할 수 있을지 검토하였습니다.사실 서로 다른 데이터센터의 Elasticsearch 노드들을 하나의 클러스터로 묶는 방법은 elastic에서는 권장하지 않는 방법입니다. 이는 노드들이 빈번하게 통신하는데 데이터센터 간의 네트워크 레이턴시가 높으면 클러스터의 전체적인 성능 저하가 발생하기 때문입니다.하지만 데이터센터 간의 거리가 짧고 네트워크 레이턴시가 작다면 리전 내의 가용성 존(AZ, Availability Zones)로 볼 수 있지 않을까 생각했습니다. AWS가 서울 리전에서 4개의 Zone으로 구성되어 있는 것처럼요. 그리고 수백 테라바이트의 로그 수집과 분석이 목적이기 때문에 조금의 지연보다 비용 절감으로 얻을 수 있는 장점이 더 크다고 생각했습니다.대신 Elasticsearch가 샤드를 복제하거나 복구할 때 많은 네트워크 트래픽을 점유하기 때문에 클러스터 안정성을 위해서 Elasticsearch 클러스터를 위한 전용 회선을 별도로 구축하였고 하나의 IDC가 장애가 났을 경우를 대비하여 replica shard는 서로 다른 IDC에 저장하도록 구성하였습니다.IDC 간에 하나의 Elasticsearch 클러스터를 구축하기 위해서는 투표 전용 마스터 노드가 필요합니다. 각각의 IDC에는 마스터 노드 1개를 배치하고, AWS에 투표 전용 마스터 노드를 배치하여 총 3대의 마스터 노드를 배치하는 구조입니다. 마스터 선출을 위한 투표만 수행하는 노드를 제3의 장소인 AWS에 배치하여 IDC1의 마스터 노드와 IDC2의 마스터 노드를 타이브레이커로 묶게 됩니다. 이를 통해 DCI 단절 시 발생할 수 있는 split brain 문제를 방지할 수 있습니다.또한 하나의 데이터센터가 장애가 발생하였을 때 데이터 유실을 방지하기 위하여 Shard awareness를 설정하여 primary shard와 replica shard가 서로 다른 IDC에 배치되도록 하였습니다. 그리고 일시적인 DCI(Data Center Interconnect) 장애 시 샤드 복제가 과도하게 일어나는 것을 방지하기 위해 force awareness를 설정하여 IDC1과, IDC2의 데이터 노드들이 클러스터에 합류하였을 때만 샤드 배치가 일어나도록 설정하였습니다. 마지막으로 전송 계층에서 인덱싱 데이터에 대해 압축 설정을 하면 전송 시 발생하는 네트워크 대역폭을 많이 줄일 수 있습니다.# IDC1의 데이터 노드\ntransport.compress: indexing_data\nnode.attr.zone: dc1# IDC2의 데이터 노드\ntransport.compress: indexing_data\nnode.attr.zone: dc2# 마스터 노드\ntransport.compress: indexing_data\ncluster.routing.allocation.awareness.attributes: zone\ncluster.routing.allocation.awareness.force.zone.values: dc1,dc2다음은 IDC1과 IDC2의 Elasticsearch 노드들을 하나의 클러스터로 묶은 전체 아키텍처 그림입니다. DCI 간 네트워크 단절이 발생한다면 IDC1 혹은 IDC2의 마스터 노드가 투표 전용 마스터 노드의 투표를 통해 마스터 노드로 선출되고 같은 구역에 있는 노드들만 클러스터에 남게 됩니다.이후 DCI 장애가 해소되면 반대편 IDC에 있는 노드들이 다시 클러스터에 합류하게 됩니다. 이런 구조를 통해 하나의 데이터센터 장애에도 견딜 수 있는 Elasticsearch 클러스터를 운영할 수 있게 되었습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠금융사 최초의 Zero Trust 아키텍처 도입기2023. 09. 01은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)2023. 08. 31레고처럼 조립하는 토스 앱2023. 08. 22",
    "29": "토스는 Gateway 이렇게 씁니다최준우ㆍServer Developer2023. 10. 12안녕하세요. 토스에서 Gateway를 개발하고 있는 서버플랫폼팀 최준우입니다. 토스에서는 목적에 맞는 다양한 Gateway를 사용하고 있는데요. 저는 이번 글에서 이러한 Gateway 아키텍처를 통해 토스가 누리고 있는 장점들과 이를 위해 어떠한 노력을 하고 있는지에 대해 간단히 소개하려고 합니다.Gateway 란?우선 Gateway에 대해 간단히 알아보겠습니다. Gateway는 라우팅 및 프로토콜 변환을 담당하며 마이크로 서비스의 중개자 역할을 하는 서버입니다. 이를 통해 서비스는 클라이언트와 독립적으로 확장할 수 있으며 보안, 모니터링을 위한 단일 제어 지점을 제공합니다. Netflix Zuul을 통해 잘 알려졌으며 현재는 Saas나 플랫폼으로도 사용할 수 있게 대중화되었습니다.그림을 통해 예를 들어 보겠습니다. 서비스가 적고 트래픽이 적다면 클라이언트에서 서비스를 직접 호출하고 각각의 서비스에서 모든 로직을 처리해도 큰 부담이 되지는 않습니다. 그러나 스케일이 커지면 공통의 로직을 모든 서버에 적용하고 배포하는 것도 큰일이 됩니다.서버가 무수히 많아지는 MSA 특성상 공통 로직을 서버마다 두기에는 어려움이 있다이러한 필요성을 위해 개발된 게 Gateway 패턴입니다. Gateway는 서버들에서 필요한 공통 로직을 통합하여 처리합니다. 모든 서비스에서 필요한 유저 정보, 보안 정책 등을 Gateway에서 처리하고 이를 업스트림 서버로 넘겨줍니다.Gateway 에서 요청을 1차로 받아서 공통 로직을 처리하고 서비스로 요청을 넘긴다Gateway는 요청이 오면 정의된 설정에 따라 요청을 라우팅하고 사전에 설정된 필터들을 작동시킵니다. 설정은 Route 단위로 구성이 되며 Route는 다시 Predicate와 Filter로 구성됩니다. Predicate는 요청을 구분할 때 사용하는 값인데, Path, Method, Host 등으로 요청을 매칭하고 Filter는 매칭된 요청에 대한 전처리나 서비스의 응답에 대한 후처리를 구현합니다.저희는 이러한 Gateway 들을 Spring Cloud Gateway를 사용하여 구성하고 개발하고 있습니다. Spring Cloud Gateway는 스프링 Webflux를 통해 구현되어 있으며 내부적으로 Reactor-Netty를 사용하여 비동기 처리를 지원하고 있습니다. 또한 필터 개발에 Kotlin Coroutine을 적극 활용하고 있으며 Istio의 Ingress / Egress Gateway 및 Envoy 필터와 함께 유기적으로 개발하고 있습니다.공통 로직 처리이제 앞에서 소개 드렸던 공통 로직들을 저희가 어떻게 개발하고 사용하고 있는지에 대해 간단히 소개해 드리겠습니다.Gateway에서 공통 로직을 처리하는 부분은 크게Request에 대한 전처리, 후처리유저 정보를 이용한 로직 수행보안 그리고 서비스 안정화를 위한 설정등이 있는데, 몇 가지 사례와 그림을 토대로 설명드리도록 하겠습니다.Sanitize우선 Request 처리입니다. Gateway에서 우선적으로 처리해 줘야 하는 것은 Request를 Sanitize 하는 것입니다. Sanitize는 Client로부터 올바르지 않은 요청이 올 경우 이를 지우거나 올바른 값으로 바꿔주는 것을 의미합니다. 그림처럼 사용자가 악의적으로 값을 주고 요청하더라도 Gateway에서 이를 올바른 값으로 바꿔서 서비스에 넘겨줍니다.유저 Passport토스 내부 서비스들도 기존에는 모든 서비스에서 유저 정보가 필요할 때 유저 API를 호출하는 방식으로 유저 정보를 가져오고 있었는데요 이는 트랜잭션 내에 불필요한 중복 요청을 유발하고 서버 리소스의 낭비로 이어졌습니다. 이를 개선하기 위해 저희는 Netflix의 Passport 구조를 참고하였습니다. Netflix는 유저 인증 시에 Passport 라는 id 토큰을 트랜잭션 내로 전파하는 방법을 사용하고 있는데요. 저희는 Netflix의 Passport 구조를 저희 팀에 맞게 변경하여 토스 Passport를 구현했습니다.Passport는 사용자 기기 정보와 유저 정보를 담은 하나의 토큰인데요. 앱에서 유저 식별키와 함께 API를 요청하게 되면 Gateway에서 이 키를 토대로 인증 서버에 Passport를 요청합니다. Passport에는 디바이스 정보와 유저 정보가 담겨 있으며 Gateway는 이를 serialize 하여 서비스에 전파합니다. 유저 정보가 필요한 서비스는 유저 정보 호출 없이 Passport 정보를 통하여 유저에 대한 정보를 사용할 수 있습니다.트랜잭션 내에서 마치 여권처럼 사용자 정보를 들고 다니게 됩니다.보안과 안정성토스는 금융 앱인 만큼 높은 수준의 보안 요구사항이 존재합니다. 저희는 Gateway에서 이러한 요구사항을 만족하기 위하여 다양한 보안 로직을 수행하고 있는데요. 아래에서는 Gateway가 핵심적으로 수행하고 있는 부분을 몇 가지 소개해 드리겠습니다.종단간 암호화토스 앱에서 사용하고 있는 대부분의 API는 종단간 암호화를 통해 패킷 분석의 허들을 높여 안전하게 정보를 전달하고 있습니다. 앱에서 암호화 키를 사용하여 요청 바디를 암호화하고 Gateway에서 이를 복호화 하여 서비스에 전달합니다. 복호화 과정에서 인증 / 인가 로직이 처리되고 복호화 된 데이터와 유저 정보를 서비스로 넘겨주게 됩니다. Gateway에서 이 과정을 전부 처리하기 때문에 서비스에서는 편하고 안전하게 사용자의 요청을 처리할 수 있게 됩니다.Dynamic Security토스는 위에서 말한 인증 / 인가를 넘어서 각 요청이 실제로 위변조 되지 않은 토스 앱에서 만들어진 요청인 지도 검증을 합니다. 토스 앱은 내부적으로 매 요청을 서명할 아주 짧은 유효기간을 가진 안전한 키 값과 변조되지 않은 토스 앱에서만 알 수 있는 정보를 활용하여 각 요청을 서명하고 이를 Gateway로 보내게 됩니다.Gateway에서는 각 요청에 들어있는 서명 값을 통해서 토스 앱에서 만들어진 요청인지, 중복해서 사용되지 않았는지, 그리고 유효기간이 만료된 키로 만들어지진 않았는지 검증하여 앱 위변조, delayed request, replaying attack을 방지하고 의심스러운 요청이 발견되면 FDS(fraud detection system)를 통해 계정을 비활성화하여 사용자를 안전하게 보호하고 있습니다.인증서를 이용한 인증 / 인가Gateway에서는 토스 앱 뿐만 아니라 외부 회사나 내부 개발자의 서비스 호출을 위해 클라이언트 인증서를 이용한 mTLS API 호출도 지원하고 있습니다. 기본적으로 Istio에서 제공하는 mTLS flow 위에 Gateway 애플리케이션을 두어 인증 / 인가 처리를 하고 있습니다. Istio 만 이용하여 인증/인가를 처리할 수도 있지만 코드 베이스의 애플리케이션이 Istio의 matching rule보다 자유도도 높고, Auditing 등의 로직을 처리할 수 있으며 카나리 배포의 이점을 누릴 수 있기 때문에 Gateway에서 인증 / 인가 처리를 담당하게 되었습니다.Edge에서 Istio는 Client 인증서의 CA 유효성을 확인한 후, 해당 인증서 정보를 헤더에 실어서 모든 트래픽을 Gateway에 전달해 줍니다. 이렇게 받은 인증서를 Decode 하여 X.509 extensions 중 Subject Alternative Name을 활용하여 인증서로부터 사용자 정보를 얻게 됩니다. 이렇게 얻은 사용자 정보와 도착지 호스트 및 요청 경로를 활용하여 각 요청에 대한 인증 / 인가 및 Auditing 처리를 하고 있습니다.Circuit breaker사용하는 마이크로 서비스 아키텍처 패턴은 많은 서비스들이 거미줄처럼 서로 상호작용을 하고 있습니다. 따라서 그중 하나의 서비스에서 응답 지연이 발생하면, 해당 서비스에 의존하는 수많은 서비스들에게 응답 지연이 전파됩니다. 이렇게 퍼져 나간 응답 지연이 시스템의 자원을 점유하여 모든 시스템이 먹통이 되는 상황이 발생합니다. 이를 방지하기 위해서는 응답 지연을 유발하는 서비스에게 요청을 더 이상 보내지 않고 빠르게 실패하게 하여 부하를 겪고 있는 서비스가 회복할 수 있게 돕고, 이러한 응답 지연이 전체 서비스로 확산되지 않게 하는 것이 중요합니다. 이를 서킷 브레이크 라고 부릅니다.또한 내부 서비스 간의 서킷 브레이킹도 중요하지만 근원적인 트래픽을 발생시키는 Client에게 백프레셔를 빠르게 주기 위해서는 Gateway에서 서킷 브레이킹을 거는 것이 중요합니다. 서킷 브레이크를 적용하는 방법에는 Istio를 활용한 인프라 레이어의 서킷 브레이킹 혹은 Resilience4J나 Hystrix와 같은 라이브러리를 이용한 애플리케이션 레이어의 서킷 브레이킹이 있습니다. 각 방법에는 장단점이 존재하는데요, Istio를 활용한다면 호스트 단위로 쉽고 빠르게 전체 적용이 가능하며 애플리케이션의 개발 주기와 독립적으로 관리될 수 있다는 장점이 있지만 Istio는 호스트 단위로만 서킷 브레이킹 설정이 가능하며, 설정할 수 있는 룰에도 한계가 있습니다.따라서 토스에서는 보이는 것 같이 각 애플리케이션이나 Gateway에 서킷 브레이킹을 적용함으로써 호스트나 Route 단위 혹은 기능단위로 정교하게 서킷 브레이킹을 걸고 있습니다.모니터링Gateway는 토스의 모든 서비스가 거치는 컴포넌트인 만큼 보다 꼼꼼한 모니터링이 필요합니다. 모니터링에 중요한 요소로는 로깅, 메트릭, 트레이싱이 있는데요, 저희 팀에서 각각 어떻게 기록하고 모니터링하는지 소개하겠습니다.로깅저희는 Gateway를 지나는 모든 요청, 응답의 Route id와 method, URI, 상태 코드 등을 Elasticsearch에 남기고 있습니다.덕분에 요청이 어떤 Route로 들어왔는지, 업스트림으로 어떤 URI를 호출했는지에 대한 정보를 바로 확인할 수 있습니다.메트릭다음은 메트릭입니다. 메트릭에는 크게 시스템에서 수집하는 메트릭과 애플리케이션에서 수집하는 메트릭이 있습니다. 두 메트릭 모두 Prometheus가 수집하는데요, Node Exporter를 통해 수집된 시스템 메트릭과 Spring의 actuator를 통해 수집된 애플리케이션 메트릭을 Grafana를 이용해 시각화하고 슬랙으로 알림을 보내고 있습니다.시스템 메트릭에는 CPU, memory, 네트워크 RX, TX 트래픽 등이 포함되어 애플리케이션 수정 사항이 시스템에 주는 영향을 1차로 파악할 수 있고, 문제가 생기는 경우 현상과 원인 파악에 활용할 수 있습니다.애플리케이션 메트릭에서는 JVM thread block 상황이나 세대별 메모리 할당을 파악하고 full GC 발생 여부 등을 확인하고 있습니다.Spring Cloud Gateway에서는 Route 별 메트릭도 제공하는데요, 저희는 여기에 Path 값을 더해 API Path 별 Route 메트릭을 확인하기도 합니다.마무리토스팀에서 Gateway를 사용하여 해결하고 있는 몇 가지 사례에 대해 소개해 드렸습니다. 사례들처럼, Gateway는 외부에 노출되는 엔드포인트에 대해 중앙 집중식으로 관리할 수 있도록 도와줍니다. 이를 통해 트래픽을 모니터링하고 속도를 제한하거나 요청 및 응답을 요구사항에 맞게 수정하는 등 거대한 마이크로 서비스 아키텍처 클러스터를 보다 쉽게 확장, 관리 및 모니터링 할 수 있도록 합니다.위 글을 통해 Gateway 사용에 많은 인사이트를 얻으셨길 바라면서 소개 드린 사례들 외에도 더 많은 사례들을 보고 싶으시다면 SLASH23 영상도 같이 보시는 것도 좋을 것 같습니다.감사합니다!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠대규모 로그 처리도 OK! Elasticsearch 클러스터 개선기2023. 10. 12금융사 최초의 Zero Trust 아키텍처 도입기2023. 09. 01은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)2023. 08. 31레고처럼 조립하는 토스 앱2023. 08. 22",
    "30": "유연하고 안전하게 배포 Pipeline 운영하기김동석ㆍDevOps Engineer2023. 10. 12토스뱅크에는 400개가 넘는 배포 Pipeline 이 있습니다. Pipeline의 개수가 많아지고, 종류가 다양해지고, 동작이 복잡해지며 여러 어려움이 생기는데요. 토스뱅크에서 Pipeline을 유연하고 안전하게 운영하기 위해 노력한 이야기를 소개해 보려 합니다.Pipeline이란?반복하는 일을 자동화하는 시스템을 말합니다. 서버 프로그램을 빌드하고 배포하는 Pipeline 이 가장 대표적인데요. 라이브러리를 업로드하거나 운영 작업을 자동화하는 데에도 많이 활용합니다. Micro Service Architecture를 적극적으로 활용하는 토스뱅크에서는 하루에도 수십~수백 번 서버를 배포하기 때문에, Pipeline 은 동료 개발자분들이 일상적으로 가장 많이 사용하는 시스템 중 하나입니다.Pipeline에 필요한 것토스뱅크에서는 새로운 서비스가 자주 생기고 서비스의 동작이 자주 변하기 때문에 Pipeline을 빠르게 만들고 수정할 수 있어야 합니다. 서비스를 안정적으로 운영하려면 Pipeline들이 일관성 있게 동작해야 하고요. 그리고 모든 Pipeline에서 반드시 지켜야 하는 Compliance 요건이 있습니다. 토스뱅크와 같은 금융권 회사들을 전자금융감독규정이 적용됩니다. 서버를 배포할 때 동료끼리 검증을 하는 등의 절차를 Pipeline에서 준수해야 합니다. 저희는 이런 요건들을 잘 충족하기 위해서는 Pipeline 설정을 중앙화하는 것이 효율적이라고 판단하였고, 여러 스쿼드가 사용하는 Pipeline 설정을 한 곳에 모아두고 DevOps Engineer가 주도적으로 운영하고 있습니다.GoCDPipeline 도구로는 Jenkins, GitHub Actions 등 여러 가지가 있는데요. 토스뱅크에서는 GoCD를 주로 사용합니다. GoCD는 Pipeline을 정의에 따라 Server가 작업을 할당하고 Agent가 실행하는 구조입니다. 널리 사용되는 Jenkins 와 비슷합니다.첫 번째 어려움: 가시성GoCD에서 새로운 Pipeline을 만들 때에는 웹 UI에서 Pipeline Wizard를 사용합니다. Pipeline 개수가 많지 않을 때에는 이렇게 Pipeline 을 만들고 수정해도 큰 어려움이 없었지만, Pipeline 개수가 많아지면서 어려움을 겪었습니다.토스뱅크에는 서비스 빌드/배포 Pipeline이 281개, 작업 자동화 Pipeline이 76개, 이외 Pipeline이 47개, 도합 400개가 넘는 Pipeline이 있습니다. Pipeline이 많으면 웹 UI에서는 어떤 Pipeline이 있는지 확인하는 것부터 쉽지 않습니다. 한 화면에서 보이는 Pipeline 개수가 많지 않기 때문인데요. 자신이 찾는 Pipeline의 이름을 알고 있다면 검색해서 찾을 수 있지만, 그렇지 않다면 모든 Pipeline을 훑어봐야 합니다.그리고 Pipeline 하나의 설정을 보는 것도 어렵습니다. 일반적으로 Pipeline은 여러 단계를 갖는데요. 웹 UI에서는 한 단계씩 볼 수 있기 때문에 각 단계의 순서와 동작을 확인하려면 여러 페이지를 왔다 갔다 해야 합니다. 마찬가지로 서로 다른 Pipeline의 설정을 비교하기도 어렵고요.가장 큰 문제는 Pipeline 설정이 어떻게 변했는지 알기 어렵다는 것입니다. Pipeline의 동작은 계속 변합니다. 배포 방식이 달라지기도 하고 Compliance 요건이 변하기도 하지요. 하지만 웹 UI에서는 지금의 설정만 볼 수 있기 때문에, Pipeline 동작이 무언가 이상해서 확인해 볼 때에 언제 어떻게 설정이 변했는지 알기 어렵습니다.Pipeline as Code이때에는 Pipeline as Code 가 도움이 되었습니다. Pipeline 을 웹 UI에서 정의하지 않고 코드로 표현하여 Git에 저장하는 것을 말하는데요. 저희는 Pipeline 설정을 YAML 파일로 작성하여 Git에 저장하고, GoCD 서버와 연동하는 gocd-yaml-config-plugin을 활용했습니다.모든 Pipeline 설정이 파일로 저장되어 있으니 어떤 Pipeline 이 있는지 Git 저장소에서 한 번에 볼 수 있습니다. 수백 개의 Pipeline 이 있더라도 한 폴더에서 모두 확인할 수 있지요.여러 단계로 이루어진 Pipeline 설정도 한 파일에서 모든 설정과 단계를 볼 수 있습니다.그리고 Pipeline 설정 파일이 Git 저장소에서 Version Control 되기 때문에 언제 어떻게 변경되었는지 확인할 수 있습니다.두 번째 어려움: 생산성토스뱅크의 Pipeline 개수를 약 6개월 간격으로 집계해 보면 1년에 2배 이상으로 많아지는 것을 볼 수 있습니다. 토스뱅크는 Micro Service Architecture를 적극적으로 활용하고 있어 새로운 서비스를 자주 만들기 때문에 Pipeline 을 빠르고 정확하게 만들 수 있어야 합니다. 한편, 새로운 서비스는 기존 서비스와 기술 스택과 구조가 비슷한 경우가 많기 때문에, 이미 있는 Pipeline 설정을 복사 붙여넣기해서 새로운 Pipeline을 만들 때가 많습니다. 이때 설정을 잘못 붙여넣기도 하고, 바꾸어야 할 부분을 빼먹는 등 실수를 하기 쉽습니다. 그리고 모든 Pipeline에 공통된 변경 사항이 있을 때에는 모든 파일을 수정해야 하는데, 이때에도 실수를 하기 쉽습니다. 잘못된 설정을 찾아내고 바로잡는 것은 Pipeline을 빠르게 만들고 수정하는 일에 걸림돌이 되었지요.GoCD Template이 문제에는 GoCD Template이 도움 되었습니다. Pipeline 동작을 추상화해서 Template으로 만들고 여러 Pipeline에서 공유하는 기능인데요. 앞서 살펴 본 gocd-yaml-config-plugin과도 함께 사용할 수 있습니다.공통 설정은 Template을 지정하는 한 줄로 대체되어 훨씬 간결해지고, 서로 달라야 하는 설정은 변수로 주입할 수 있습니다. 공통 변경 사항이 있을 때에는 Template에서 변경하면 모두에게 적용되고요. Pipeline을 만들고 수정할 때 실수를 할 여지가 상당히 줄어듭니다.세 번째 어려움: 확장성프로그래밍에서 변경에 유연하게 대처할 수 있는 코드를 확장성이 높다고 이야기하는데요. Pipeline 역시 설정을 변경할 때 유연하게 대처할 수 있어야 합니다. 하지만 지금까지 살펴 본 방식으로는 Pipeline 설정은 Git에서, Template 설정은 웹 UI 에서 봐야 하기 때문에 둘을 비교하면서 Template을 수정하는 것이 꽤나 불편했습니다. 그리고 Template을 수정하면 많은 Pipeline에 한 번에 적용되기 때문에 실수를 했을 때 영향이 굉장히 큽니다. 결과적으로 Pipeline 설정을 유연하게 변경하기 어려웠는데요.Helm Template저희는 Template Rendering 도구인 Helm Template으로 이 문제를 해결해 보기로 했습니다. Helm Template은 공통 부분을 Template으로 만들고 고유한 부분을 Values로 주입하여 Rendering 할 수 있으며, 조건문과 반복문, 변수를 활용하여 꽤나 복잡한 내용도 표현할 수 있습니다.Helm Template은 YAML 파일을 잘 지원하기 때문에 앞서 보았던 gocd-yaml-config-plugin과도 잘 어울립니다. YAML로 된 Pipeline 설정을 추상화하여 Helm Template으로 만들면 GoCD Template을 대체할 수 있을 뿐만 아니라, GoCD Template에서 지원되지 않는 부분도 Template으로 사용할 수 있습니다. 이렇게 만든 Helm Template은 파일로 저장되기 때문에, Pipeline 설정 파일과 동일한 Git 저장소에 두면 Pipeline 설정과 Template 설정을 한 곳에서 확인할 수 있습니다.그리고 Helm Template의 조건문을 활용하면 Template 변경을 일부 Pipeline에서만 테스트해 볼 수도 있습니다. 전체 Pipeline에 적용하기 전에 테스트용 Pipeline에서 충분히 동작을 검증할 수 있게 되었고, Template 변경 실수가 모든 Pipeline에 영향을 주는 일이 줄어들었습니다.한편, Helm Template을 활용하면 호환성에서도 이점이 있습니다. GoCD Template 등 Pipeline 도구 고유의 기능은 그 도구에서만 사용할 수 있지만, Helm Template의 Values 파일은 Pipeline 도구와 무관한 추상적인 설정을 담고 있습니다. GitHub Actions 등 다른 Pipeline 도구를 사용할 때에는 그에 맞도록 Template 파일만 새로 작성하면 Values 파일을 재사용할 수 있습니다.네 번째 어려움: 복잡성Pipeline Template은 시간이 지나며 점점 복잡해집니다. 2023년 1월 기준으로 토스뱅크의 Pipeline은 5가지 Type과 72개의 설정값이 있고, Template 파일 길이를 모두 더하면 1,182 줄이 됩니다. Template이 복잡할수록 실수로 오타를 내거나 의도하지 않은 변경을 만들기 쉽습니다. Helm Template 문법은 일반 프로그래밍 언어만큼 직관적이지는 않기 때문에 더욱 그렇습니다. Pipeline Template을 잘못 수정해서 배포가 안되거나 잘못된 배포가 되는 일이 종종 있었는데요.CI저희는 CI를 도입해 보기로 했습니다. CI는 Continuous Integration의 약자로, 변경 사항에 대하여 자동으로 검증을 진행하고 성공했을 때에만 반영하는 것을 말합니다. Template 파일은 develop 브랜치에서만 수정하고, 검증을 통과했을 때에만 master 브랜치에 반영되도록 GitHub Actions로 CI를 구성했습니다.검증은 develop 브랜치에 변경 사항이 생기면 모든 Pipeline 설정 파일을 다시 Rendering 하는 것입니다. 의도한 변경은 이미 반영되어 있을 것입니다. 반면 다시 Rendering 할 때 바뀌는 파일 내용은 의도하지 않은 변경일 확률이 높습니다. CI를 수행하는 동안 Pipeline 설정 파일 내용이 바뀌었다면 한 번 확인해 봐야 합니다. 이때에는 검증이 실패하고, 사내 메신저로 알람을 보내 작업자가 다시 확인하도록 합니다.Kubernetes Object CI한편, 토스뱅크의 채널계 서비스는 Kubernetes에서 운영하고 있으며, 서비스를 배포할 때에는 Helm Template을 사용해 Kubernetes Object를 배포합니다. 2023년 1월 기준으로 토스뱅크의 Kubernetes Object Template은 7가지 Object와 110개의 설정 값이 있고, Template 파일 길이를 모두 더하면 762줄이 됩니다. 앞서 살펴 본 Pipeline Template처럼 복잡한 Template을 사용하기 때문에, 마찬가지로 Template을 수정할 때 실수를 하기 쉬웠습니다.한편, Kubernetes Object는 Pipeline 설정과 다른 점이 있었는데요. Helm Template으로 Rendering 한 결과를 Kubernetes에 배포할 뿐, Git에 파일로 저장하지 않는다는 것입니다. CI로 검증을 수행할 때에 비교할 파일이 없는 것인데요. 그래서 검증용 Kubernetes Object 파일을 Git에 저장하기로 했습니다.CI를 수행할 때에는 모든 검증용 Kubernetes Object 파일을 다시 Rendering 합니다. 의도한 변경은 이미 검증용 Kubernetes Object 파일에 반영되어 있을 것이고, 다시 Rendering 할 때 바뀌는 파일 내용은 의도하지 않았을 확률이 높습니다. 이때에는 Pipeline CI와 마찬가지로 사내 메신저 알람을 보냅니다.CI를 수행하는 동안 Pipeline 설정 파일과 검증용 Kubernetes Object 파일 모두 내용이 바뀌지 않는다면, 의도하지 않은 변경 사항이 없는 것으로 간주하고 master 브랜치에 자동으로 반영합니다. 이후에 실행하는 Pipeline 과 이후에 배포되는 Kubernetes Object는 변경 사항이 반영된 Template을 사용합니다.마치며마무리하며 글의 내용을 요약해 보면 아래와 같습니다.Pipeline이 너무 많아서 어떤 것들이 있고 어떻게 설정되어 있는지 확인하기 어렵다면 Pipeline as Code가 도움이 됩니다.Pipeline을 빠르고 정확하게 만들기 위해서는 Template 기능이 도움이 됩니다.Pipeline Template을 수정하기 어렵다면 Helm Template이 도움이 됩니다.Pipeline 이 너무 복잡해서 수정할 때 실수가 많다면 CI를 도입하는 것이 도움이 됩니다.Pipeline as Code는 일반적으로 Version-control이 되는 Pipeline을 뜻하는 경우가 많습니다. Pipeline 설정을 파일 형태로 저장하고 Git에서 확인하는 것까지를 말하지요. 여기에 Helm Template으로 Programmable 한 특성을, CI로 Testable 한 특성을 더하면 정말 Code처럼 유연하고 안전하게 Pipeline을 운영할 수 있다고 생각합니다. 비슷한 고민을 하셨던 분이 계시다면 조금이나마 도움이 되었으면 좋겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠토스는 Gateway 이렇게 씁니다2023. 10. 12대규모 로그 처리도 OK! Elasticsearch 클러스터 개선기2023. 10. 12금융사 최초의 Zero Trust 아키텍처 도입기2023. 09. 01은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)2023. 08. 31레고처럼 조립하는 토스 앱2023. 08. 22",
    "31": "웹에서 복잡한 퍼널 쉽게 관리하기임재후/최수민2023. 10. 18엔지니어링 노트 1: 복잡한 퍼널 쉽게 관리하기\n\n엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. 첫 번째는 프론트엔드 이야기인데요. 웹에서 퍼널을 손쉽게 관리할 수 있도록 만든 모듈을 소개합니다.토스 제품 디자인 원칙(PP: Product Principle)엔 “One thing for One Page”라는 원칙이 있어요. 화면 하나에는 명확한 목표 하나만 있어야 한다는 건데요. 이 원칙에 따라 제품을 만들다 보면 ‘퍼널’이 많이 생깁니다. 토스페이먼츠의 제품도 예외는 아니에요. 그래서 저희 프론트엔드 개발자들은 퍼널의 흐름을 잘 관리해야 하죠. 퍼널이란 사용자가 웹사이트를나 애플리케이션을 방문해서 최종 목표까지 달성하는데 거치는 단계를 뜻합니다.오늘은 토스페이먼츠의 프론트엔드 개발자들이 이 흐름을 어떻게 관리하고 있는지 소개하고, 이 방법의 좋은 점과 아쉬운 지점까지 공유해 볼게요.문제: 퍼널 관리의 어려움결제를 떠올리면 보통 어떤 흐름이 생각나시나요? 주문서에서 신용∙체크카드와 같은 결제 수단을 선택한 뒤(퍼널 1) 결제하기 버튼을 누르고(퍼널 2), 카드 결제창에서 내 카드사를 선택한 뒤(퍼널 3) 앱카드 등으로 인증한 뒤(퍼널 4) 다시 주문하던 페이지로 돌아오는(퍼널 5) 과정이 익숙한 흐름입니다.그런데 결제 제품에는 이렇게 최종 사용자가 경험하는 단순한 흐름 말고도 개발자가 다뤄야 하는 다른 여러 퍼널들이 있습니다. 쉬운 계좌이체를 도와주는 퀵계좌결제를 처음 사용하는 시나리오에서 내 계좌를 연결하는 과정은 다음과 같아요.결제 페이지 - ‘계좌 추가’ 선택은행 선택 페이지 - 은행 선택계좌번호 입력 페이지 - 계좌번호 입력ARS 인증 페이지 - ARS 인증 진행 및 완료계좌 등록 완료 페이지 - ‘확인’ 클릭결제페이지로 돌아옵니다.이때 각 단계에서 유저가 ‘뒤로가기’를 한다고 생각해볼게요. 2, 3, 4단계에서 뒤로가기를 하면 바로 전 단계로 이동하면 됩니다. 그런데 5, 6단계에서는 뒤로가기를 하면 어떨까요? 5단계에서 4단계로 이동한다면, 이미 인증된 ARS 페이지에 다시 접근하는 것이기 때문에 유저에게는 비정상적인 흐름이 됩니다. 6단계에서도 5단계로 이동하기보다는, 직관적으로 1단계에서 뒤로가기를 했을 때와 똑같이 작동하는게 자연스러워 보여요.이렇게 퍼널이 많아지면서 히스토리를 관리하기가 까다로워졌어요. 모바일 애플리케이션이라면 회원가입 내비게이션 스택이나 ARS 인증 내비게이션 스택을 통째로 없애면 될 텐데, 토스페이먼츠의 결제 제품들은 웹 기술로 만들고 있어요. 웹 브라우저의 히스토리는 단일 스택으로만 관리하기 때문에 같은 방식으로 구현할 수가 없었어요.해결 방법: 퍼널 관리 모듈이 문제를 해결하기 위해 검토했던 두 가지 방법과 최종 해결 방법을 소개할게요.첫 번째 해결 방법은 ‘각 퍼널을 싱글 페이지 앱(SPA)처럼 만들기’예요. 작은 퍼널 안에선 히스토리를 쌓지 않고 싱글 페이지로 만드는 방법이에요. 그런데 이 방법을 사용하면 퍼널 중간에 브라우저 뒤로가기를 대응할 수 없다는 문제가 있어요. 예를 들어 계좌 등록 퍼널에서 은행 선택 후 계좌번호 입력 퍼널에서 뒤로가기를 선택하면 은행 선택 페이지로 돌아갈 수 없어요. 또, 사용자가 새로고침을 하면 해당 페이지를 유지하지 못하고 시작점으로 돌아갈 수 밖에 없는 문제도 있었죠.다음으로 쿼리 파라미터로 관리하는 해결 방법을 고민했어요. 각 단계를 /user-register?step=1과 같은 URL 쿼리 파라미터로 표현하는 방식이에요. 그런데 이 방식을 사용하면 ‘작은’ 퍼널을 재사용하고 싶을 때 까다로워지더라고요. 퍼널이 끝난 뒤 어디로 돌아가야 하는지에 대한 정보를 계속 URL에 들고 다녀야 하기 때문이에요. 예를 들어 비밀번호 등록이라는 작은 퍼널이 끝났을 때, 회원가입 퍼널로 돌아가야 하는지, 계좌등록 퍼널로 돌아가야 하는지 알아야 하니까요.그래서 저희는 ‘Flow’라는 이름의 퍼널 관리 모듈을 만들었어요. 이 모듈을 사용하면 여러 개의 페이지를 오가며 퍼널에서 해야 할 일을 수행하고, 할 일을 마치면 히스토리 스택을 비울 수 있어요. 실제 모듈 코드보다 단순화한 코드로 먼저 살펴볼게요.class SimpleFlow {\n  private pageCount: number;\n  private unsubscribeRouteChange: null | (() => void) = null;\n  private static router = Router;\n\n  async start(url: string | UrlObject) {\n    // pageCount를 초기화해요.\n    this.pageCount = 0;\n\n    // listener를 붙여요. 반환받은 반환한 클린업 함수는 end 안에서 호출해요.\n    this.unsubscribeRouteChange = SimpleFlow.router.listen(event => {\n      if (event.action === Action.Push) {\n        this.pageCount++;\n      }\n    });\n\n    await SimpleFlow.router.push(url);\n  }\n\n  async end() {\n    if (this.pageCount > 0) {\n      await SimpleFlow.router.back(this.pageCount);\n    }\n\n    this.unsubscribeRouteChange?.();\n  }\n}여기서 핵심은 pageCount입니다. 이 모듈은 결국 시작한 지점에서 페이지를 몇 개 지나왔는지를 추적하는 객체인 셈이죠. 이번에는 사용하는 쪽 코드도 살펴볼게요. 퀵계좌결제에서 에스크로 관련 정보를 등록하는 흐름이에요. 먼저 전체 흐름을 이미지로 살펴볼게요.계좌 결제 → 에스크로 서비스 정보 등록 → SMS 인증 → 비밀번호 등록 // useEscrowInputFlow.ts\n// 훅을 호출하는 곳마다 flow를 새로 만들지 않고 flow를 유지하려 싱글턴으로 관리합니다.\nlet flow: Flow;\n\nexport function useEscrowInputFlow() {\n  const startEscrowInputFlow = () => {\n    flow = new Flow();\n\n    return flow.start(라우트.결제_에스크로);\n  };\n\n  const endEscrowInputFlow = flow.end;\n\n  return {\n    startEscrowInputFlow,\n    endEscrowInputFlow,\n  };\n}\n\n// PayScreen.tsx\n  const { startEscrowInputFlow } = useEscrowInputFlow();\n  // ... 다른 flow 훅들\n\n  return <PayForm\n    onSubmit={() => {\n      if (escrowConfig.isAvailable) {\n        await startEscrowInputFlow(); // 에스크로 흐름 시작하기\n      }\n      if (isSmsAuthRequired) {\n        await startSMSFlow(); // SMS 인증 흐름 시작하기\n      }\n      if (isPINRequired) {\n        await startPaymentPasswordFlow(); // 결제 비밀번호 입력 흐름 시작하기\n      }\n      await confirm();\n    }}\n  />;\n\n// EscrowScreen.tsx\n  const { endEscrowInputFlow } = useEscrowInputFlow();\n\n  return <EscrowForm \n    onSubmit={() => {\n      ...\n      endEscrowInputFlow(); // 에스크로 흐름 끝내기\n    }\n\n위와 같이 모듈을 사용해 봤어요.PayScreen에서 에스크로 흐름을 시작하고, EscrowScreen에선 에스크로 로직을 모두 끝낸 뒤 에스크로 흐름을 끝내면 돼요.모듈을 통해 히스토리 관리가 힘들었던 문제도 풀어냈어요. 훅에서 반환하는 end 함수만 호출하면 모바일 애플리케이션에서 내비게이션 스택을 없애는 것과 같은 효과를 얻었습니다.만약 Flow가 없다면 코드가 분산되어 유스케이스 읽듯이 자연스럽게 읽을 수 없어요.// PayScreen.tsx\n...\n  if (escrowConfig.isAvailable) {\n    router.push('/에스크로-입력');\n\t\treturn;\n  }\n  if (isPINRequired) {\n    router.push('/비밀번호-입력');\n\t\treturn;\n  }\n  if (isSmsAuthRequired) {\n    router.push('/SMS-2차인증');\n\t\treturn;\n  }\n\n  await confirm();\n...\n\n// 에스크로페이지\n<Button\n  onClick={() => {\n\t  if (isPINRequired) {\n\t    router.push('/비밀번호-입력');\n\t\t\treturn;\n\t  }\n\t  if (isSmsAuthRequired) {\n\t    router.push('/SMS-2차인증');\n\t\t\treturn;\n\t  }\n\t\n\t  await confirm();\n  }}\n>\n  입력완료\n</Button>\n\n// 비밀번호입력페이지\n<Button\n  onClick={() => {\n\t  if (isSmsAuthRequired) {\n\t    router.push('/SMS-2차인증');\n\t\t\treturn;\n\t  }\n\t\n\t  await confirm();\n  }}\n>\n  입력완료\n</Button>장점과 한계Flow를 사용하면 히스토리 관리를 편하게 할 수 있을 뿐 아니라 선언적으로 코드를 작성할 수 있다는 점이 정말 매력적이에요. 간단한 예를 들어볼게요.<Button\n  onClick={() => {\n    // 이 코드보다\n    router.push('/select-bank');\n    // 다음 코드가 더 의도를 잘 드러내요!\n    startAccountAddFlow();\n  });\n>\n  + 계좌 추가\n</Button>최근 토스페이먼츠 프론트엔드 챕터는 다음 이미지에 있는 것처럼 개발을 시작하기 전에 전체적인 시나리오(개발 명세)를 먼저 작성한 뒤 개발하기 위해 노력하고 있는데요. Flow를 이용하니 이렇게 작성한 시나리오 그대로 구현하기 편리했어요. // PayScreen.tsx\nconst { startEscrowInputFlow } = useEscrowInputFlow();\n// ... 다른 flow 훅들\n\nreturn <PayForm\n  onSubmit={() => {\n    if (escrowConfig.is가능) {\n      await startEscrowInputFlow(); // 에스크로 흐름 시작하기\n    }\n    if (SMS인증필요) {\n      await startSMSFlow(); // SMS 인증 흐름 시작하기\n    }\n    if (isPINRequired) {\n      await startPaymentPasswordFlow(); // 결제 비밀번호 입력 흐름 시작하기\n    }\n    await confirm();\n  }}\n/>;위에서 살펴본 코드 예제 중 위 부분은 사실 다음 시나리오를 그대로 코드로 표현한 결과물이에요.또 흐름의 시작과 끝을 독립적으로 관리할 수 있다는 장점도 있어요. 앞서 고려했던 URL 쿼리 파라미터를 사용할 때처럼 이전 퍼널과 다음 퍼널에 대한 정보를 들고 있을 필요 없이 시작과 끝을 모두 독립적으로 처리할 수 있어요. Flow.end()만 호출하면 되니까요.물론 Flow도 완벽하진 않아요. Flow 를 여러 개 이어 붙이는 경우를 생각해 볼게요. 유저에겐 A → B → C → D 순서대로 흘러가는 것처럼 보이지만, Flow를 사용하면 실제 화면 이동 흐름은 A → B → A(잠깐) → C → A(잠깐) → D가 돼요. 그래서 이동할 때 잠깐씩 이전 화면인 A가 보여 깜빡이는 것 같은 문제가 생겼어요.또, 시작과 끝이 독립적인 대신 start를 부르는 곳과 end를 호출하는 곳이 멀어져서 코드를 읽기가 어려워지는 문제도 있어요.복잡하고 반복되는 퍼널 관리, 토스페이먼츠에서는 이렇게 해결하고 있어요. 다른 분들은 퍼널을 어떻게 관리하고 계신지 궁금해요. 참고로 토스 계열사 중 한 곳인 '토스모바일'에서는 useFunnel을 사용하고 있는데요. 토스페이먼츠에서는 히스토리 관리가 필요해서 useFunnel을 사용하는 대신 Flow 모듈을 새로 만들었어요. Flow 모듈이나 문제 해결 방법에 대한 피드백, 혹은 우리 팀에서 사용하고 있는 더 나은 방법이 있다면 자유롭게 의견을 남겨주세요!Writer 임재후, 최수민 Edit 한주연 Graphic 이은호, 이나눔재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "32": "null 리턴은 왜 나쁠까?나재은2023. 11. 8엔지니어링 노트 2: 코드 복잡성 관리하기엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. 두 번째로 코드 복잡성을 관리하는 방법을 소개합니다.개발자의 고객은 누구라고 생각하시나요? 우리 제품을 사용하는 사용자(End-user)죠. 그런데 또 다른 고객이 있어요. 컴파일 타임의 고객, 바로 동료 개발자입니다. 복잡하고 나쁜 코드는 사용자 고객에게는 버그와 장애를, 개발자 고객에게는 낮은 생산성을 줍니다. 이번 시리즈에서는 사용자 고객뿐 아니라 개발자 고객을 위한 코드 복잡성 관리에 대해 이야기해 볼게요.먼저 내 코드가 얼마나 복잡한지 체크리스트로 확인해 볼게요.내 코드는 얼마나 복잡할까?✅ 코드를 읽고 있을 때 누군가 말을 걸면 어디까지 읽었는지 놓쳐서 처음부터 다시 읽어야 한다.✅ 코드 한 줄을 바꾸기 위해 바꿔야 할 다른 코드가 많다.✅ 새로운 사람이 팀에 합류하면 그 사람이 몇 주 내내 프로젝트 코드를 읽을 시간을 확보해야 한다.✅ 메서드 인자에 값을 전달하기 위해 지나가는 모든 메서드 인자 값을 추가한 적이 있다. 혹은 이 문제를 해결하기 위해 전역 변수를 사용하고 싶은 유혹을 받은 적이 있다.✅ 프로젝트 코드가 너무 복잡해서 처음부터 다시 만들면 적어도 지금보단 나았을 거라는 생각을 해본 적이 있다.사실 모두 제 경험담인데요. 혹시 체크리스트를 읽으며 ‘개발자의 당연한 삶’이라는 생각이 들었다면, 만성적으로 높은 코드 복잡성을 경험하고 있는 거라고 해도 될 거예요. 이제부터 예시를 보면서 함께 복잡성 관리를 시작해 봐요.null 리턴은 왜 나쁠까?null이 왜 나쁜지에 대해서는 이미 많은 의견을 접해봤을 거예요. ‘백만 달러짜리 실수다’, ‘Optional 타입을 도입하면 해결되는 문제다’, ‘Null Object Pattern 사용으로 해결할 수 있다’ 등이죠. 지금 이 예제에서는 이런 생각들을 뒤로하고, 오로지 코드를 읽는 사람 입장에서 null이 왜 복잡성을 만드는 ‘나쁜 코드’인지 알아볼게요.문제: 의미를 축약한 코드 표현val user: User? = userRepository.findByName(\"김토스\")\nprintln(user) // nullable위 예제 코드에서 user 변수가 null이라면 그 이유는 무엇일까요? 아래와 같이 여러 생각을 할 수 있어요.데이터베이스에 “김토스”라는 이름을 가진 사람이 없는 것 아닐까?데이터베이스와의 네트워크 연결이 불안정했던 것 아닐까?“김토스”는 탈퇴한 회원인 것 아닐까?“김토스”는 운영 환경에서만 존재하는 사용자인 것 아닐까?사실은 이런 이유가 있었어요. (글의 이해를 돕기 위한 예시입니다.)매주 월요일마다 새로운 직원이 입사한다.매주 월요일마다 인사 관리 시스템에 새로운 직원의 입사 정보가 추가된다.인사 관리 시스템에 정보가 업데이트되는 정확한 시점은 알 수 없다.이 코드가 동작하는 서버의 데이터베이스는 매주 월요일에서 화요일로 넘어가는 00시에 인사 관리 시스템과 동기화된다.“김토스”는 예외적으로 월요일이 아닌 수요일에 입사했다.따라서 “김토스” 유저는 아직 이 코드가 동작하는 서버의 데이터베이스에 존재하지 않는다.모든 배경을 이해한 개발자가 null을 리턴하기로 결정했어요. 이 결정은 코드를 읽는 사람에게 위의 모든 문맥 정보를 null 값 하나로 추론하게 만드는 거예요.또, 읽는 사람이 같은 프로젝트 안에서 null을 리턴하는 비슷한 코드를 만날 수도 있겠죠.val pullRequest: PullRequest? = githubClient.getPullRequestById(19)\nprintln(pullRequest) // nullablepullRequest 변수는 왜 null일까요? user가 null인 이유와 같을까요? 비슷한 이유일 수도 있고, 자신만의 고유한 원인과 히스토리가 있을지도 모르죠. 자세히 알아보기 위해 userRepository.findByName(\"김토스\") 의 세부 구현을 들여다보기 시작하는 순간 개발자의 생산성은 이미 떨어집니다.비슷한 문제들null이 아니더라도 여러 원인을 하나의 표현으로 가려버리는 방식이라면 비슷한 문제를 만들게 될 거예요. 빈 문자열과 Int 타입, 리스트로 표현한 예시도 살펴볼게요.빈 문자열 “”을 사용한 코드를 읽었을 때사용자가 입력 시도를 하지 않았나?사용자가 무언가를 입력했지만 잘못된 입력이었나?사용자가 실제로 빈 문자열을 입력했나?person.getAge() 함수가 Int 타입의 -1을 반환하는 코드를 읽었을 때함수 실행이 잘 됐고, person의 age 데이터는 실수로 누락되어서 알 수 없는 걸까?함수 실행이 잘 됐고, person의 age 데이터는 입력 선택 사항이라 알 수 없는 걸까?함수 실행에 실패했고, 원인은 알 수 없는 걸까?person.getPhoneNumbers() 함수가 리스트를 반환하는 코드를 읽었을 때빈 리스트 []가 돌아왔다면 함수 실행이 잘 되었고, 이 사람은 핸드폰 번호가 없는 걸까?null이 돌아왔다면 함수 실행이 잘 되었고, 이 사람은 핸드폰 번호가 없는 걸까?해결 1단계: 로그에 맥락 남기기이런 문제를 만들지 않으려면 코드에 담긴 다양한 의미를 축약하거나 없애지 않고 자세히 풀어 코드에 녹여내면 됩니다. 주석으로는 충분하지 않아요. 실제 코드가 아니기 때문입니다. 실제 동작하는 코드, 즉 로그에 직접 찍히는 내용만이 믿을 수 있는 정보라고 할 수 있어요.user가 null인 이유를 IllegalStateException 예외 인스턴스에 상세하게 적었어요.class UserRepository {\n  fun findByName(name: String): User {\n    val result: User? = db.getUserBy(name)\n\t  if (result == null) {\n\t\tthrow IllegalStateException(\"\"\"|\n\t      |인사관리 시스템과 동기화 되지 않은 유저의 이름을 입력한 경우 이 메시지를 볼 수 있습니다.\n\t      |매주 월->화 넘어가는 자정에 인사 관리 시스템과의 데이터 동기화가 수행되므로, 새로운 사람이 월요일이 아닌 다른 날짜에 입사하지 않았는지 확인하십시오.\n\t      |다음 주 월요일까지 기다리거나, 수동 동기화를 실행하면 문제가 해결될 수 있습니다.\n\t      |\n\t      |인사 관리 시스템과의 데이터 동기화 로직은 UserRepositorySync 클래스를 참고하십시오.\n\t      |문제가 된 name=[$name]\n\t      \"\"\".trimMargin())\n\t  }\n      return result\n  }\n}이제 코드를 자세히 들여다보지 않아도, user가 null인 이유는 물론이고 그 배경까지 별다른 노력 없이 알게 되었어요. 코드 형태만 단순한 게 아니라, 원인 파악도 복잡하지 않고 간단하게 할 수 있어요.여기서 던진 예외는 @ExceptionHandler 어노테이션을 통해 처리할 수도 있고, 예외로 던지지 않고 로깅만 남길 수도 있겠네요. 핵심은, 맥락을 코드에 명시적으로 드러내기만 하면 어떤 방법이든 더 낫다는 거예요.해결 2단계: 맥락 처리를 위한 기능 만들기이번에는 개발자가 findByName() 함수 바깥에서 null인 경우를 알고 싶다면 어떨까요? 예를 들면 재시도 로직을 넣고 싶은 상황인 거죠. ‘인사관리 시스템과 동기화되지 않은 유저의 이름을 입력했을 때’를 User? 반환 타입으로 풍부하게 표현할 수 없으니 null을 리턴할 수 밖에 없을까요? 이 질문 역시 품질을 높이기 위한 고민이 기술적인 영역으로 넘어온 것이니 이미 절반의 성공을 했다고 볼 수 있는데요. 해결 방법을 하나씩 살펴봅시다.val user: User? = userRepository.findByName(\"김토스\")\nif (user == null) {\n  // user가 null이면 유저가 인사관리 시스템과 동기화 되지 않은 경우임.\n  // 동기화를 한 번 트리거 시켜주도록 하자\n  userRepositorySync.trigger()\n}\n\nval user2: User? = userRepository.findByName(\"김토스\")\nprint(user2!!) // 위에서 동기화를 한 번 시켜주었기 때문에 null일 수 없다이렇게 코드를 작성하는 건 어떨까요? 여전히 null을 리턴했지만 주석으로 그 이유를 표현해주고 있죠. 그러나 여전히 중요한 문맥 정보를 주석과 null 으로 표현했고 리턴 타입이 User?이기 때문에 재시도를 수행한 뒤에도 null이 리턴되는 케이스를 설명하기 어려워요. null을 리턴하는 또 다른 상황을 표현할 수 없는 문제도 있고요.이 모든 이유와 복잡성을 findByName() 함수를 읽는 개발자에게 알아서 해석하라고 할 수는 없죠. 특히 마지막 주석인 위에서 동기화를 한 번 시켜주었기 때문에 null일 수 없다같은 내용이 반복되면, 코드를 읽는 사람은 '이 정보를 무시해야 한다'라는 생각이 머릿속을 가득 채워버려서 코드 읽기가 갈수록 버거워져요.findByName()를 사용하는 10명 중 9명은 '이름을 통해 User 타입을 얻어가고 싶은' 개발자 고객일거에요. 그렇다면 이렇게 작성해 봅시다.val user: User = userRepository.findByName(\n    name = \"김토스\",\n    retryHandlerWhenMissing={ userRepositorySync.trigger() }\n)\n\nprint(user) // non-null typeuser도 더 이상 nullable 하지 않고, 분기문도 사라졌기 때문에 9명의 개발자 고객들은 편안해할 거예요. 그래도 아직 불편한 부분이 있네요. retryHandlerWhenMissing 를 항상 넣어줘야 한다는 점이죠.해결 3단계: 필요할 때만 제공하기retryHandlerWhenMissing이라는 함수를 추가했지만 이것도 역시 개발자에게 부담이 되네요. 적절한 기본값을 주거나 필요할 때만 기능을 사용할 수 있도록 제공하는 방식으로 바꿔보면 어떨까요?val user: User = userRepository\n  .withRetryPolicy(ResyncWhenUserMissing()) // 이 라인을 삭제해도 findByName() 호출에는 문제 없음\n  .findByName(\"김토스\")\n\nprint(user) // non-null type\n\n// 이렇게 사용해도 문제 없음\nval user2: User = userRepository\n  .findByName(\"김토스\")\n\nprint(user2) // non-null typeKotlin 언어를 사용한다면 default arguments 기능을 사용할 수 있어요. Kotlin이 아니라면 위와 같이 코드를 작성할 수 있고, retryHandlerWhenMissing 에 무엇을 넣어야 하는지 모르는 문제를 해결할 수 있죠. 비슷한 이유로 인자가 많은 함수는 나쁜데, 이 내용은 다음 글에서 다룰게요.이제는 retryHandlerWhenMissing을 매번 사용하지 않고, ResyncWhenUserMissing라고 미리 정의해 둔 동작을 사용할 수 있어요. 로직을 그대로 노출하지 않으면서도 코드의 의도를 더 분명히 드러냈어요.그런데 아직도 문제가 있어요. 이런 식으로 코드를 작성하려면 UserRepository를 구현하기 까다로워요.import org.springframework.data.jpa.repository.JpaRepository\n\ninterface UserRepository : JpaRepository<User, Long> {\n    // 이건 Spring Framework가 알아서 해주는데,\n    fun findByName(name: String): User\n\n    // 이건 어떻게 하지?\n    fun withRetryPolicy(retryPolicy: RetryPolicy): UserRepository\n}품질을 위해서 고민하다 보니 한 번 더 기술적인 문제에 부딪히게 됐네요. 이번엔 Spring 프레임워크에 대한 이해가 필요하겠군요. Spring은 저보다 여러분들이 더 잘 아실 테니 이건 연습 문제로 남겨둘게요. 반드시 이런 코드 형태가 아니어도 괜찮아요. 코드를 읽는 개발자가 혼란스럽지 않기만 하면 되거든요.여러 단계를 거치며 문제를 해결해 보니 어떠셨나요? '이렇게까지 해야 하나?' 혹은 '제품이 당장 내일 망할 수도 있는데… 이럴 시간 없는데…' 같은 생각을 하신 분들도 있을 거예요. 회사 코드는 오랜 시간 많은 개발자들의 손끝을 거치며 어렵고 복잡해지기 쉬워요. 문제가 수면 위로 드러났을 때 학습해서 적용하기엔 회사 코드의 난이도는 높습니다.그래서 우리는 미리 대비해야 합니다. 품질 높은 코드는 작성하는데 오래 걸리시나요? 어떻게 줄일 수 있을지 미리 고민해 보세요. 머릿속에 한 번에 넣어야 할, 기억해야 할 코드가 너무 많으신가요? 어떻게 하면 기억할 코드를 줄일 수 있을지 미리 고민해 보세요.다음 글에서는 더욱 구체적인 사례를 소개할게요. 감사합니다.👉 이런 문제 해결에 관심이 있다면 토스페이먼츠 서버 챕터에 지금 합류하세요!Writer 나재은 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "33": "Feign 코드 분석과 서버 성능 개선김성두2023. 11. 22엔지니어링 노트 3: Feign 코드 분석과 서버 성능 개선엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. Feign과 다중 스레드를 사용하는 과정에서 생긴 문제를 이해하고 성능 개선까지 한 경험을 공유해요.얼마 전 토스페이먼츠 서버 모니터링 시스템을 통해 성능 저하 문제를 발견했어요. 약 15,000건 정도의 데이터를 10분 내외로 처리해야 하는 요구사항이 있어 다중 스레드를 활용했는데, 여기서 예상치 못하게 동시성 문제가 생겼습니다. 문제의 핵심은 HTTP 클라이언트 인터페이스인 Feign의 내부 구조에 숨어있었어요. 보통 Feign은 기본 설정을 그대로 사용하기 때문에 여기서 문제가 생길 거라고 예상하지 못했어요. 그래서 이번 문제를 해결하면서 직접 Feign의 내부를 들여다보았습니다. 그 내용과 문제 해결 과정을 공유드릴게요.💡 문제가 감지된 당시에 사용하던 서버 환경은 Sprinig Boot 2.7.9, 그리고 JDK 11입니다.1단계: 문제 이해하기당시 문제가 되었던 지점의 로그를 함께 살펴볼게요.[BlockedThread][blocker:http-nio-8080-exec-67][blocked:http-nio-8080-exec-101]\n...\n\nat java.base@11.0.17/sun.net.www.http.ClientVector.put(KeepAliveCache.java:309) - locked sun.net.www.http.ClientVector@51f75cb3\nat java.base@11.0.17/sun.net.www.http.KeepAliveCache.put(KeepAliveCache.java:172) - locked sun.net.www.http.KeepAliveCache@71a6e252\nat java.base@11.0.17/sun.net.www.protocol.https.HttpsClient.putInKeepAliveCache(HttpsClient.java:663)\nat java.base@11.0.17/sun.net.www.http.HttpClient.finished(HttpClient.java:439)\nat java.base@11.0.17/sun.net.www.http.KeepAliveStream.close(KeepAliveStream.java:99)\n\n로그 중간에 눈에 들어오는 부분이 있어요. locked가 보이네요. 토스페이먼츠 서버 모니터링 시스템은 블로킹 스레드를 탐지했을 때 스택 트레이스에 locked 문구를 추가해 줘요. 모니터링 시스템 덕분에 문제가 발생한 지점은 쉽게 찾을 수 있었지만, 원인은 아직 이해하지 못해서 좀 더 자세히 살펴보기로 했습니다.먼저, 문제가 발생한 KeepAliveCache 클래스의 put 메서드를 살펴봤어요. 다음 예시 코드에서 볼 수 있듯이 put 메서드에는 synchronized 키워드가 붙어있어요. 여러 스레드에서 대량으로 API를 호출하고 있는 상황에서 의심이 드는 지점이었죠.// https://github.com/openjdk/jdk/blob/da75f3c4ad5bdf25167a3ed80e51f567ab3dbd01/src/java.base/share/classes/sun/net/www/http/KeepAliveCache.java#L83-L127\n\n/**\n  * Register this URL and HttpClient (that supports keep-alive) with the cache\n  * @param url  The URL contains info about the host and port\n  * @param http The HttpClient to be cached\n  */\npublic synchronized void put(final URL url, Object obj, HttpClient http) {\n    // ...\n}이어서 put 메서드를 호출하고 있는 지점이 어디인지 거슬러 올라가 봤어요. HttpsClient 클래스의 kac 라는 변수가 put 메서드를 호출하고 있었어요.// https://github.com/openjdk/jdk/blob/9938b3f62babfc35ee682bd979a6bf08ac7cd348/src/java.base/share/classes/sun/net/www/protocol/https/HttpsClient.java#L670-L683\n\n@Override\nprotected void putInKeepAliveCache() {\n    // ...\n    kac.put(url, sslSocketFactory, this);\n}마지막으로 KeepAliveCache kac이 무엇인지 알아보니 HttpClient 클래스의 정적 변수였다는 것을 알게 되었습니다.// https://github.com/openjdk/jdk/blob/da75f3c4ad5bdf25167a3ed80e51f567ab3dbd01/src/java.base/share/classes/sun/net/www/http/HttpClient.java#L96-L97\n\npublic class HttpClient extends NetworkClient {\n    /* where we cache currently open, persistent connections */\n    protected static KeepAliveCache kac = new KeepAliveCache();\n}이제 실마리가 잡혔어요. KeepAliveCache kac 변수에 static 키워드가 붙어있기 때문에 HttpClient 클래스의 모든 인스턴스가 단 하나의 KeepAliveCache kac를 공유해서 사용하게 된 거였어요. KeepAliveCache kac는 공유자원인데 여러 스레드가 동시에 synchronized put 메서드를 수행하려고 하니 동시성 문제가 발생한 거예요.정리하자면 Feign 클라이언트를 사용한 API 호출이 엄청나게 많이 발생해서 다중 스레드를 활용했고, 그 과정에서 여러 스레드가 공유자원에 동시에 접근했어요. 결국 필요한 자원이 사용 가능해질 때까지 계속해서 대기 상태에 있었고요. 그래서 서버 성능이 저하되고, 제대로 요청을 받을 수 없는 상태가 되었던 거죠.더 알아보기: Feign 설정문제 원인은 알았지만 여전히 이해하기 어려운 부분이 있었습니다. 저는 분명히 Feign이 Apache HttpClient 5 버전을 사용하도록 설정해 뒀거든요. 눈썰미가 좋은 분이라면 눈치채셨을 수 있는데요. 문제가 된 sun.net.www.http.HttpClient 클래스는 Feign이 설정하는 기본 HTTP 클라이언트가 아니에요. 그럼 이 클래스는 도대체 어디서 온 걸까요?Feign의 내부 구현 코드를 따라가 보면 Feign 기본 HTTP 클라이언트는 Java Standard Library의 HttpURLConnection 클래스를 사용하는 것을 확인할 수 있어요. // Client.java: https://github.com/OpenFeign/feign/blob/b0c5db0ddfd24e0515a9143d82353a8d03def32d/core/src/main/java/feign/Client.java#L104-L107\n\n@Override\npublic Response execute(Request request, Options options) throws IOException {\n    HttpURLConnection connection = convertAndSend(request, options);\n    return convertResponse(connection, request);\n}이 HttpURLConnection 클래스가 내부적으로 사용하는 HTTP 클라이언트에서 문제가 되는 sun.net.www.http.HttpClient를 사용하고 있어요.// HttpURLConnection.java: https://github.com/JetBrains/jdk8u_jdk/blob/94318f9185757cc33d2b8d527d36be26ac6b7582/src/share/classes/sun/net/www/protocol/http/HttpURLConnection.java#L308\n\npublic class HttpURLConnection extends java.net.HttpURLConnection {\n    protected HttpClient http;\n}정리해 볼게요. Feign의 기본 HTTP 클라이언트는 내부적으로 Java Standard Library의 HttpURLConnection 클래스를 사용합니다. 자바에서는 이 클래스를 통해 HTTP 커넥션 관련 기능들을 제공하는데요. 이를 구현하는 과정에서 sun.net.www.http.HttpClient를 사용하게 되고, 이로 인해 동시성 문제가 발생할 수 있어요. (참고로 JDK 버전 17 이상에서는 Keep-Alive의 해당 구현을 개선했기 때문에, 더 이상 발생하지 않아요. 아래에서 좀 더 자세히 설명할게요.)2단계: 문제 해결과 성능 개선당장 문제를 해소할 수 있는 방법은 두 가지였어요. 첫 번째는 Feign 클라이언트가 사용하는 HTTP 클라이언트의 구현체를 변경하는 것, 두 번째는 JDK 버전을 17 이상으로 업그레이드하는 것이었어요.1. Feign Client 구현체 변경 (Apache HttpClient 5)Feign은 Apache HttpClient, OkHttp Client 등 다양한 HTTP 클라이언트를 주입 받아서 동작해요. 이때 특별한 설정을 하지 않으면 Feign이 제공하는 기본 클라이언트를 사용해요. 하지만 앞서 보았듯이, Feign의 기본 클라이언트는 HttpURLConnection 클래스를 사용하기 때문에 동시성 문제가 발생할 수 있어요. 즉, HttpURLConnection 클래스를 사용하지 않는 HTTP 클라이언트를 사용해야 하죠.토스페이먼츠에서는 여러 구현체 중 빠르고 효율적인 Apache HttpClient 5를 사용하고 있어요. 다음과 같이 Apache HttpClient 5 의존성을 설정해요.// build.gradle.kts\ndependencies {\n    implementation(\"org.springframework.cloud:spring-cloud-starter-openfeign\")\n    implementation(\"io.github.openfeign:feign-hc5\")\n}그런 뒤 Feign이 Apache HttpClient 5를 사용하도록 설정을 변경합니다.// application.yaml\nfeign.httpclient.hc5.enabled: true제가 겪은 문제는 사실 두 번째 Feign 설정을 누락하면서 발생했어요. Apache HttpClient 5 의존성만 주입해 주면 나머지는 언제나처럼 Spring Boot가 마법처럼 해결해 줄 거라 믿은 게 실수였죠. Spring Boot 2.x 버전을 사용하고 계신다면, 꼭 Feign의 위 설정을 true 로 바꿔주셔야 해요. 참고로 Spring Boot 3.x 버전부터는 의존성만 주입해도 자동 설정됩니다.Feign에 HTTP 클라이언트가 주입되는 방식이 궁금하다면, 아래 클래스에서 시작해서 코드를 따라가 보시는 것을 추천할게요. 다음과 같이 registerFeignClients 메서드를 따라가다 보면,// FeignClientsRegistrar.java: https://github.com/spring-cloud/spring-cloud-openfeign/blob/eb3a9cb7bc5b9c8b1d03393e5ea34889a5d6e606/spring-cloud-openfeign-core/src/main/java/org/springframework/cloud/openfeign/FeignClientsRegistrar.java#L148-L152\n\n@Override\npublic void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) {\n    registerDefaultConfiguration(metadata, registry);\n    registerFeignClients(metadata, registry);\n}Feign.Builder static 클래스의 client 변수가 적절하게 설정되는 것을 확인할 수 있을 거예요.// Feign.java: https://github.com/OpenFeign/feign/blob/b0c5db0ddfd24e0515a9143d82353a8d03def32d/core/src/main/java/feign/Feign.java#L97\n\npublic abstract class Feign {\n    public static class Builder extends BaseBuilder<Builder> {\n        private \n\n특히 Feign이 Apache HttpClient 5 구현체를 사용하도록 설정하는 부분의 클래스 코드는 다음과 같아요. // FeignAutoConfiguration.java: https://github.com/spring-cloud/spring-cloud-openfeign/blob/cce0de9e898318b7edabab902afda5a45f2ecb41/spring-cloud-openfeign-core/src/main/java/org/springframework/cloud/openfeign/FeignAutoConfiguration.java#L337-L350\n\n@Configuration(proxyBeanMethods = false)\n@ConditionalOnClass(ApacheHttp5Client.class)\n@ConditionalOnMissingBean(org.apache.hc.client5.http.impl.classic.CloseableHttpClient.class)\n@ConditionalOnProperty(value = \"feign.httpclient.hc5.enabled\", havingValue = \"true\")\n@Import(org.springframework.cloud.openfeign.clientconfig.HttpClient5FeignConfiguration.class)\nprotected static class HttpClient5FeignConfiguration {\n    @Bean\n\t  @ConditionalOnMissingBean(Client.class)\n\t  public Client feignClient(org.apache.hc.client5.http.impl.classic.CloseableHttpClient httpClient5) {\n\t\t    return new ApacheHttp5Client(httpClient5);\n    }\n\n}2. JDK 버전 17 이상으로 업데이트JDK 17 버전에서는 위의 디버깅 과정에서 보았던 동시성 문제를 야기하는 내부 구현이 개선되었어요. 코드를 보면 synchronized 키워드가 제외된 것을 확인하실 수 있어요. 따라서 위에서 제가 겪은 문제가 발생하지 않아요.위의 두 가지 해결 방법 중 어느 것을 선택해도 괜찮아요. 아주 간단한 방법이지만 동시성 문제를 해결한 결과,API 처리량이 최소 8배 이상 향상되는 것을 확인할 수 있었어요.평균 10분 정도 소요되던 작업이 1분 대로 줄었어요.지금까지 Feign과 다중 스레드를 사용하는 과정에서 생긴 문제에 대해 알아보았어요. 문제 해결과 성능 개선에서 그치지 않고 우리가 의심없이 사용하고 있는 도구 중 하나인 Feign을 깊이 있게 이해할 수 있는 기회였습니다.👉 이런 문제 해결에 관심이 있다면 토스페이먼츠 서버 챕터에 지금 합류하세요!Writer 김성두 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "34": "인자가 많은 메서드는 왜 나쁠까?나재은2023. 11. 29엔지니어링 노트 4: 인자가 많은 메서드는 왜 나쁠까?엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. 이번에는 인자가 많은 메서드를 함께 리팩토링 하면서 코드 사용자 입장에서 코드 복잡성을 관리하는 방법을 알아봅니다.이전 글 ‘null은 왜 나쁠까?’의 핵심은 코드를 읽는 사람의 입장을 생각하자는 거였어요. 이번 글에서는 코드를 사용하는 사람의 입장에 관해 이야기해 볼게요.재전송, 메일 수신자 필터링, SMS 전송(fallback) 등 다양한 기능을 제공하는 메일 발송 기능이 있다고 상상해 볼게요. 그 기능을 하는 메서드의 인자가 11개 정도 있다면 어떨까요?class Mail(\n\t// ...\n) {\n\n  fun send(\n    phoneFallback: Boolean?,\n    phoneNumber: String?,\n    isForceSend: Boolean?,\n    recipient: String,\n    id: Long,\n    mailDomainFilterService: MailDomainFilterService?,\n    mailRetryService: MailRetryService?,\n    title: String,\n    body: String,\n    param: Map<Any, Any>,\n    reservedAt: Instant?,\n  )\n}이 메서드를 사용하기 위해 다음과 같이 호출 준비를 해봤어요.mail.send(\n  phoneFallback = null,\n  phoneNumber = null,\n  isForceSend = null,\n  recipient = \"jaeeun.na@tosspayments.com\",\n  id = -1,\n  mailDomainFilterService = null,\n  mailRetryService = null,\n  title = \"안녕하세요\",\n  body = \"메일 본문입니다\",\n  param: emptyMap(),\n  reservedAt: null,\n)코드만 봤을 때 몇 가지 인자는 이해돼서 값을 넣었지만, phoneFallback이나 param 같은 인자에는 무슨 값을 넣어줘야 하는지 모르겠네요. 메일 제목과 내용, 받는 사람만 채워서 메일 한 통을 보내고 싶을 뿐인데 이렇게 인자가 많고 의미를 파악하기 어려우면 메서드의 사용성이 떨어집니다.생산성 악마의 미소잠깐 같은 기능을 하는 Gmail UI를 한 번 볼까요? 받는 사람만 입력하면 본문 내용이 없어도 메일이 발송됩니다. 어떤 값이 필수인지도 바로 알 수 있고요. 그런데 Mail.send메서드에는 입력해야 하는 ‘필수’ 인자가 너무 많습니다. 잘 모르는 인자, 사용할 필요 없는 인자에 null을 채워 넣는 것도 불편해요. mailDomainFilterService 객체는 어떻게 만들 수 있는지, isForceSend의 의미는 뭔지 알기 어렵죠. 답답한 마음에 git blame을 입력할까 망설이다가 다른 선택지를 고민해 봅니다.send 메서드는 다른 곳에서 어떻게 쓰고 있을까? 비슷하게 따라 해야겠다.아무 인자나 넣고 일단 실행해 볼까? 실행에 성공하면 좋고, 예외가 생긴다면 그때 확인해야지.send 메서드 구현은 어떻게 되어있지? 각 인자의 의미를 파악해 보자.메서드를 사용하는 개발자는 세 가지 중 하나를 선택할 수밖에 없게 됩니다. 어떤 선택지를 고르더라도 모두 생산성이 낮은 방법이죠. 선택지 하나를 고르는 순간, 생산성 악마는 이런 질문을 던집니다.‘비슷하게 따라 한다’를 선택한 당신에게메서드 실행에 과연 성공할까? 따라 한 방법이 올바른지 검증할 수 있을까?따라 한 코드의 맥락과 사용하는 맥락이 같은가?‘아무 인자나 넣고 일단 실행해 본다’를 고른 당신에게이 상태로 라이브 배포를 할 수 있을까?내가 만드는 코드를 정확히 이해하지 못해도 괜찮은가?‘내부 구현을 파악해본다’를 고른 당신에게내부 구현을 정말로 이해할 수 있을까?그 시간에 다른 기능을 만드는 게 낫지 않을까?질문에 답을 하다 보면 세 가지 선택지 모두 좋은 접근이 아니라는 생각을 하게 됩니다. 문제를 해결하기 위해서는 근본적이고 장기적인 해결책이 필요합니다. 한걸음에 천리를 갈 필요는 없습니다. 대신 ‘이런 메서드는 쓰기 어렵겠는데’라며 메서드의 소비자 입장에서 생각하는 것부터 시작하세요.(1) 함께 사용하는 인자는 하나로 묶는다이해하기 어려운 인자부터 하나씩 볼게요. 옆자리 동료에게 phoneFallback 인자가 뭔지 물어봤어요. 메일 발송이 실패했을 때 메일에 있는 내용을 문자로 대신 보내기 위한 인자라고 하네요. phoneNumber도 왜 있는지 알겠네요. phoneFallback이 true 라면 phoneNumber에 적힌 번호로 메일 내용이 전송되는 거에요. 즉, phoneFallback 과 phoneNumber은 항상 같이 사용되는 인자에요. 알게 된 내용을 코드에 반영해 볼게요.// 아래 두 변수가 둘 다 null이거나, 둘 다 null이 아니어야 한다는 사실을\n// 더 이상 기억하고 있을 필요가 없다  \ndata class FallbackFeatureOption(\n  val phoneFallback: Boolean,\n  val phoneNumber: String,\n)\n\nclass Mail(\n  // ...\n) {\n  fun send(\n    fallbackFeatureOption: FallbackFeatureOption?,    \n    isForceSend: Boolean?,\n    recipient: String,\n    id: Long,\n    mailDomainFilterService: MailDomainFilterService?,\n    mailRetryService: MailRetryService?,\n    title: String,\n    body: String,\n    param: Map<Any, Any>,\n    reservedAt: Instant?,\n) { \n\tsendInternal(\n      phoneFallback = fallbackFeatureOption?.phoneFallback,\n      phoneNumber = fallbackFeatureOption?.phoneNumber,\n      isForceSend = isForceSend,\n      recipient = recipient,\n      id = id,\n      mailDomainFilterService = mailDomainFilterService,\n      mailRetryService = mailRetryService,\n      title = title,\n      body = body,\n      param = param,\n      reservedAt = reservedAt,\n    )\n  }\n\n  @Deprecated(\"이 메서드는 너무 인자가 많아서 코드를 이해하기 어렵습니다. 다른 send() 메서드를 사용하세요.\")\n  fun sendInternal(\n  \tphoneFallback: Boolean?,\n      phoneNumber: String?,\n      isForceSend: Boolean?,\n      recipient: String,\n      id: Long,\n      mailDomainFilterService: MailDomainFilterService?,\n      mailRetryService: MailRetryService?,\n      title: String,\n      body: String,\n      param: Map<Any, Any>,\n      reservedAt: Instant?,\n    ) { ... }\n}아직 완벽한 코드는 아닙니다. 하지만 코드 사용자가 기억할 내용을 하나 줄였기 때문에 원래 코드보다는 나아졌습니다. 좀 더 보다 보니 fallbackFeatureOption에 null을 넣어야 한다는 게 마음에 들지 않네요. 문자 발송은 더 이상 신경 쓰고 싶지 않아요. 코드를 좀 더 수정해 볼게요.// fallbackFeatureOption 인자를 삭제했다\nfun send(\n  // ... \n) {\n  sendInternal(\n    phoneFallback = null,\n    phoneNumber = null,\n    // ...\n  )\n}\n\n// 메서드 이름으로 의도를 표현했다\nfun sendWithFallback(\n  fallbackFeatureOption: FallbackFeatureOption, // non-nullable\n  // ...\n) {\n  sendInternal(\n    phoneFallback = fallbackFeatureOption.phoneFallback,\n    phoneNumber = fallbackFeatureOption.phoneNumber,\n    // ...\n  )\n}드디어 send 메서드가 노출하고 있던 phoneFallback 맥락이 사라졌어요. 하지만phoneFallback이 false 인데 phoneNumber 값을 넣어야 해서 여전히 어색하네요. 문자를 보내지 않을 건데 핸드폰 번호를 넣어야 하니까요. 이 부분은 조금 뒤에서 다시 개선해 볼게요.(2) 관련 없는 것은 분리한다isForceSend에 대해서도 알아볼게요. 동료에게 물어보니, 메일 수신을 거부한 이메일 주소에도 발송 기록을 남겨야 하는 정책 때문에 사용하는 인자라고 하네요. 즉, isForceSend를 true로 설정하면 수신 거부한 주소에도 메일을 발송할 수 있어요.이메일 수신 거부 목록은 데이터베이스에 들어 있고, MailDomainFilterService에서 데이터베이스에 접근해서 발송할지 결정합니다.class MailDomainFilterService {\n  fun isFiltered(domain: String): Boolean { \n    // database 에서 목록을 가져온 뒤, 발송 여부 결정\n  }\n}그래서 isForceSend 가 true면, mailDomainFilterService 인자가 사용되지 않아요. 메일 한 통을 보내고 싶을 뿐인데, 이런 세부 사항까지 알아야 하네요. 다음에 코드를 읽는 개발자가 같은 경험을 하지 않기를 바라며 이렇게 코드를 변경해 봤어요.class Mail(\n  private val mailDomainFilterService: MailDomainFilterService,\n) {\n  fun send(\n    // ... \n  )\n\n  fun sendWithFallback(\n    // 기존과 동일\n  )\n \n  fun sendWithFallbackAndForced(\n    fallbackFeatureOption: FallbackFeatureOption?,   \n    recipient: String, \n    id: Long,\n    mailRetryService: MailRetryService?,\n    title: String,\n    body: String,\n    param: Map<Any, Any>,\n    reservedAt: Instant?,\n  ) { \n    sendInternal(\n      phoneFallback = fallbackFeatureOption?.phoneFallback,\n      phoneNumber = fallbackFeatureOption?.phoneNumber,\n      isForceSend = true,\n      recipient = recipient,\n      id = id,\n      mailDomainFilterService = null,\n      mailRetryService = mailRetryService,\n      title = title,\n      body = body,\n      param = param,\n      reservedAt = reservedAt,\n    )\n  }\n}그런데 문제가 생겼습니다. fallbackFeatureOption 기능과 isForceSend 기능이 서로 겹치기 때문에, 다음과 같이 다섯 개의 메서드를 만들어야 하는 상황이 되어 버렸어요.send()sendInternal()sendWithFallbackAndForced()sendWithFallback()sendWithForce()아직 포함하지 않은 기능까지 고려하면 메서드가 수십 가지가 되어야 하는 상황이네요. 연관이 없는 메서드들은 독립적으로 사용하도록 만들어 볼게요.class Mail(\n  private val mailDomainFilterService: MailDomainFilterService\n) {\n  private var enableForceSendFeatureForCompliancePurpose: Boolean = false\n  private var smsFallbackFeatureOption: SmsFallbackFeatureOption? = null\n\n  fun enableSmsFallbackFeature(smsFallbackFeatureOption: SmsFallbackFeatureOption): Mail {\n    this.smsFallbackFeatureOption = smsFallbackFeatureOption\n    return this\n  }\n\n  // isForceSend 라는 이름에서, 조금 더 서술적인 이름으로 변경했다.\n  fun enableForceSendFeatureForCompliancePurpose(): Mail { \n    this.enableForceSendFeatureForCompliancePurpose = true\n    return this\n  }\n\n  fun send(\n    id: Long,\n    recipient: String,\n    mailRetryService: MailRetryService?,\n    title: String,\n    body: String,\n    param: Map<Any, Any>,\n    reservedAt: Instant?,\n  ) {\n    // 이제 인자가 아닌 객체 필드에서 값을 얻어올 수 있다.\n    sendInternal(\n      phoneFallback = this.smsFallbackFeatureOption?.phoneFallback,\n      phoneNumber = this.smsFallbackFeatureOption?.phoneNumber,\n      isForceSend = this.enableForceSendFeatureForCompliancePurpose,\n      recipient = recipient,\n      id = id,\n      mailDomainFilterService = if (this.enableForceSendFeatureForCompliancePurpose) {\n        this.mailDomainFilterService\n      } else {\n        null\n      },\n      mailRetryService = mailRetryService,\n      title = title,\n      body = body,\n      param = param,\n      reservedAt = reservedAt,\n    )\n  }\n}의미가 뒤섞여서 사용하기 어색했던 SmsFallbackFeatureOption 클래스도 마저 개선해 봐요. 메서드 사용성을 개선하면서 의미가 충돌하게 됐거든요. // sms fallback 기능을 사용하겠다고 했는데 true를 입력하는게 어색하다. \nmail\n  .enableSmsFallbackFeature(SmsFallbackFeatureOption(true, \"010-1234-5678\"))\n  .send(...)\n\n// sms fallback 기능을 사용하겠다고 했는데 false를 입력하면 어떻게 되는거지...?\nmail\n  .enableSmsFallbackFeature(SmsFallbackFeatureOption(false, \"???\"))\n  .send(...)불필요한 인자를 삭제하고, 이름을 개선했어요. 그래서 이렇게 바꿀 수 있어요. class Mail(\n  private val mailDomainFilterService: MailDomainFilterService\n) {\n  private var enableForceSendFeatureForCompliancePurpose: Boolean = false\n  private var smsFallbackFeaturePhoneNumber: String? = null\n\n  fun enableSmsFallbackFeature(smsFallbackFeaturePhoneNumber: String): Mail {\n    this.smsFallbackFeaturePhoneNumber = smsFallbackFeaturePhoneNumber\n  }\n\n  fun enableForceSendFeatureForCompliancePurpose(): Mail { \n    this.enableForceSendFeatureForCompliancePurpose = true\n  }\n\n  fun send(\n    id: Long,\n    recipient: String,\n    mailRetryService: MailRetryService?,\n    title: String,\n    body: String,\n    param: Map<Any, Any>,\n    reservedAt: Instant?,\n  ) {\n    sendInternal(\n      phoneFallback = this.smsFallbackFeaturePhoneNumber != null,\n      phoneNumber = this.smsFallbackFeaturePhoneNumber,\n      isForceSend = this.enableForceSendFeatureForCompliancePurpose,\n      recipient = recipient,\n      id = id,\n      mailDomainFilterService = if (this.enableForceSendFeatureForCompliancePurpose) {\n        this.mailDomainFilterService\n      } else {\n        null\n      },\n      mailRetryService = mailRetryService,\n      title = title,\n      body = body,\n      param = param,\n      reservedAt = reservedAt,\n    )\n  }\n}이제는 코드의 사용자가 깊게 고민하지 않아도 편리하게 사용할 수 있겠네요. intellij 같은 IDE의 도움을 받으면 mail 객체에 .(점)만 찍으면 사용할 수 있는 메서드가 나오고, 메서드 이름을 보고 충분히 기능을 유추할 수 있으니까요. 게다가 이제는 send() 메서드의 주의 사항을 더 기억하지 않아도 돼요. 그냥 사용하고 싶은 기능을 호출하기만 하면 되거든요.mail\n  .enableSmsFallbackFeature(\"010-1234-5678\")  // 이 메서드를 호출하지 않아도 괜찮다\n  .enableForceSendFeatureForCompliancePurpose()  // 이 메서드를 호출하지 않아도 괜찮다\n  .send(...)(3) 가장 중요한 인자만 남긴다몇 주간에 걸쳐 동료에게 맥락을 물으며 리팩토링하니 다른 개발자들이 충분히 납득할 수 있는 형태가 됐어요. 이제 send() 메서드에 어떤 인자가 있을지 상상이 되나요?class Mail(\n  private val mailDomainFilterService: MailDomainFilterService,\n  private val mailRetryService: MailRetryService,\n  private val param: Map<Any, Any>?,\n) {\n  private var enableForceSendFeatureForCompliancePurpose: Boolean = false\n  private var smsFallbackFeaturePhoneNumber: String? = null\n  private var scheduleSendReservedAt: Instant? = null\n  private var enableRetryFeature: Boolean = false\n\n  fun enableSmsFallbackFeature(smsFallbackFeaturePhoneNumber: String): Mail {\n    this.smsFallbackFeaturePhoneNumber = smsFallbackFeaturePhoneNumber\n  }\n\n  fun enableForceSendFeatureForCompliancePurpose(): Mail { \n    this.enableForceSendFeatureForCompliancePurpose = true\n  }\n\n  fun enableScheduleSendFeature(reservedAt: Instant): Mail { \n    this.scheduleSendReservedAt = reservedAt \n  }\n\n  fun enableRetryFeature(): Mail { \n    this.enableRetryFeature = true\n  }\n\n  fun send(\n    recipient: String,\n    title: String,\n    body: String,\n  ) {\n    sendInternal(\n      phoneFallback = this.smsFallbackFeaturePhoneNumber != null,\n      phoneNumber = this.smsFallbackFeaturePhoneNumber,\n      isForceSend = enableForceSendFeatureForCompliancePurpose,\n      id = Random.nextLong(),  // 내부 구현에서 랜덤으로 생성해서 사용함\n      mailDomainFilterService = if (this.enableForceSendFeatureForCompliancePurpose) {\n        this.mailDomainFilterService\n      } else {\n        null\n      },\n      mailRetryService = if (this.enableRetryFeature) {\n        this.mailRetryService\n      } else {\n        null\n      },\n      title = title,\n      body = body,\n      param = emptyMap(),\n      reservedAt = this.scheduleSendReservedAt,      \n  }\n\n  fun templateSend(\n    recipient: String,\n    title: String,\n    body: String,\n    param: Map<Any, Any>\n  ) {\n    sendInternal(\n      // ...\n      param = param\n      // ...\n    )\n  }\n}sendInternal 메서드 구현을 전혀 건드리지 않고도 이젠 코드에 내가 원하는 값만 넣을 수 있게 됐어요. 코드를 읽기도, 사용하기도 편리해졌죠.// 가장 간단하게 사용할 때\nmail.send(\n  recipient = \"jaeeun.na@tosspayments.com\",\n  title = \"안녕하세요\",\n  body = \"메일 본문입니다\"\t\n)\n\n// 지원하는 모든 기능을 사용할 때\nmail\n  .enableSmsFallbackFeature(\"010-1234-5678\")\n  .enableForceSendFeatureForCompliancePurpose()\n  .enableScheduleSendFeature(Instant.now().plus(Duration.ofHours(2)))\n  .enableRetryFeature()\n  .send(\n    recipient = \"jaeeun.na@tosspayments.com\",\n    title = \"안녕하세요\",\n    body = \"메일 본문입니다\"\n  )Mail 생성자에 의존성이 생겼으니 조삼모사가 아니냐고 반문할 수도 있겠네요. 함수 인자에 의존성을 주입하는 것은 코드 사용자에게 부담을 주지만, 객체에서 의존성을 관리하면 코드 작성자가 그 부담을 감당할 수 있어요. 코드 사용자 입장에서는 더 편리해지죠. 중복으로 호출되던 의존성 주입 코드를 한곳으로 모은 효과도 있고, 객체 간의 표현도 더 풍부해졌죠.정리하기지금까지의 과정을 정리해 볼게요.메일을 보내기 위해 mail.send() 메서드를 재사용하려고 했다.사용자에게 친절하지 않은 코드 때문에 mail.send()를 사용하기 위해 너무 많은 맥락을 알아야만 했다.팀 내부의 도메인 지식을 습득했다.습득한 지식을 가지고 코드를 개선했다.가장 중요한 과정은 무엇이었을까요? 팀원들이 저처럼 도메인 지식을 다시 학습하지 않아도 되도록 코드를 리팩토링한 네 번째 과정이 가장 중요합니다. 만든 개발자만 이해하는 코드는 좋지 않아요. 개발자가 요구사항을 위해 사용해야 하는 코드를 쉽게 읽을 수 있는 상황이 가장 바람직하죠. 그런 환경이 개발자가 가장 생산적으로 활동할 수 있는 환경이고요. 리팩토링 후 달라진 개발자의 환경을 비교해 볼까요.Q. 이 프로젝트에서 메일을 보내려면 어떻게 해야 해요?Before: 일단 Mail 클래스의 send() 메서드 통해서 보낼 수 있는데요, id는 꼭 랜덤하게 넣어야 하고요, 예약 메일이 아니면 reservedAt 에 null을 넣으세요. 아, 혹시 수신자 메일 도메인이 어떻게 되나요? 강제로 보내야 할 때가 있어서요.After: Mail 클래스가 제공하는 공개 메서드 목록을 보세요.도메인 지식과 코드가 아주 간단해졌죠? 이 리팩토링 과정을 돕기 위해 디자인 패턴과 클린 아키텍처, 도메인 주도 개발(DDD) 등이 존재합니다. 구현을 어떻게 할지는 당신의 손에 달려있습니다. 코드를 읽는 동료 개발자를 항상 고객처럼 생각하세요.👉 이런 문제 해결에 관심이 있다면 토스페이먼츠 서버 챕터에 지금 합류하세요!Writer 나재은 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기이전 글 읽어보기null 리턴은 왜 나쁠까?2023. 11. 08",
    "35": "Reactor Netty Memory Leak 이슈 탐방기김성현ㆍServer Developer2023. 12. 11토스의 백엔드로 오는 요청은 모두 Spring Cloud Gateway 기반의 API Gateway를 통해 클러스터로 들어와요. 그리고 클러스터 안에서는 수많은 서버들이 Spring WebClient를 통한 REST API로 통신해 요청을 처리해요.최근 두 가지 도구에서 동일한 Memory Leak 이슈를 경험했는데요. 원인이 같았습니다. 문제 원인을 찾은 과정과 그 내용을 공유드릴게요.1. Spring Cloud Gateway Memory Leak 이슈 파악하기어느 날 한 게이트웨이로부터 OOMKilled 알림을 받았습니다. OOMKilled 알림은 OS가 프로세스를 죽였다는 알림인데요. 해당 컨테이너에 지정된 메모리 상한을 컨테이너가 사용하는 총 메모리가 초과했음을 뜻해요. 죽은 게이트웨이에는 최근에 변경된 사항이 없었고, 게이트웨이가 OOM으로 죽은 적이 처음이라 의아한 상황이었어요. 그래서 하나하나 증거를 살펴보기로 했습니다.우선 컨테이너가 OOMKilled로 죽었다는 것은 JVM에서 일반적으로 사용하는 Heap 영역의 문제일 가능성이 거의 없습니다. 토스에서는 메모리 할당에 드는 오버헤드를 최대한 줄이기 위해 -XX:+AlwaysPreTouch JVM 옵션을 사용하고 있는데요. 이 옵션을 사용하면 어플리케이션 부팅 시 Heap 영역만큼의 메모리를 미리 할당하고 시작하기 때문입니다. 그래서 일반적으로 토스의 서버들은 RSS 메모리 지표가 부팅할 때를 제외하고는 큰 변화가 없습니다.하지만 이번에 OOM으로 죽은 서버의 residential set size (RSS) 메모리 지표를 살펴보면 변화가 있었을 뿐 아니라 꾸준히 우상향 중이었습니다. 여기서부터는 JVM heap 영역이 아닌 native 영역의 메모리를 사용하는 부분을 샅샅이 뒤져 범인을 찾아야 합니다. 하지만 문제가 된 게이트웨이는 JNI나 JNA같이 native 영역의 메모리를 쓰는 곳은 없어서 어디에서 문제가 발생했는지 바로 알기 어려웠습니다. 한 가지 더 이상한 점은 한 쪽 데이터센터의 게이트웨이만 RSS가 증가한다는 것이었습니다. 토스는 안정적인 서비스 운영을 위해 Active-Active 구조로 이중화 된 데이터센터를 구축하고 있기 때문입니다. 이런 상태에서는 대부분의 트래픽이 양쪽 데이터센터에 1:1의 비율로 인입되는 게 정상입니다.게이트웨이 접근 로그를 통해 확인해보니, 특정 라우트에 대한 요청은 RSS 지표가 우상향하는 데이터센터로만 인입되고 있었습니다. 그리고 그 라우트에서 다른 라우트에는 없는 필터가 있는걸 확인했습니다. CacheRequestBody 필터였는데요. 캐시로 인한 memory leak. Spring Cloud Gateway에서 기본으로 제공하는 필터지만 이게 범인일 가능성이 크겠다는 생각이 들었습니다. 그래서 Spring Cloud Gateway Github을 살펴보니 관련된 수정 사항을 확인할 수 있었습니다. 요청 바디를 캐싱한 후 이 메모리를 제대로 해제하지 않아 memory leak이 발생한 것이었습니다. 그리고 위 수정은 캐싱된 메모리를 제대로 해제하도록 수정한 내용이었어요. 관련 내용이라는 생각이 들어 그 수정사항이 반영된 버전을 사용해봤더니 native memoery leak이 사라진 것을 확인할 수 있었습니다.2. WebClient Memory Leak 이슈 파악하기비슷한 이슈가 서비스에서도 발생을 했는데요. 이 이슈의 문제 원인은 토스 뱅크에서 담당 서버 개발자분과 뱅크 서버 플랫폼팀에서 찾아주셨습니다. 먼저 문제 상황을 설명해 볼게요.어느 순간부터 토스 뱅크의 특정 서버에서 다음과 같은 로그가 발생하고, 컨테이너 메모리 사용량이 지속적으로 증가하는 것을 발견했습니다.LEAK: ByteBuf.release() was not called before it's garbage-collected이 로그는 Netty의 ResourceLeakDetector에서 찍는 로그인데요. 메시지에서 알 수 있는 것처럼 ByteBuf가 GC되기 이전에 release()함수가 불리지 않아 memory leak이 발생했습니다. 원인을 찾기 위한 여정은 길었지만, 결론적으로 찾아낸 원인은 Spring WebClient를 생성하는 다음 코드였어요.@Bean\nfun webClient(): WebClient {\n    ...\n\n    val builder: WebClient.Builder = WebClient.builder()\n        ...\n        .filter { request, next -> next.exchange(request).cache() }\n\n    ...\n    return builder.build()\n}해당 서비스에서는 토스 내부의 다른 마이크로서비스에 접근하기 위해 Spring Framework의 Reactive Stack에서 제공하는 HTTP 클라이언트인 WebClient를 사용하고 있었습니다. 그리고 매 요청마다 connection을 맺는 오버헤드를 줄이기 위해 connection pool을 사용하고 있었는데요. WebClient에서 사용하는 Reactor Netty는 HTTP 요청 후 취소가 발생하면 요청에 사용한 connection을 connection pool에 반납하는 대신 connection을 끊어버리도록 구현되어 있었습니다. 따라서 취소가 많이 발생하면 다른 요청들에서 connection을 새로 맺었고, 이로 인해 성능 저하 문제가 생겼습니다.WebClient를 사용하는 WebFlux 프로젝트를 사용해 WebClient 요청 취소를 유도하고 실험해보면 아래처럼 connection을 계속 맺는 것을 확인할 수 있습니다.다시 기존의 Memory Leak 문제로 돌아가 볼게요. WebClient를 생성하는 코드의 .filter { request, next -> next.exchange(request).cache() } 구문은 이러한 connection pool 문제를 해결하기 위해 추가된 구문입니다.  Mono.cache를 사용하면 이후의 subscriber에서 cancel 신호가 와도 source mono로 해당 cancel 신호가 전달되지 않습니다. 이 점을 이용해 WebClient 요청에 cancel이 발생해도 Reactor Netty로 전달하지 않고, 결국 conncetion을 끊지 않도록 한 것이죠.하지만 여기에서 memory leak 문제가 발생했습니다. 앞서의 WebFlux 프로젝트를 사용한 실험과 동일한 환경에서 .cache를 이용한 WebClient를 사용하도록 사용하도록 설정하고 다시 실험해보면 아래처럼 connection을 새로 맺지는 않지만 Netty Direct Buffer 사용량이 계속 늘어나는 것을 확인할 수 있습니다.WebClient는 Reactor Netty를 이용해 HTTP 요청을 한 후 Reactor가 받은 응답 바디를 원하는 객체 형태로 역직렬화화면서 응답 바디에 대한 메모리를 해제합니다. .cache를 사용하는 경우에는 WebClient로 cancel 신호가 온 이후에 이러한 로직을 탈 수 없어 memory leak이 발생한 것이죠.위에서 설명한 WebClient의 .filter 구문을 아래처럼 정리했어요. cancel이 발생한 경우 Mono를 정리할 때 응답 바디를 명시적으로 release하도록 수정한 코드입니다..filter { request, next ->\n    val isCancelled = AtomicReference(false)\n    val responseRef = AtomicReference<ClientResponse?>(null)\n\n    next.exchange(request)\n        .doOnNext(responseRef::set)\n        .doFinally {\n            if (isCancelled.get()) {\n                releaseResponseBody(responseRef.get())\n            }\n        }\n        .cache()\n        .doOnCancel {\n            isCancelled.set(true)\n            releaseResponseBody(responseRef.get())\n        }\n}이렇게 connection pool 이슈와 memory leak 이슈 두가지를 모두 잡을 수 있었습니다.3. Native Memory Leak의 원인 깊이 이해하기하지만 왜 heap 영역이 아닌 native memory가 증가하는 걸까요? 원인은 Spring Cloud Gateway와 Spring WebClient가 공통적으로 사용하는 Reactor Netty의 요청∙응답 바디 취급 방식과 관련이 있습니다. Netty가 요청∙응답 바디를 저장하는 데 사용하는 Buffer에는 Direct Buffer와 Heap Buffer가 있는데요. Reactor Netty HttpClient의 기본 설정으로는 소켓 I/O 성능에 이점을 가지는 Direct Buffer를 사용하고 있습니다. JVM heap 영역에 할당되는 Heap Buffer와 다르게 Direct Buffer는 native memory에 직접 할당됩니다. 이러한 특징 때문에 Reactor Netty를 사용하는 프로젝트에서 요청∙응답 바디를 올바르게 처리하지 않으면 heap 영역이 아닌 native 영역에서 memory leak이 발생합니다.조금 더 구체적으로 살펴볼게요. Reactor Netty는 기본적으로 HTTP 요청∙응답을 담는 데 사용하는 Buffer (ByteBuf)를 항상 새로 생성하지 않습니다. 대신 Buffer pool을 구성해두고 Buffer가 필요할 때 pool에서 가져다 쓰는 방식을 사용하는데요. Buffer를 사용하고, 사용을 마친 Buffer를 다시 pool로 돌려주는 등의 로직을 구현하는 데 reference counter를 이용합니다. 처음 pool을 통해 Buffer를 할당받을 때 초기 reference counter 값은 1이고, 동일한 Buffer를 다른 곳에서도 사용한다면 .retain()함수로 reference counter를 올립니다. 그리고 버퍼의 사용이 끝나면 .release()함수로 reference counter를 내려줍니다.  이 때 reference counter 값이 0이 되면 해당 Buffer는 다시 pool로 돌아가게 됩니다. 좀 더 자세한 설명은 Reference Counted Object 문서에서 확인하실 수 있습니다.정리해볼게요. Spring Cloud Gateway의 memory leak은 요청 바디를 캐싱하기 위해 바디를 가져올 때 reference counter가 올라갔지만, 저장된 바디를 잃어버리면서 reference counter를 다시 내려주는 로직을 타지 못해 발생했어요.WebClient의 memory leak은 .cache() 이후 WebClient 요청이 cancel되어 응답 바디에 대한 메모리 해제가 되지 못해 발생했습니다. 정상적인 경우 Jackson2JsonDecoder에서 응답 바디 bytes를 Buffer로부터 읽고 특정 타입의 Java 객체로 변환한 이후 Netty Buffer의 reference counter를 내려주는데 이 로직을 타지 못한거죠.지금까지 Reactor Netty를 사용하는 프로젝트들에서 발생한 native memory leak 이슈들과 원인을 살펴봤는데요. Reactor Netty 기반의 프로젝트에서는 Direct Buffer를 사용하는 응답∙요청 바디를 다룰 때는 항상 Buffer를 올바르게 소모하거나 잘 해제하는지 꼼꼼하게 확인을 해야 한다는 것을 다시 느낄 수 있었습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기함께 읽어보면 좋을 콘텐츠토스는 Gateway 이렇게 씁니다2023. 10. 12",
    "36": "프론트엔드 로깅 신경 안 쓰기최진영2023. 12. 20엔지니어링 노트 5: 프론트엔드 로깅 신경 안 쓰기엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. 이번에는 프론트엔드 개발자라면 한 번쯤 고민해봤을 클라이언트 로깅 개선 과정을 공유합니다.제품을 개발하다 보면 사용자가 어떻게 제품을 사용하는지, 제품을 사용할 때 어떤 행동을 했는지 알아야 할 때가 있어요. 그래서 우리는 로그 데이터를 사용해요. 로그를 기록하는 것을 '로깅'이라고 하는데요. 로깅으로 수집한 데이터로 사용자의 행동을 분석하거나, A/B 테스트의 결과를 확인하거나, 재현하기 어려운 환경에서 디버깅 할 수 있어요.이번 글에서는 토스페이먼츠 프론트엔드 챕터에서 로깅 방법을 개선한 과정을 소개해 볼게요.들어가기 전에로깅을 어떻게 개선했는지 소개하기 전에, 기존 방식을 살펴보려고 해요. 다음은 카드를 등록하는 페이지의 코드예요.import { Button, useToaster } from '@tossteam/tds';\n// ...\n\nconst REGISTER_CARD_SCREEN_LOG_ID = 123;\nconst REGISTER_CARD_CLICK_LOG_ID = 456;\nconst REGISTER_CARD_POPUP_LOG_ID = 789;\n\nconst PAGE_TITLE  = '카드 정보를 입력해주세요';\n\nfunction RegisterCardPage() {\n  // ...\n  const toast = useToaster();\n  const logger = useLogger();\n\n  useEffect(() => {\n    // 스크린 로그 요청\n    logger.screen({\n      logId: REGISTER_CARD_SCREEN_LOG_ID,\n      params: { title: PAGE_TITLE } \n    });\n  },[])\n\n  return (\n    <>\n    // ...\n    <Button onClick={async () => {\n      try {\n        // 클릭 로그 요청\n        logger.click({ \n          logId: REGISTER_CARD_CLICK_LOG_ID,\n          params: { title: PAGE_TITLE, button: '다음' }\n         });\n        await registerCard(cardInfo);\n        // ...    \n        } catch (error) {\n        // 토스트 팝업 로그 요청\n        logger.popup({\n          logId: REGISTER_CARD_POPUP_LOG_ID,\n          type: 'toast',\n          params: { title: PAGE_TITLE, message: error.message }\n        });\n        toast.open(error.message);\n       }\n     }}>\n     다음\n    </Button>\n   </>\n  );\n}위 코드에는 유저가 페이지에 접속했을 때 찍는 스크린 로그, 유저가 클릭했을 때 찍는 클릭 로그, 유저가 토스트나 모달을 봤을 때 찍는 팝업 로그들이 있어요. 그리고 이 로그들은 프론트엔드 개발자가 직접 코드 안에 작성했어요. 정의한 액션이 일어날 때마다 액션 직전에 로그 식별자인 logId와 함께 로깅해요.이 코드를 여러 차례 개선을 거쳐 아래와 같이 변경했어요.import { Button, useToaster } from '@tosspayments/log-tds';\nimport { LogScreen } from '@tosspayments/log-core';\n\nfunction RegisterCardPage() {\n  const toast = useToaster();\n  \n  return (  \n    <LogScreen title=\"카드 정보를 입력해주세요\">   \n      // ...\n      <Button onClick={async () => {\n       try {   \n         await registerCard(cardInfo);\n         // ...\n        } catch (error) {\n          toast.open(error.message);\n        }\n      }}> \n        다음\n      </Button>\n    </LogScreen>  \n  );\n}로깅과 관련된 로직들이 사라진 게 보이시나요? 개발자들이 직접 로깅 하는 코드는 대부분 사라지고 import 문이 바뀌었어요. 또 남아있는 로깅 로직은 선언적으로 작성되었고, LOG_ID나 PAGE_TITLE 와 같은 상수도 사라진 걸 볼 수 있어요.어떻게 이렇게 변경했을까요?로깅 지식 없애기위에서 봤던 예제 코드에서 가장 신경 쓰였던 부분은 어디였나요? 혹시 저와 같은 부분을 생각하고 계신가요?맞아요. 코드 처음부터 등장하는 LOG_ID라는 상수값이 신경 쓰였을 거예요. 토스의 기존 로깅 시스템을 모르는 사람이라면 이 값이 왜 필요하지? 라는 생각도 들 거고요.const REGISTER_CARD_SCREEN_LOG_ID = 123;\nconst REGISTER_CARD_CLICK_LOG_ID = 456;\nconst REGISTER_CARD_POPUP_LOG_ID = 789;logId는 간단히 말하면 로그 식별자예요. 여러 로그를 식별해 주는 id 값으로, 로그를 요청할 때 필수에요. 그래서 프론트엔드 개발자는 로그를 심을 때마다 문서에서 logId를 복사해서 코드에 붙여넣는 작업을 반복적으로 해야 했어요.로깅을 하려면 logId를 알아야 한다는 게 번거롭다고 생각했어요. 그래서 logId를 없애고 프론트엔드 개발자가 몰라도 되는 로그 식별자를 만들었어요. 서비스의 이름과 페이지의 라우트, 그리고 로깅 타입과 이벤트 타입 등 여러 정보를 합쳐서 자동으로 식별자를 만들고 logName이라는 이름을 붙였어요. 사람이 미리 정의하고 전달하는 대신, 애플리케이션이 스스로 식별자를 생성하고 이를 이용해서 로깅 하는 구조죠.\nfunction createLogName({ logType, eventType }: { logType: LogType, eventType?: EventType }) {\n  const serviceName = packageJson.name;\n  const routerPath = location.pathname;\n\n  const screenName = `payments_${serviceName}__${rotuerPath}`;\n  const eventName = `::${eventType}__${eventName}`;\n\n  const logName = `${screenName}${logType === 'event'? eventName : ''}`;\n\n  return logName;\n}logName을 도입함으로써 개발자는 더 이상 logId를 신경 쓸 필요가 없어졌어요. 매번 logId를 복사해서 코드에 붙여넣기를 하지 않아도 되고, 커뮤니케이션 비용도 자연스럽게 줄었어요. 로깅 작업을 하기 위해 개발자가 알아야 하는 지식이 하나 없어져서 기존의 불편함이 사라졌고요. 기존 코드와 비교해 볼게요. 다음과 같이 logId를 파라미터로 전달했던 코드가 사라진 걸 알 수 있어요.function RegisterCardPage() {\n  // ...\n  // (제거됨)\n  // const REGISTER_CARD_SCREEN_LOG_ID = 123; \n  // const REGISTER_CARD_CLICK_LOG_ID = 456;\n  // const REGISTER_CARD_POPUP_LOG_ID = 789;\n\n  useEffect(() => {\n    logger.screen({\n      // logId: REGISTER_CARD_SCREEN_LOG_ID, (제거됨)\n      params: { title: PAGE_TITLE }\n    });\n  },[])\n\n  return (\n    <>\n      // ...\n      <Button onClick={async () => {\n        try {\n          logger.click({ \n            // logId: REGISTER_CARD_CLICK_LOG_ID, (제거됨)\n            params: { title: PAGE_TITLE, button: '다음' }\n          });\n          await registerCard(cardInfo);\n          // ...    \n        } catch (error) {\n          logger.popup({ \n            // logId: REGISTER_CARD_POPUP_LOG_ID, (제거됨)\n            type: 'toast', \n            params: { title: PAGE_TITLE, message: error.message }\n          });\n          toast.open(error.message);\n        }\n      }}>\n        다음\n      </Button>\n    </>\n  );\n}하지만 아직 불편한 부분들은 남아있어요. 카드를 등록하는 페이지에 좀 더 복잡한 로직을 추가해 볼게요.function RegisterCardPage() {\n  // ...\n\n  return (\n    // ...\n    <Button onClick={async () => {\n      try {\n        logger.click({ params: { title: PAGE_TITLE, button: '다음' }});\n        await validatecCrdNumber({ cardNumber });\n        await validateCardOwner({ cardNumber, name });\n        await registerCard({ cardNumber, ... });\n        router.push('/identification');     \n      } catch (error) {\n        logger.popup({ \n          type: 'toast', \n          params: { title: PAGE_TITLE, message: error.message }\n        });\n        toast.open(error.message);\n      }\n    }}>\n      다음\n    </Button>\n  );\n}코드를 보면 다음 버튼을 눌렀을 때 동작하는 로직이 한눈에 읽히나요? 로직을 읽기 전에 로깅과 관련된 코드가 먼저 눈에 들어오진 않나요?실제로 페이지에서 수행하는 로직이 복잡할수록, 또는 로깅이 많을수록 가독성이 떨어졌어요. 페이지의 비즈니스 로직만 읽고 싶어도 로깅 관련 코드가 함께 섞여 있었기 때문이에요. 개발자가 신경 쓰는 부분은 로깅이 아니라 비지니스 로직일 때가 많은데, 매번 로깅 관련 코드를 한번 마주치고 난 뒤에 비즈니스 로직을 읽어야 하니 불편했어요.로깅 선언적으로 관리하기그래서 비즈니스 로직으로부터 로깅 로직을 격리하는 것, 그리고 어떻게 하면 읽기 좋게 선언적으로 코드를 작성할 수 있을지 고민했어요. 오직 ‘해당 영역에 로깅을 한다’라는 관심사만 남겨두고 코드를 최소한으로 작성할 방법을 고민한 끝에 LogScreen, LogClick 과 같은 로깅 컴포넌트를 만들었어요. 로깅 컴포넌트는 로깅을 수행하는 역할만 해요.export function LogScreen({ children, params }: Props) {\n  const router = useRouter();\n  const logger = useLogger();\n\n  useEffect(() => {\n    if (router.isReady) {\n      logger.screen({ params });\n    }\n  }, [router.isReady]);\n\n  return <>{children}</>;\n}export function LogClick({ children, params }: Props) {\n  const child = Children.only(children);\n  const logger = useLog();\n\n  return cloneElement(child, {\n    onClick: (...args: any[]) => {\n      logger.click({ params });\n\n      if (child.props && typeof child.props['onClick'] === 'function') {\n        return child.props['onClick'](...args);\n      }\n    },\n  });\n}이제 개발자는 오직 로깅하고 싶은 화면이나 버튼에 LogScreen, LogClick 컴포넌트를 사용하도록 바뀌었어요. 로깅을 좀 더 선언적으로 처리할 수 있게 됐죠. 또 비즈니스 로직에서 로깅 관련 로직을 격리하자는 처음의 목표도 달성했어요. ‘어느 시점에 로직을 남겨야 하지?’와 같은 구현도 신경 쓸 필요 없이 로깅 컴포넌트를 사용하기만 하면 되고요.function RegisterCardPage() {\n  // ...\n\n  return (\n    <LogScreen params={{ title: PAGE_TITLE }}>\n    // ...\n      <LogClick params={{ title: PAGE_TITLE, button: '다음' }}>\n        <Button onClick={async () => {\n          try {\n            await validatecCrdNumber({ cardNumber });\n            await validateCardOwner({ cardNumber, name });\n            await registerCard({ cardNumber, ... });\n            router.push('/identification');     \n          } catch (error) {\n            logger.popup({ \n              type: 'toast', \n              params: { title: PAGE_TITLE, message: error.message }\n            });\n            toast.open(error.message);\n          }\n        }}>\n          다음\n        </Button>\n      </LogClick>\n    </LogScreen>\n  );\n}로깅으로부터 코드 보호하기로깅을 어느 정도 고도화한 뒤에 아래와 같은 패턴의 요구사항이 많아졌어요.”스크린 로그에 찍힌 페이지 제목이 스크린 하위의 버튼들에도 찍히면 좋겠어요!““특정 영역에는 전부 A 로그 파라미터를 추가해 주세요!““이 서비스에서 사용하고 있는 식별자가 모든 로그 파라미터에 찍히게 해주세요!“이런 요구사항을 처리하려면 코드가 좀 더 복잡해질 수밖에 없었는데요. 이번에도 예시와 함께 볼게요. 카드를 등록하는 페이지의 모든 로그에 페이지 타이틀과 사용자의 id가 남도록 작성된 코드예요.function RegisterCardPage() {\n  const { userId } = useUser();\n  // ...\n\n  return ( \n    <LogScreen params={{ title: PAGE_TITLE }}>\n      // ...\n      <RegisterCardForm\n        logParameters={{ title: PAGE_TITME, userId }} \n        onSubmit={...}\n      />\n      <LogClick params={{ title: PAGE_TITLE, userId, button: '다음' }}>  \n        <Button onClick={...}> \n          다음\n        </Button>\n      </LogClick>\n    </LogScreen>\n  );\n}\n\nfunction RegisterCardForm({ logParameters, onSubmit }) {\n  return ( \n    <form onSubmit={onSubmit}>\n      <CardNumberField />\n      <LogClick params={{ ...logParameters, button: '카드번호 초기화' }}>\n        <Button>\n          카드번호 초기화\n        </Button>\n      </LogClick>\n      // ...\n    </form>\n  );\n}\n\n이런 코드에서는 로그 파라미터를 전달하기 위해 하위 컴포넌트의 props를 수정할 수밖에 없어요. logParameters라는 props를 추가해서 로그 파라미터를 전달해야 했고, 전달해야 하는 컴포넌트의 깊이가 깊어질수록 prop drilling이 심해졌어요.또, logParameters 라는 컴포넌트의 역할과는 거리가 먼 props가 추가되면서 컴포넌트의 인터페이스가 어색해졌어요. RegisterCardForm이라는 컴포넌트는 form이라는 역할에 필요하지 않은  logParameters라는 props가 추가된 것처럼요.이런 문제를 해결하기 위해 리액트의 Context를 사용했어요. 로그 파라미터를 관리하는 Context를 만들고 Provider로 특정 영역에 로그 파라미터들을 주입할 수 있게 만들었어요. 각 컴포넌트는 가장 가까운 LogParamsContext를 읽어서 로깅해주기만 하면 props 전달 없이도 로그 파라미터를 각 컴포넌트에 전달할 수 있었어요.interface Props {\n  children: ReactNode;\n  params?: LogPayloadParameters;\n}\n\nconst LogParamsContext = createContext<LogPayloadParameters | null>(null);\n\nexport function LogParamsProvider({ children, params }: Props) {\n  return (\n    <LogParamsContext.Provider\n      value={params}\n    >\n      {children}\n    </LogParamsContext.Provider>\n  );\n}\n\nexport function useLogParams() {\n  return useContext(LogParamsContext);\n}function useLogger() {  \n  const parentParams = useLogParams();\n\t\n  const log = (params) => {\n    logClient.request({ params: { ...parentParams, ...params });\n  }\n\t// ...\n}또 LogScreen 하위에 찍힌 로그는 모두 LogScreen의 로그 파라미터를 받을 수 있도록 LogScreen이 children을 LogParamsProvider로 감싸도록 만들었어요.export function LogScreen({ children, params }: Props) {\n  const router = useRouter();\n  const logger = useLogger();\n\n  useEffect(() => {\n    if (router.isReady) {\n      logger.screen({ params });\n    }\n  }, [router.isReady]);\n\n  return <LogParamsProvider params={params}>{children}</LogParamsProvider>\n}이렇게 함으로써 특정 영역 하위 로깅에 로그 파라미터가 필요한 경우 logParameters를 props로 전달할 필요가 없어졌어요. 오직 LogParamsProvider나 LogScreen을 활용하면 이전처럼 props를 만들어서 전달하거나 전역 상태를 직접 만들 필요 없이, 여러 로그 파라미터를 주입할 수 있게 되었어요. 로깅을 위해 인터페이스를 해치는 코드를 작성하거나 비즈니스 로직에 불필요한 데이터를 전달하지 않도록 바뀌었죠.function RegisterCardPage() {\n  // ...\n  return (\n    <LogScreen params={{ title: PAGE_TITLE, userId }}>   \n      // ...\n      <RegisterCardForm onSubmit={...}/> \n      <LogClick params={{ button: '다음' }}>\n        <Button onClick={...}>\n          다음  \n        </Button>   \n      </LogClick>  \n    </LogScreen> \n  );\n}\n\nfunction RegisterCardForm({ onSubmit }) { \n  return (  \n    <form onSubmit={onSubmit}>   \n      <CardNumberField />\n      <LogClick params={{ button: '카드번호 초기화' }}>\n        <Button>\n          카드번호 초기화 \n        </Button>\n      </LogClick>\n      // ... \n    </form>  \n  );\n}\n\n로깅 숨기기로깅하는 코드를 쭉 살펴보니, 비슷한 패턴이 정말 많았어요. 예를 들면 button을 LogClick으로 감싼다거나, radio를 LogClick으로 감싼다거나, toast가 뜰 때 logger.popup을 찍는 등 다양한 서비스에서 공통으로 나타나는 패턴들이 많았어요. 그리고 이런 코드는 토스의 디자인 시스템인 TDS의 컴포넌트와 함께 자주 사용됐어요.import { Button, useToaster } from '@tossteam/tds';\n\nfunction RegisterCardPage() {\n  const toast = useToaster();\n  // ...\n\n  return (\n    // ...\n    <LogClick params={{ button: '다음' }}>  \n      <Button onClick={() => {  \n        try { \n          // ...\n        } catch (error) {\n          logger.popup({ \n            type: 'toast',    \n            params: { message: error.message }  \n          }); \n          toast.open(error.message);\n        } \n       }}>  \n       다음  \n      </Button>\n    </LogClick>\n  );\n}TDS 또한 우리가 핸들링하는 영역이고, 어느 제품에서나 모두 사용하고 있으니 TDS 인터페이스를 그대로 따르는 TDS 로깅 컴포넌트를 만들면 어떨까? 라는 생각을 하기 시작했어요. 인터페이스는 일반 TDS와 같되, 내부적으로는 여러 이벤트의 로그를 찍는 컴포넌트로요.이런 아이디어를 기반으로 TDS 컴포넌트에 로그에 대한 책임을 추가로 부과하는 컴포넌트, 즉 하나의 로그 데코레이터를 단 TDS 컴포넌트들을 만들었어요. TDS를 감싸서 특정 이벤트가 발생했을 때 로깅하는 컴포넌트죠.interface Props extends ComponentProps<typeof TdsButton> {\n  logParams?: LogPayloadParameters;\n}\n\nexport const Button = forwardRef(function Button({ logParams, ...props }: Props, ref: ForwardedRef<any>) {\n  return (\n    <LogClick params={{ ...logParams, button: getInnerTextOfReactNode(props.children) }} component=\"button\">\n      <TdsButton ref={ref} {...props} />\n    </LogClick>\n  );\n});import { LogPayloadParameters } from '@tosspayments/log-core';\nimport { useLog } from '@tosspayments/log-react';\nimport { useToaster as useTDSToaster } from '@tossteam/tds-pc';\nimport { useMemo } from 'react';\n\ninterface LogParams {\n  logParams?: LogPayloadParameters;\n}\n\nexport function useToaster() {\n  const toaster = useTDSToaster();\n  const log = useLog();\n\n  return useMemo(() => {\n    return {\n      open: (options: Parameters<typeof toaster.open>[0] & LogParams) => {\n        log.popup({\n          component: 'toast',\n          params: { popupTitle: options.message, ...options?.logParams },\n        });\n\n        return toaster.open(options);\n      },\n      close: toaster.close,\n    };\n  }, [log, toaster]);\n}이제 프론트엔드 개발자는 import 문만 수정하면 로깅을 할 수 있어요. @tossteam/tds에서 @tosspayments/log-tds만 변경하면 컴포넌트가 스스로 로깅 하도록 변경할 수 있는 거죠. 또한 이미지의 ARIA 라벨이나 버튼 텍스트와 같은 UI 요소를 매번 로그 파라미터로 넘기지 않아도 자동으로 로깅할 수 있게 해서 중복 코드들도 사라졌어요.import { Button, useToaster } from '@tosspayments/log-tds';\n\nfunction RegisterCardPage() {\n  // ...\n\n  return (\n  // ...\n    <Button onClick={() => {\n      try {    \n        ... \n      } catch (error) {  \n        toast.open(error.message);\n      }\n    }}>  \n     다음\n    </Button>\n  );\n}정리하기개선된 코드를 다시 한번 볼까요? logId 를 넘겨주지 않아요.LogScreen을 이용해서 선언적으로 로그를 작성할 수 있어요.로그에 페이지 타이틀을 전달하지 않아도 돼요.버튼이나 토스트는 아예 로깅을 하지 않아도 돼요.로깅에 관한 코드는 import문을 제외하면 단 한 줄, LogScreen만 있어요.import { Button, useToaster } from '@tosspayments/log-tds';\nimport { LogScreen } from '@tosspayments/log-core';\n\nfunction RegisterCardPage() {\n  const toast = useToaster();\n\n  return (\n    <LogScreen title=\"카드 정보를 입력해주세요\">\n      // ...\n      <Button onClick={async () => {\n        try {\n          await registerCard(cardInfo);\n          // ... \n        } catch (error) {\n          toast.open(error.message);  \n        }\n      }}>\n        다음\n      </Button>   \n    </LogScreen>\n  );\n}그럼에도 불구하고…‘로깅을 최대한 신경 쓰지 않게 하자’는 목표로 시작한 프로젝트였지만 현재의 로깅 모듈 구조로는 해당 목표를 완수했다고 말하기 어려워요. 아래와 같은 이유 때문이에요.특정 서비스에 국한된 로그파라미터는 서비스 개발자가 일일이 추가해줘야 해요.서비스 개발자가 매번 LogScreen를 이용해서 스크린 로깅을 추가해줘야 해요.TDS를 커스텀하게 만들어서 사용하는 경우에는 로그 TDS 컴포넌트를 사용할 수 없어요.그래서 토스페이먼츠의 프론트엔드 챕터는 앞으로도 ‘어떻게 이런 문제를 해결하고 로깅을 더 신경 쓰지 않을 수 있을까’를 계속 고민하고 더 좋은 방법을 찾아나갈 예정이에요.Write 최진영 Review 임재후 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "37": "브라우저용 번들링 플러그인, 직접 만들었어요신지호2024. 1. 10엔지니어링 노트 6: 브라우저용 번들링 플러그인, 직접 만들었어요엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. 이번에는 직접 브라우저용 프론트엔드 번들링 플러그인을 만든 과정에 대한 이야기예요.지난해 말, 토스페이먼츠 DX(Developer eXperience) 팀은 개발자 경험을 위해 샌드박스라는 제품을 출시했어요. 샌드박스는 브라우저에서 바로 연동 흐름을 체험하고, 테스트 연동을 해볼 수 있는 개발자 도구예요. GitHub 레포지토리를 클론하거나 VS Code를 띄울 필요 없이 브라우저에서 바로 코딩하고 실행할 수 있는 코드 에디터라고 할 수 있어요. 샌드박스를 활용하면 코드와 결과물이 어떻게 서로 연결되어 있는지 한눈에 볼 수 있어 편리해요. 이 코드 에디터에서 가장 중요한 기능은 사용자가 코드를 수정할 때마다 미리보기에 업데이트해 주는 것이었어요. 이 기능을 위해 프론트엔드 번들링 플러그인을 만든 방법을 소개할게요.샌드박스 환경과 번들링번들링은 여러 개의 파일을 하나로 묶어 최종적으로 실행되는 웹 애플리케이션을 만드는 과정을 뜻해요. 우리 프로젝트에는 React, 결제위젯 SDK, react-router-dom 등의 외부 라이브러리를 사용하고 있고, 이 라이브러리들을 번들링해서 보여줘야 하죠.샌드박스에서는 실시간으로 변경되는 코드를 번들링 한 뒤 컴파일해야 하는데요. 번들링 해야 하는 환경이 브라우저라는 점이 일반적인 웹 애플리케이션과 다른 점이에요. 일반적인 번들링 과정은 파일 시스템에 직접 접근하여 파일들을 처리하고 합치는데, 브라우저는 보안상의 이유로 파일 시스템에 직접 접근할 수 없어요. 그래서 다른 방법을 찾아야 했습니다.브라우저에서 번들링하기이 문제를 풀 수 있는 방법 중 하나는 가상 환경을 활용하는 거예요. Sandpack이나 WebContainer 같은 도구들은 가상환경에 사용자가 입력한 코드를 전달하고 실제로 서버를 띄우는 방식이에요. 서버로 띄운 결과를 브라우저의 iframe에 임베드해서 보여주는 방식이죠. 이렇게 하면 실제 파일 시스템에 접근할 필요 없이, 브라우저 상에서 코드를 실행하고 결과를 볼 수 있어요.하지만 이 방식을 모든 상황에 적용하기는 어려워요. 예를 들어, iframe 특성상 주소 이동을 할 수 없는데 토스페이먼츠 SDK는 연동 과정에서 리다이렉트가 꼭 필요합니다. 그래서 위와 같이 가상 환경을 활용하는 iframe 기반 샌드박스 라이브러리는 사용할 수 없었어요.그래서 코드를 어떻게 브라우저 위에서 번들링 할 수 있을지 계속 찾아보다가 Rollup에서 제공하는 브라우저용 라이브러리를 알게 됐습니다. Rollup은 모듈 번들러인데요, @rollup/browser는 Node.js 환경이 아닌 브라우저에서 Rollup이 동작하도록 Node.js와 관련된 의존성을 제거하고 browser API로만 만든 라이브러리에요. 브라우저 환경에서의 번들링에 사용하죠. 그래서 토스페이먼츠처럼 샌드박스를 구현한 곳들(예: Rollup, Svelte , Preact) 대부분이 이 라이브러리를 사용하고 있어요.@rollup/browser 라이브러리 이해하기그래서 저도 이 라이브러리를 활용해 보기로 했어요. @rollup/browser는 파일 시스템이 아닌 메모리상의 데이터를 다뤄요. 실제 파일 시스템에 접근하는 대신 JavaScript 객체에 저장된 코드를 처리한다는 뜻이에요. 아래 예시를 보면서 이해해 볼게요.const modules = {\n  'index.js': `\n    import { createRoot } from 'react-dom/client';\n    import { Checkout } from './Checkout.jsx';\n  `,\n  'Checkout.jsx': `\n    import { useEffect, useRef, useState } from \"react\";\n    import { loadPaymentWidget, ANONYMOUS } from \"@tosspayments/payment-widget-sdk\";\n    // ...\n    function Checkout() {\n      // ...\n      return <div></div>\n    }\n  `,\n  // ...\n};\n\nrollup({})파일 시스템을 대신하는 데이터위 코드 예제에서 modules라는 객체는 여러 JavaScript 파일을 표현합니다. 각 키는 파일의 경로처럼 사용되고, 각 값은 해당 파일의 소스 코드를 나타냅니다. 이 객체는 파일 시스템에 있는 실제 파일이 아니라, 메모리상에 존재하는 데이터에요. 전통적인 번들링 도구는 파일 시스템에서 파일을 읽고 쓰지만, @rollup/browser는 이 modules 객체 내의 정보를 사용합니다. 코드에서 보이는 index.js와 Checkout.jsx는 실제 파일이 아니라, modules 객체 내에 정의된 가상의 파일입니다.가상의 '파일' 처리rollup은 이 가상의 파일들을 읽고 처리하여, 마치 실제 파일 시스템에서 작업하는 것처럼 번들링합니다.번들링이 잘 되면 개발자가 브라우저에서 코드를 수정할 때마다 변경 사항이 메모리상의 객체에 즉시 반영됩니다. 그리고 @rollup/browser는 이 변경된 코드를 사용하여 새로운 번들을 생성합니다.Rollup 설정rollup({}) 부분은 Rollup을 구성하는 설정을 정의합니다. 이 설정에는 번들링 과정에서 어떤 모듈을 어떻게 처리할지, 결과물을 어떻게 출력할지 등의 정보가 포함됩니다. 여기에 번들링 플러그인 로직을 작성하면 됩니다.이제 하나의 파일로 만들기 위한 번들링 로직을 직접 구현해 볼게요.브라우저용 번들링 플러그인 만들기React 같은 외부 라이브러리를 불러온다고 생각해 볼게요. 먼저 일반적인 Node.js 환경이라고 생각하면, 해당 라이브러리가 어디 있는지 Rollup이 알 수 없어요. 그래서 Rollup은 @rollup/plugin-node-resolve라는 플러그인을 사용해서 외부 라이브러리의 코드를 node_modules에서 찾아냅니다.그런데 브라우저 환경에는 파일 시스템이 없어서 네트워크를 통해 외부 라이브러리를 불러옵니다. Rollup은 ESM을 기반으로 하고 있으니 esm.sh를 활용해서 CDN 주소를 생성했어요. 이제 지정한 위치의 코드를 불러오기 위한 로직이 필요하겠죠. 그래서 외부 라이브러리를 불러오는 Rollup 플러그인을 직접 만들었는데요. Rollup 플러그인의 라이프사이클과 함께 자세히 살펴볼게요.라이프 사이클이 좀 복잡해 보이지만, 이 중에서 resolveId, load, transform 메서드만 사용해도 충분했어요. 각 메서드와 작성한 로직은 아래와 같아요.resolveId: 모듈 위치 식별 및 맵핑모듈 식별자가 어디에 존재하는지 해석하는 단계에요. 예를 들어 다음과 같은 import 문에서 .Checkout.jsx을 실제 파일 시스템 경로로 맵핑합니다.import { Checkout } from './Checkout.jsx'우리는 파일 시스템 구조를 사용하지 않기 때문에 이 단계에서 해당 파일이 메모리상에 존재하는지 확인하는 로직을 추가해야 합니다.const modules = {\n  'index.js': `\n    import { createRoot } from 'react-dom/client';\n    import { Checkout } from './Checkout.jsx';\n  `,\n  'Checkout.jsx': `\n    import { useEffect, useRef, useState } from \"react\";\n    import { loadPaymentWidget, ANONYMOUS } from \"@tosspayments/payment-widget-sdk\";\n    // ...\n    function Checkout() {\n      // ...\n      return <div></div>\n    }\n  `,\n  // ...\n};\n\nplugins: [\n  {\n    name: 'sandbox-loader',\n    resolveId(source) {\n      if (modules.hasOwnProperty(source)) {\n        return source;\n      };\n\n      // ./Checkout.jsx -> Checkout.jsx\n      if (modules.hasOwnProperty(getOnlyFileName(source))) { \n        return getOnlyFileName(source);\n      }\n\t},위 예제 코드는 React를 불러오는 부분인데요. 만약 resolvedId에 들어온source가 ‘react’라면 다음과 같이 https://esm.sh/react를 반환하는 방식이에요. 앞서 설명했던 esm.sh를 활용했어요.const url = new URL(source, 'https://esm.sh');\n\nreturn url.href;load: 코드 가져오기resolveId에서 식별한 위치에 있는 코드를 반환하는 단계에요. modules 객체로 관리되는 파일이면 객체에 있는 코드 내용을 사용하고, 외부 라이브러리면 resolveId에서 정의한 CDN 링크로 코드 내용을 가져옵니다.load(id) {\n  if (modules.hasOwnProperty(id)) {\n    return modules[id];\n  }\n\t\n  return getExternalModule(id);\n}\n\n// getExternalModule.ts\nconst cache = new Map(); // 이미 불러온 라이브러리는 cache에 저장해 둡니다.\nexport async function getExternalModule(url: string): Promise<string> {\n  if (cache.has(url)) {\n\ntransform: 코드 변형하기load에서 불러온 코드를 변형하는 단계에요. 기본 JavaScript를 사용한다면 필요하지 않은 단계지만, React와 같은 라이브러리를 사용해 코드에 JSX 문법이 있다면 JavaScript로 변환해 줘야 합니다. 코드 변환을 위한 도구로는 sucrase를 사용했어요.babel은 다양한 브라우저 커버리지가 높지만, sucrase는 브라우저 커버리지보다는 모던 JavaScript 환경을 타겟팅하여 속도에 초점을 맞춘 트랜스파일러인데요. 샌드박스 제품은 코드 수정이 생기는 빈도가 잦고, 수정이 생길 때마다 빌드를 새로 해서 속도가 빠른 게 중요하다고 판단해서 sucrase를 사용했습니다. @babel/standalone(브라우저에서 동작하는 babel 모듈)과 React 코드의 트랜스파일 속도를 비교해 보면 5~10배 정도 차이가 났답니다.sucrase를 사용해서 JSX를 기본 JavaScript 문법으로 변환하기 위한 코드는 다음과 같아요.import { transform as sucraseTransform } from 'sucrase';\n\ntransform(code, id) {\n  const output = sucraseTransform(code, {\n    filePath: `/${id}`,\n    transforms: ['jsx'],\n    jsxRuntime: 'automatic', // React 17 이후 변경된 JSX Transform 방식\n    production: true,\n  });\n\n  return output.code;\n},위 설정까지 마치면 이제 React 코드를 브라우저에서 실시간으로 빌드할 수 있어요. 마지막으로 사용자가 빌드 결과물을 볼 수 있도록 미리보기 영역의 HTML 요소에 script를 삽입해 주면 완성입니다. element.innerHTML에서는 script 태그가 동작하지 않아서 다음과 같은 방법으로 script 태그를 넣어줬어요.function injectHtmlWithBundledCode(previewElement: HTMLDivElement, html: string, js: string) {\n  // 1. script 태그를 제외한 html 삽입\n  previewElement.innerHTML = html;\n  const scriptElement = document.createElement('script');\n  const scriptText = document.createTextNode(js);\n  scriptElement.appendChild(scriptText);\n  // 2. script 엘리먼트 삽입\n  previewElement.appendChild(scriptElement);\n}\n\n// Preview.tsx\n\nconst { html, js } = useSandboxCode();\n\nuseEffect(() => {\n  if (previewRef.current != null) {\n    injectHtmlWithBundledCode(previewRef.current, html, js);\n  }\n}, [html, js]);\n\n...이제 브라우저 위에서 파일 시스템과 같은 방식으로 코드를 번들링하고 실행할 수 있게 됐어요.남은 과제들저에게는 앞으로 남은 과제들이 몇 가지 더 있는데요. 이번 MVP에서는 JavaScript와 React만 사용할 수 있어요. 그래서 React를 위한 처리 로직만 있는데요. 앞으로 Vue, Svelte와 같은 프론트엔드 도구들도 비슷한 방식으로 빌드할 수 있도록 로직을 추가하려고 해요. 또, 지금은 샌드박스에 파일 시스템이 없어서 모든 파일이 같은 레벨로 열려있는데, 가상 파일 시스템을 만든 뒤 플러그인 로직을 파일 시스템에 맞춰 작성해 볼 예정입니다. 앞으로 토스페이먼츠 개발자경험이 어떻게 더 진화하는지 지켜봐 주세요.함께 읽으면 좋은 콘텐츠📍결제 연동 코드없이 시작하기Write 신지호 Review 한재엽 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "38": "달리는 기차의 바퀴 교체하기 1. Planning한재엽ㆍFrontend Developer2024. 1. 11이미 운영 중인 제품을 전반적으로 다시 만들거나 리팩토링 하는 경험을 해볼 수 있는 기회는 흔치 않은데요. 운 좋게 팀 내 공감대가 형성되어 여러 팀원과 하나의 제품을 온전히 개선해 볼 수 있었어요. 이 글에서는 구조 개선에 앞서 어떻게 프로젝트를 플래닝하고 진행했는지 소개할게요.브랜드페이?토스페이먼츠의 결제 제품 중에는 브랜드페이라는 화이트 라벨(white label) 결제 제품이 있어요. 가맹점만의 간편 결제를 만들 수 있도록 해주는 제품이죠. 브랜드페이를 사용하면 가맹점 어디나 본인만의 브랜드를 입힌 간편결제 서비스를 고객 대상으로 제공할 수 있어요.가맹점은 우리가 제공하는 SDK를 통해 결제 수단 등록 및 관리 퍼널, 결제 화면 등에 브랜드명, 색상, 결제 화면 구성 등 다양한 요소를 커스터마이징할 수 있어요. 그래서 결제하는 소비자 입장에서는 브랜드페이를 통해 가맹점의 자체 간편결제 서비스를 이용하는 것처럼 느낄 수 있어요.제품이 마주한 문제브랜드페이를 도입하려는 회사의 상황은 다양했어요. 어떤 팀은 API로만 연동해서 클라이언트 기능은 건드리지 않았고, 어떤 팀은 버튼 텍스트 색상까지 바꾸고 싶어했죠. 이렇게 다양한 층위로 들어오는 요구사항을 받아들이려면 충분한 확장성이 필요했지만, 빠르게 성장하는 제품이다 보니 설계를 되돌아 보지 못하고 여러 요구사항에 대응하는데 보다 많은 리소스를 할애하게 되었어요. 그러다보니 새로운 기능을 추가하는 시간은 갈수록 길어졌어요. 운영에 드는 비용이 점점 불어나는 걸 느꼈죠.사실 우리가 일상에서 접하는 많은 제품이 이런 상황을 겪어요. 제품을 처음 만들 때는 이 제품이 얼마나 지속될지, 어떤 요구사항이 생겨날 지 알 수 없어요. 미리 변수를 예상할 수 없기 때문에 제품의 성숙도가 올라오는 시점에 이런 문제가 발생하는 건 어찌 보면 자연스러워요. 소프트웨어 제품의 숙명이라고 할 수도 있겠죠. 보통 이런 경우 ‘아무것도 하지 않거나’, 무언가를 개선하고자 시도하는데요. 후자를 선택한 제품팀은 아래 세 가지 선택의 기로에 놓여요.1번. 끊임없이 리소스를 더 투입하기현실적으로 불가능에 가까운 선택이에요. 회사는 비용 대비 생산되는 결과물이 효율적이어야 지속 가능한데, 리소스를 끊임없이 더 투자하는 것은 지속 가능하지 않기 때문이에요. 물론 새로운 기능을 추가하지 않는다면 리소스 투입 없이 운영만 할 수도 있겠죠. (Secret: 신규 개발 없이 운영만 하는 일을 누가 해야 하는지에 대한 문제가 추가로 생길 수 있어요.)2번. 새로 만들기가장 재밌고 쉬워 보이네요. 그린필드 프로젝트(Greenfield project: 기존에 존재하는 코드없이 새로 시작하는 프로젝트)라니, 얼마나 설레나요! 써보고 싶은 기술도 이것 저것 사용해 볼 수 있고, 왠지 이 프로젝트는 잘 만들 수 있을 것 같은 의지가 넘쳐 흐르죠. 그러다보니 대부분의 의사결정 방향은 2번으로 흘러가요. (Secret: 자칫 잘못하면 이후에 개편할 또 하나의 프로젝트를 만드는 것과 다름 없어요.)다만 이 선택에 있어서 한 가지 주의해야 할 점이 있다면 기존 제품을 운영하는 비용인데요. 기존에 운영하고 있던 제품을 완전히 신경쓰지 않고 종료할 수 있다면 다행이지만, 그렇지 못한 환경이라면(대표적으로 SDK 제품) 기존 제품과 새 제품을 동시에 운영해야 하는 슬픈 운명에 처하게 됩니다. 이렇게 N번이 반복되면 N개의 제품을 운영해야 하는 최악의 결말에 이르게 되죠.3번. 적당히 잘 개선하기누구나 이 방식을 원하고, 당연한 선택처럼 보이지만 운영 중인 프로젝트를 적당히 개선하기란 쉽지 않아요. 쉬웠다면 제품이 마주하는 문제는 없었을 겁니다. 관련해 \"달리는 기차의 바퀴를 갈아 끼운다\"는 유명한 격언도 있죠.우선 제품에 대한 맥락을 모두 알고 있어야 가능한 작업입니다. 코드를 수정할 때 더 이상 사용되지 않는 코드인지 어떤 방식으로 동작하는 코드인지 알고 있어야 코드를 수정할 수 있어요. 신뢰할 수 있는 문서라도 남아있어야 하는데 오래된 제품일수록 코드 외적인 것들이 제대로 챙겨지지 않더라구요.맥락을 알고 있어도 기존 동작을 온전히 보전하면서 코드를 수정하는 것은 쉽지 않은 작업이예요. 그렇기 때문에 테스트 코드를 작성하고 제품 코드를 수정하곤 하는데요. 보통 이런 제품들은 테스트 코드를 작성하기 어려운 구조로 작성되어 있습니다. 구조적인 어려움을 극복하고 동작하고 있는 모든 기능을 테스트 코드로 표현한 다음 코드를 수정해야 하기 때문이죠. 시간도 오래 걸리고 기술적으로 난이도가 높은 작업입니다. (Spoiler: 저희는 이 3번을 선택했어요.)문제 해결 기반 만들기1. 근본적인 문제 정의하기먼저 발생하는 현상을 관찰하고 왜 발생하는지 근본적인 원인을 파악하여 문제를 제대로 정의하는게 우선이었어요.현상 1. 장애 발생배포 후, 간헐적으로 일부 요소에 대한 장애가 발생하기도 했고, 좀 더 많은 고민이 소요되는 케이스가 있기도 했어요. 배포 후 발생하는 장애 상황은 담당 엔지니어로 하여금 자신감을 떨어뜨리는 요소 중 하나인데요, 이렇게 떨어진 자신감은 궁극적으로 코드 개선에 소극적으로 만들고 이로 인해 제품이 근본적으로 개선되지 못하는 결과를 만듭니다. 개선되지 못하는 제품은 또 다른 장애를 만들게 되어 악순환을 만들곤 해요.현상 2. 기능 추가의 어려움근본적인 개선이 필요하지만, 기능이 추가되거나 버그를 수정하기 위해 추가되는 코드 등으로 인해 복잡도가 높은 구조가 만들어졌어요. 기능을 추가하는데 어려워진 제품은 속도가 느려질 수 밖에 없고 담당 엔지니어의 고민도 깊어지게 되었습니다. 제품을 담당하는 엔지니어 입장에서 본인의 문제로 생각할 수 있으나, 이는 제품의 문제이자 팀의 문제이므로 보다 근본적인 접근이 필요했습니다.다행히 브랜드페이 제품의 문제는 명확했어요. 코드 중복과 테스트 코드였죠.문제 1. 파편화된 코드제품의 히스토리가 축적되고, 다양한 기능 추가와 업데이트가 진행됨에 따라 모든 코드를 이해하는 분을 찾기 어려울 정도가 되었고, 어떤 코드를 상황에 따라 사용해야 할지 여부에 대한 난이도도 점차 높아졌어요. 자연스럽게 같은 기능을 하는 코드들이 재생산되곤 했습니다. 화이트 라벨 제품의 특성상, 특정 가맹점을 위한 기능이 일부 존재하는데요. 이런 기능은 코드 외부에서 조합하는 방식으로 만들어져야 했지만, 그렇지 않은 경우도 간혹 발생했어요.문제 2. 리팩토링 내성이 약한 테스트 코드테스트 코드는 제품 코드가 명세대로 동작하는지 확인할 수 있는 중요한 코드입니다. 브랜드페이 제품 또한 결제가 이루어지는 제품이라 안정성이 중요하다고 판단했기 때문에 테스트 코드를 많이 작성했어요. 더 나아가 다음 세 가지를 고려하여 좋은 테스트 코드를 작성하기 위해 신경썼습니다.제품 명세를 잘 반영해야 함.테스트 케이스의 성공/실패가 빠르게 전달되어야 함.제품 코드의 구현을 수정해도 그 결과가 달라지지 않아야 함.그런데 몇몇 테스트 케이스에서 3번이 부족한 경우가 있었습니다. 테스트 코드를 잘 작성하려고 하다 보면 제품 코드도 자연스럽게 더 좋은 구조로 수정하곤 하는데요, 일정 상 구조 변경은 잠시 미뤄두고 기능을 검증하는데 집중하여 테스트 코드를 작성하곤 했어요. 당시에는 명세를 잘 표현하는 테스트 코드이고 제품 코드의 정상 동작을 잘 보장하여 많은 도움이 됐는데요, 나중에 제품 코드를 변경하려고 할 때 간혹 발목을 잡는 경우가 발생하곤 합니다. 이렇게 작성된 테스트 케이스는 여러 의존성을 가지게 되어 검증하는 비즈니스 로직이 변경되지 않았음에도 다른 변경에 의해 실패하곤 했는데요, 이 부분을 신경써줘야 했습니다.제품 코드를 수정했을 때, 기존 기능들이 제대로 동작하는지 믿고 맡길 수 있는 테스트 코드가 있어야 개발자가 자신감을 갖고 제품 코드를 변경할 수 있을 것이라 판단했습니다.2. 공감대 형성앞서 정의한 문제들이 얼마나 시급한지 엔지니어끼리만 인지하고 있다면 현실적으로 이를 개선하기 어려워요. 결국 개선 작업에 드는 비용 대비 기대 효과가 명확하게 정의되어야 하는데 기술적인 개선이라는 추상적인 결과물로는 설득하기 어렵기 때문입니다.브랜드페이 제품은 가맹점으로 하여금 지속적으로 이용하고 싶은 제품 중 하나였어요. 앞으로도 많은 기능들이 추가될 예정이었고 토스페이먼츠와 가맹점의 성장에 많은 기여를 할 제품이라는 의견이 모아졌습니다. 장기적인 관점에서 엔지니어링 해야 하는 이유가 있는 제품이라고 판단했어요.이러한 의견을 바탕으로 이해관계자 분들과 리소스에 대한 이야기를 나눌 수 있었어요. 제품을 담당하는 TPO(Technical Product Owner)분도 리소스 대비 효율성을 높여야 한다는 측면에 대해 공감하였고, 제품의 방향성과 현재 상황을 고려했을 때 추가적인 리소스 투입을 통해 제품을 고도화할 필요가 있음을 Head of Technology 분과의 논의를 통해 도출할 수 있었어요.3. 목표 설정하기단순 리팩토링이 아닌 ‘재구조화’ 우선브랜드페이라는 제품은 결제 고객이 마주하는 클라이언트 제품뿐만 아니라 가맹점에서 사용하는 SDK 제품도 있어요. 즉 기존에 운영하고 있던 제품을 완전히 종료할 수 없는 제품이라는 뜻인데요. 앞서 이야기한 방법 중 2번째 방법인 ‘새로 만들기’는 운영 비용이 많이 들어간다고 판단했어요. 자연스럽게 3번인 어떻게 하면 지금의 제품을 잘 개선해 볼 수 있을까 고민하게 됐어요.'재구조화'라는 단어는 리팩토링보다 좀 더 거시적인 관점에서의 개선을 뜻해요. 리팩토링은 아시다시피 모듈 간의 협력 관계를 개선하거나, 함수의 구현을 수정하는 등 가독성을 높이기 위한 작업 등을 의미하는데요, 리팩토링을 위한 프로젝트 구조 개선 작업이라고 생각하면 좋을 것 같아요. ‘나무보다는 숲’을 중심에 두고 싶었죠. 물론 리팩토링도 제품을 개선하는 데 있어서 꼭 필요한 작업이기 때문에 재구조화 작업을 진행한 다음 다시 시스템을 만들어서 진행하기로 했어요.이번 프로젝트에서는 테스트 코드의 리팩토링 내성을 가장 먼저 개선하려고 했어요. 테스트 코드가 변경을 방해하지 않도록 수정하고 개발자가 자신 있게 수정, 배포를 할 수 있도록 말이죠.그러면서 동시에 테스트를 작성하기 쉽게 만드는 것 또한 중요했어요. 지금 당장의 문제만 해결한다면 시간이 지나 또 같은 문제가 발생할 것이기 때문에 앞으로 추가될 기능에 대해서도 테스트 코드를 잘 작성할 수 있는 기반을 마련해야 했어요.프리모템(pre-mortem)으로 시작하기즉각적인 리소스 투입을 통해 코드를 수정하기에는 제품 전체를 대상으로 하는 작업이기에 크기가 컸어요. 그래서 개선에 대한 플래닝이 필요했고, 먼저 프리모템을 해보기로 했습니다. 장애가 발생하면 재발을 방지하기 위해 사후 회고(Postmortem)를 하는데, 프리모템은 발생할 것 같은 문제 상황을 가정하고 미리 회고하는 것을 뜻해요.이 개선 프로젝트가 제대로 진행되지 않았다면, 그리고 원하는 목표를 달성할 수 없었다면 왜 그랬을지 먼저 회고해봤어요.우선순위이미 운영 중인 제품이기 때문에 새로운 기능 개발을 멈출 수는 없었어요. 새 기능 개발과 기존 기능 개선을 동시에 해야 했죠. 자칫 리소스 분배를 잘못하면 새로운 기능을 만드는 것에 집중하게 되고 개선 작업을 못 하게 될 것 같았어요. 일정이 정해져 있는 상황에서 눈에 보이는 이익을 가져다주는 작업과 그렇지 않은 개선 작업을 비교했을 때 후자는 대부분 우선순위가 밀리는 경우가 많은 편이기 때문이에요.따라서, 브랜드페이 제품에 투입되는 리소스를 구성할 때, 신규 기능을 개발하기 위한 리소스와 운영하면서 개선 작업을 진행할 리소스를 철저히 분리했어요. 신규 개발과 개선 작업의 우선순위를 동일 선상에서 파악하지 않고 다른 트랙에서 고민하기 위해서였어요. 최종적으로 운영/개선 작업엔 필자 포함 2명, 신규 개발엔 1명으로 총 3명이 작업을 진행했습니다.기술적인 역량개선 작업을 위한 리소스를 확보해도 문제를 정확하게 개선할 수 있는 기술적인 역량이 뒷받침되어야 해요. 똑같이 문제있는 테스트 코드를 작성하거나 변경에 유연하지 못한 코드로 수정한다면 원하는 목표를 달성하지 못해 개선 작업을 하는 의미가 없어지겠죠.다행히 팀 내에 신뢰할 수 있는 분이 계셔서 별도의 신규 채용 없이 팀을 셋업할 수 있었어요.프로젝트 매니징개선 프로젝트는 제품의 로드맵을 설계하고 기능을 구현하기 위해 요구사항을 분석하는 프로젝트와는 다른 면이 있었어요. 앞서 이야기한 ‘기대 효과’라는 부분을 측정하기 어려운데요, ‘이 기능을 추가 했을 때 몇 명의 사용자가 늘어난다.’처럼 ‘제품을 개선한다면 생산성이 몇 % 오를 수 있다.’와 같이 수치화하기 어렵기 때문입니다.일정 산정도 쉽지 않아요. 기능을 구현하기 위한 일정은 기능을 배포하면 마무리되는 것이지만 개선 작업은 끝도 없이 진행할 수 있기 때문이죠. 앞서 이야기한 것처럼 목표를 구체적으로 정할 수 없으니 개선 작업을 어느 시점에 마무리해야 하는지도 고민이 필요했어요.별다른 성과 없이 작업 기간만 길어진다면 작업에 참여하는 엔지니어들의 동기부여도 떨어질 것이고 해결하고자 했던 문제도 제대로 해결 못할 수 있다고 판단했어요.문서화이 프로젝트가 제대로 돌아가려면 목표가 명확해야 하고 업무가 제대로 분배되어 진행하고 있는 것들이 투명하게 공유되는 것이 중요하다고 판단했어요. 그래서 프로젝트가 늘어지지 않도록 기대하는 설계를 먼저 그렸고, 이 설계에 이르기까지 진행 상황을 가시화 했습니다.브랜드페이 개선 프로젝트 노션 데이터베이스마무리PM 직군은 아니지만 팀의 목표를 달성하는 데 있어서 중요한 문제라고 생각했고 직접 팀을 셋업해서 진행해 봤어요. 문제를 명확하게 정의하는데 집중했고 프로젝트가 잘 진행될 수 있도록 여러 장치들을 고민했는데 어려운 점도 많았고 배운 점도 많았습니다.이번 글에서는 제품 개선 작업을 어떻게 시작했고 어디에 집중했는지 다뤘다면 다음 편에서는 실제로 어떻게 개선했는지 기술적인 내용을 중심으로 다룰 예정입니다. 어떤 방식으로 테스트 코드의 신뢰를 확보하고 제품을 리팩토링해 나갔는지를 소개할 예정이니 많은 관심 부탁드려요.Write 한재엽 Review 박성현 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "39": "Spring JDBC 성능 문제, 네트워크 분석으로 파악하기강민주ㆍ토스페이먼츠 서버 개발자2024. 1. 17엔지니어링 노트 7: Spring JDBC 성능 문제, 네트워크 분석으로 파악하기엔지니어링 노트 시리즈는 토스페이먼츠 개발자들이 제품을 개발하면서 겪은 기술적 문제와 해결 방법을 직접 다룹니다. 이번에는 토스페이먼츠 정산 플랫폼에서 많은 양의 정산 데이터 처리 과정에서 생긴 지연 이슈를 처리한 방법을 소개해요.토스페이먼츠 정산 플랫폼에서는 가맹점의 모든 정산 거래 건을 처리하고 있는데요. 많은 양의 정산 데이터 처리를 위해 스프링 배치(Spring Batch)와 JDBC(Java Database Connectivity)를 사용해요. 최근 신규 정산 시스템을 구현하는 과정에서 문제가 있었는데요. 스프링 배치 내에서 JDBC로 대량의 데이터 insert가 이루어질 때 속도가 지연되는 현상이었어요. 문제 현상의 원인을 찾고 해결한 과정을 공유합니다.bulk insert 성능 저하 현상 발견JDBC 템플릿은 스프링에서 제공하는 데이터베이스 연결 및 작업을 쉽게 할 수 있도록 하는 도구인데요. 템플릿에서 제공하는 batchUpdate()는 여러 개의 데이터베이스 업데이트(예: insert, update) 명령을 한 번에 묶어서 처리합니다. 이를 'bulk insert'라고 부르는데요. 많은 양의 데이터를 데이터베이스에 삽입하는 작업이에요.JDBC를 이용해 bulk insert를 하기 위해 다음처럼 Repository 코드를 작성합니다.@Component\nclass SettlementStepRepository(\n  @Qualifier(\"settlementJdbcTemplate\")\n    private val jdbc: NamedParameterJdbcTemplate,\n) {\n  @Transactional\n    fun insertAll(steps: List<SettlementStep>) {\n      val namedParameters = steps.map { it.toSqlParam() }\n      jdbc.batchUpdate(\n        \"\"\" \n          INSERT INTO SETTLEMENT_STEP\n          (....) \n          VALUES \n          (....)\n          \"\"\".trimIndent(),\n        namedParameters.toTypedArray(),\n      ) \n    }\n//...정말 흔하게 볼 수 있는 batchUpdate를 활용한 대량 insert 구현체에요. 그런데 이 Repository를 이용해 구현한 스프링 배치 ItemWriter에서 5000개의 객체를 삽입할 때 무려 1분 이상이 걸리는 현상을 발견했습니다.문제 원인 찾기1. TCP 패킷 분석처음에는 지연 원인을 찾기가 어려웠습니다. 다른 로직 없이 단순히 데이터를 삽입하는 Writer 단계였기 때문이죠. 막연하게 데이터 삽입 작업을 블로킹하는 로직이 배치 내에서 돌고 있다는 느낌은 있었지만, 내부 어플리케이션 로그에서는 문제를 발견할 수 없었어요. 그래서 JDBC에서 실행되는 쿼리를 확인하기 위해 로그 레벨을 변경해 봤어요. 데이터베이스와의 상호작용 중에 발생하는 모든 쿼리를 기록해 본 거죠. 하지만 이 방법으로도 insert 쿼리 외에 다른 쿼리는 발견되지 않았어요. 즉, 지연의 원인이 insert 쿼리 자체는 아닌 것 같았어요.복잡한 로직이 없는데도 지연 문제가 발생한다면 데이터베이스와의 통신 중에 어떤 블로킹이 발생한 게 아닐까 추측했어요. 그래서 TCP 패킷 캡쳐(네트워크를 통해 전송되는 데이터 패킷을 포착하여 분석하는 것)를 하기로 했어요. 데이터베이스와 배치 프로그램 간에 실제로 어떤 쿼리를 주고받는지 확인하기 위해서였죠.WireShark라는 프로그램을 통해 로컬 데이터베이스 호스트에 연결하면, 해당 호스트를 통해 주고받는 모든 TCP 패킷을 확인할 수 있습니다. 캡쳐링한 TCP 패킷은 follow TCP Stream 기능을 사용하면 쉽게 읽을 수 있는 형태로 스트림을 재조립해 줘요.Wireshark를 세팅한 후 배치를 실행해 보니, 놀라운 결과가 나왔습니다. bulk insert 쿼리가 실행되기 전에 해당 데이터베이스 테이블에 대한 select 쿼리가 다량으로, 계속해서 실행되고 있었거든요.2. JDBC 코드 디버깅select 쿼리가 실행된 이유를 알아내기 위해 JDBC의 NamedJdbcTemplate 클래스의 batchUpdate 함수를 디버깅 해봤어요. 전체 디버깅 흐름은 아래와 같아요.구현할 때 사용한 batchUpdate()는 JdbcTemplate 내 setValues()을 오버라이딩 해서 JdbcTemplate.batchUpdate()를 그대로 호출합니다.오버라이딩한 setValue()를 살펴보니PreparedStatementCreatorFactory의 setValues()에서 문제가 발생하고 있었어요.PreparedStatementCreatorFactory의 setValues() 내부에서는StatementCreatorUtils의 setParameterValue() 메서드를 실행하고 있었습니다.// NamedJdbcTemplate.java\n\n@Override\n  public int[] batchUpdate(String sql, SqlParameterSource[] batchArgs) {\n    if (batchArgs.length == 0) {\n      return new int[0];\n    }\n\n    ParsedSql parsedSql = getParsedSql(sql);\n    PreparedStatementCreatorFactory pscf = getPreparedStatementCreatorFactory(parsedSql, batchArgs[0]);\n    \n    // 여기서 this.getJdbcOperations()은 JdbcTemplate 객체를 반환\n    return getJdbcOperations().batchUpdate(\n      pscf.getSql(),\n      new BatchPreparedStatementSetter() {\n        @Override\n        public void setValues(PreparedStatement ps, int i) throws SQLException {\n          Object[] values = NamedParameterUtils.buildValueArray(parsedSql, batchArgs[i], null);\n          pscf.newPreparedStatementSetter(values).setValues(ps);\n        }\n        @Override\n        public int getBatchSize() {\n          return batchArgs.length;\n        }\n    });\n\t}// PreparedStatementCreatorFactory.java\n@Override\n  public void setValues(PreparedStatement ps) throws SQLException {\n    // Set arguments: Does nothing if there are no parameters.\n    int sqlColIndx = 1;\n    for (int i = 0; i < this.parameters.size(); i++) {\n      Object in = this.parameters.get(i);\n      SqlParameter declaredParameter;\n      // ...\n\n      if (in instanceof Iterable && declaredParameter.getSqlType() != Types.ARRAY) {\n        // ...\n      } else {\n        StatementCreatorUtils.setParameterValue(ps, sqlColIndx++, declaredParameter, in);\n      }\n    }\n  }StatementCreatorUtils의 setParameterValue()는 내부에서 setParameterValueInternal()를 호출하는데, 이 함수는 PreparedStatement에 인자로 넘긴 값을 어떻게 세팅할지 결정하는 로직을 담고 있습니다. PreparedStatement는 데이터베이스에 SQL 쿼리를 보내기 전에 SQL 문을 미리 준비하고 매개변수화하는 객체에요.// StatementCreatorUtils.java\nprivate static void setParameterValueInternal(PreparedStatement ps, int paramIndex, int sqlType,\n  @Nullable String typeName, @Nullable Integer scale, @Nullable Object inValue) throws SQLException {\n\n  String typeNameToUse = typeName;\n  int sqlTypeToUse = sqlType;\n  Object inValueToUse = inValue;\n\n  // override type info?\n  if (inValue instanceof SqlParameterValue) {\n    //...\n    inValueToUse = parameterValue.getValue();\n  }\n\n  if (logger.isTraceEnabled()) {\n    logger.trace(\"Setting SQL statement parameter value: column index \" + paramIndex +\n      \", parameter value [\" + inValueToUse +\n      \"], value class [\" + (inValueToUse != null ? inValueToUse.getClass().getName() : \"null\") +\n      \"], SQL type \" + (sqlTypeToUse == SqlTypeValue.TYPE_UNKNOWN ? \"unknown\" : Integer.toString(sqlTypeToUse)));\n  }\n\n  if (inValueToUse == null) {\n    setNull(ps, paramIndex, sqlTypeToUse, typeNameToUse);\n  } else {\n    setValue(ps, paramIndex, sqlTypeToUse, typeNameToUse, scale, inValueToUse);\n  }\n}문제는 넘긴 값으로 null이 들어갈 때였어요. 만약 인자로 들어가는 값이 null이면, 해당 값은 inValueToUse라는 지역 변수를 null로 설정하고, setNull() 이라는 내부 함수를 실행해요.setNull() 함수 내에서 문제가 된 부분은 세팅하는 값이 null일 때 이 값에 대응하는 SqlType을 JDBC가 알 수 없다는 거였어요. SqlType은 JDBC에서 사용하는 데이터 타입을 정의하는데요. PreparedStatement에서 해당 값을 어떻게 세팅할지 알 수 없어서 직접 데이터베이스로부터 타입 정보를 가져오려고 시도한 거죠.즉, null 값을 처리할 때 필요한 데이터 타입 정보를 데이터베이스로부터 가져오는 이 추가 작업이 성능 저하를 일으켰던 거예요.setNull 함수가 성능에 미치는 영향아래 코드를 보면서 setNull 함수를 좀 더 자세히 살펴볼게요. 함수 내에서 shouldIgnoreGetParameterType 조건에 따라 다른 작업이 실행되는데요. 만약 이 값이 false면 SqlType을 찾아오기 위해서 getParameterMetaData()를 호출합니다. getParameterMetaData()는 사용하는 데이터베이스의 드라이버 구현체에 정의된 함수로, PreparedStatemenet에 설정된 파라미터의 타입 정보(ParameterType)를 가져오는 역할을 하죠.private static void setNull(PreparedStatement ps, int paramIndex, int sqlType, @Nullable String typeName)\n  throws SQLException {\n    if (sqlType == SqlTypeValue.TYPE_UNKNOWN || (sqlType == Types.OTHER && typeName == null)) {\n      boolean useSetObject = false;\n      Integer sqlTypeToUse = null;\n      if (!shouldIgnoreGetParameterType) {\n        try {\n          sqlTypeToUse = ps.getParameterMetaData().getParameterType(paramIndex);\n        }\n        catch (SQLException ex) {\n          if (logger.isDebugEnabled()) {\n            logger.debug(\"JDBC getParameterType call failed - using fallback method instead: \" + ex);\n          }\n        }\n      }\n    //...\n}getParameterMetadata 함수가 SQLException을 내려줄 수 있는 것을 보니, 실제로 데이터베이스와 직접 커넥션을 맺어 쿼리 한다는 것을 유추할 수 있어요.저는 Oracle에 있는 테이블에 데이터를 insert하고 있는 상황이었기 때문에 JDBC 내에서는 OracleParameterMetaData 라는 구현 객체를 사용하고 있었는데요. 이 클래스의 구현을 보니, 실제 해당 PreparedStatement에 대한 메타데이터를 조회하는 쿼리를 직접 생성해서 요청을 데이터베이스에 보내고 있습니다.// OracleParameterMetaData.class\nstatic final ParameterMetaData getParameterMetaData(OracleSql var0, Connection var1, OraclePreparedStatement var2) throws SQLException {\n  OracleParameterMetaData var3 = null;\n  String var4 = var0.getSql(true, true);\n  int var5 = var0.getParameterCount();\n  OracleParameterMetaDataParser var6 = null;\n  String var7 = null;\n  if (!var0.sqlKind.isPlsqlOrCall() && var0.getReturnParameterCount() < 1 && var5 > 0 && !BAD_SQL.contains(var4.hashCode())) {\n    var6 = new OracleParameterMetaDataParser();\n    var6.initialize(var4, var0.sqlKind, var5);\n            \n    try {\n      var7 = var6.getParameterMetaDataSql(); // metadata를 가져오기 위한 sql 생성\n    } catch (Exception var14) {\n      var7 = null;\n    }\n  }\n\n    if (var7 == null) {\n      var3 = new OracleParameterMetaData(var5);\n    } else {\n      PreparedStatement var8 = null;\n\n      try {\n        var8 = var1.prepareStatement(var7); // 쿼리 실행\n        ResultSetMetaData var9 = var8.getMetaData();\n\t\t\t}\n    // ...\n// ...해결하기: 파라미터 타입 명시위 내용을 바탕으로 해결 방법은 2가지로 추려질 수 있었는데요.shouldIgnoreGetParameterType의 설정 변경하기첫 번째 방법은 shouldIgnoreGetParameterType 설정을 변경하는 거예요. getParameterType을 무시하는 설정을 하면 타입을 지정하기 위해 추가 쿼리를 실행하는 대신, Spring JDBC가 내부적으로 null 값을 어떻게 설정할지 결정할 수 있어요. 이는 데이터베이스 벤더에 따라 다르게 처리되거나, 처리가 불가능하면 각 데이터베이스 드라이버의 구현체로 이 문제를 위임합니다. 이 설정은 Spring 내에서 spring.jdbc.getParameterType.ignore를 true로 설정해서 적용할 수 있어요.파라미터를 넘길 때 타입 명시하기두 번째 방법은 파라미터를 전달할 때 파라미터의 SqlType을 명시하는 방법이에요. 값이 null이어도 해당 파라미터에 대한 SQL 타입을 명확히 알고 있다면, setNull()이 메타데이터를 조회하기 위해 쿼리를 실행하지 않을 수 있어요.저는 이 두 가지 방법 중에서 두 번째 방법을 선택했어요. 먼저 전체 시스템에 영향을 줄 수 있는 Spring 설정을 바꾸는 것이 좋은 방법 같지 않았어요. 그리고 특정 값이 null로 설정될 수 있는 경우가 제한적이고 예외적이라고 판단해서 타입 명시로 충분하다고 생각했습니다. 그래서 다음과 같이 직접 해당 파라미터의 SqlType을 선언하는 방식으로 문제를 해결했어요. return MapSqlParameterSource()\n  .addValue(\"originId\", internalOriginId)\n  .addValue(\"authDate\", authDate)\n  .addValue(\"cancelDate\", cancelDate, Types.NULL)\n\n이렇게 파라미터 타입을 명시하는 개선 작업을 한 뒤, 100만 건 내외의 거래 데이터를 insert 하는 데 18분 소요되던 배치가 2분으로 줄어들었습니다.다른 데이터베이스에서는 문제가 없을까?앞서 살펴본 것처럼 Oracle은 타입이 null로 설정될 때마다 메타데이터를 조회해주는 쿼리를 실행하는 것을 알 수 있는데요. 다른 데이터베이스에도 비슷한 문제가 발생할지 궁금했어요. 그래서 MySQL의 드라이버인 mysql-connector에서 PreparedStatement의 구현을 살펴봤습니다. MySQL은 parameterMetaData를 한 번 초기화한 후에는 이 객체를 계속 재사용해서 결과를 제공하고 있었습니다. 즉, 같은 PreparedStatement를 사용한다면, 이미 가져온 parameterMetaData를 재사용해서 결과를 제공하는 것이죠.// ClientPreparedStatement.java\n@Override\n  public ParameterMetaData getParameterMetaData() throws SQLException {\n    synchronized (checkClosed().getConnectionMutex()) {\n      if (this.parameterMetaData == null) {\n        if (this.session.getPropertySet().getBooleanProperty(PropertyKey.generateSimpleParameterMetadata).getValue()) {\n          this.parameterMetaData = new MysqlParameterMetadata(((PreparedQuery) this.query).getParameterCount());\n        } else {\n          this.parameterMetaData = new MysqlParameterMetadata(this.session, null, ((PreparedQuery) this.query).getParameterCount(),\n          this.exceptionInterceptor);\n        }\n      }\n      return this.parameterMetaData; // 한번 가져온 parameterMetaData를 재사용\n    }\n  }Oracle과 MySQL의 드라이버 구현체 차이가 여기서 확연히 드러나는 것을 확인할 수 있었는데요. MySQL은 ParameterMetaData라는 객체를 PreparedStatement 내부에서 직접 관리하고 있어서 같은 Statement에 대한 메타데이터를 재사용할 수 있는 구조였어요. 반면 Oracle 구현체는 PreparedStatement와 ParameterMetadata 객체 간의 직접적인 연관 없이 별개의 객체로 구성되어 있고, 필요할 때마다 sqlObject를 넘겨 쿼리에 대한 메타데이터를 가지고 오고 있었어요.이런 구조적 차이가 있어서 Oracle에서는 하나의 PreparedStatement에 null 값을 가진 요청이 있을 때마다 매번 ParameterMetaData 값을 조회하기 위해 관련 함수를 호출했던 것이죠. 이 과정이 성능에 영향을 미쳤고요.// OraclePreparedStatement.class\npublic ParameterMetaData getParameterMetaData() throws SQLException {\n  this.connection.beginNonRequestCalls();\n\n  ParameterMetaData var1;\n  try {\n    this.ensureOpen();\n    // PreparedStatement와 별도로 ParameterMetaData를 관리\n    var1 = OracleParameterMetaData.getParameterMetaData(this.sqlObject, this.connection, this);\n  } finally {\n    this.connection.endNonRequestCalls();\n  }\n  return var1;\n}이번 성능 저하 현상을 해결하면서 저에게 두 가지 중요한 경험이 남았는데요. 첫 번째는 성능 이슈를 진단하기 위해 네트워크 패킷을 직접 분석하는 새로운 접근 방식을 시도한 거예요. 이 방법은 단순히 로그 분석만으로는 알 수 없었던, 문제의 근본 원인이 되는 부분을 파악하는 데 매우 효과적이었어요. 두 번째는 Spring JDBC와 각각의 데이터베이스 드라이버 구현체가 어떻게 상호 작용하고 실행되는지에 대한 깊이 있는 이해를 얻었다는 것이고요. 처음에는 이해하기 어려웠던 성능 문제를 깊이 파악하고 해결해 볼 수 있어 의미 있는 시간이었습니다. 이런 문제 해결에 관심이 있는 분이라면 토스페이먼츠 서버 챕터에 합류해서 함께 도전해 봐요!Write 강민주 Review 황진성, 박동호 Edit 한주연재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "40": "그 많은 개발 문서는 누가 다 만들었을까 (1) 토스페이먼츠 테크니컬 라이터가 하는 일한주연ㆍ테크니컬 라이터2024. 1. 23안녕하세요, 저는 토스페이먼츠 테크니컬 라이터 한주연입니다. 지난 3년 동안 토스페이먼츠와 토스 커뮤니티에서 테크니컬 라이터로 일을 해왔는데요. 토스 커뮤니티의 테크니컬 라이터들은 개발자를 위한 문서를 쓰는 것뿐만 아니라, 멋진 개발자 경험을 제공하기 위해 다양한 일을 하고 있답니다. 이번 시리즈에서는 테크니컬 라이터의 일, 그리고 개발자 경험으로 역할을 확장해 온 과정을 소개할게요.테크니컬 라이터(Technical Writer)란?테크니컬 라이터는 기술적인 내용을 글로 설명해요. 우리가 어떤 제품을 사서 조립하거나 사용하기 위해 매뉴얼을 읽는 것처럼, 개발자들도 새로운 도구를 사용하려면 매뉴얼이 필요한데, 그 매뉴얼을 쓰는 사람이 테크니컬 라이터에요.UX 라이터가 좀 더 넓은 범위의 대고객을 대상으로 한 UI 텍스트를 통해 고객을 돕는다면, 테크니컬 라이터는 좀 더 좁은 범위인 개발자 고객을 대상으로 문서를 써요. 그래서 일반적으로 독립적으로 구성된 기술 문서 제품을 만들어 제공해요.테크니컬 라이터의 목표는 개발자들이 우리 회사의 도구나 기술을 더 빠르고 효과적으로 익힐 수 있게 돕는 거예요. 그래서 복잡한 기술을 명확하고 이해하기 쉬운 언어로 전달하는 게 중요해요.테크니컬 라이터의 일그런데 테크니컬 라이터가 글쓰기만 하는 건 아니에요. 우리 제품을 쓰는 개발자들에게 훌륭한 개발자 경험을 제공한다는 비전 아래 할 수 있는 모든 일을 하고, 외부에 알리는 DevRel(Developer Relations) 관련 업무도 하죠. 그래서 테크니컬 라이터가 실무에서 하는 일도 다음 그림에서 볼 수 있듯 다양해요. 리서치와 테스팅, 프로덕트 매니징 등 여러가지가 업무에 자연스럽게 녹아있어요.https://www.linkedin.com/posts/fabrizioferri_saw-a-similar-chart-for-data-scientists-activity-7087521227426140160-aDzW지금부터 토스페이먼츠의 테크니컬 라이터가 하는 일을 좀 더 자세히 소개해 볼게요.1. 연동 문서의 모든 것테크니컬 라이터의 가장 중요한 미션은 양질의 기술 문서로 개발자를 돕는 거에요. 문서의 전체 메뉴와 정보 구조를 만들고, 튜토리얼이나 상세한 가이드를 작성해요. 가이드에 들어갈 이미지를 직접 만들기도 하고, 필요하다면 예제 코드를 작성하기도 합니다. 또 새로운 기능을 설명하기 전에 직접 개발자 고객 입장에서 테스트도 하고, 간단한 코드를 짜볼 때도 있어요.토스페이먼츠 문서에는 섹션마다 피드백을 남길 수 있는 기능이 있는데요. 개발자들이 남긴 피드백과 문의에 답변하고 문서를 계속 유지보수하는 일도 중요해요. 문서 설명이 충분해도 반복해서 들어오는 문의가 있다면 제품의 문제일 수 있어서 제품 팀에 피드백을 전달하기도 하고요.토스페이먼츠 연동 문서 - 튜토리얼토스페이먼츠 연동 문서 - 용어사전토스페이먼츠 연동 문서 - 개념 설명👉 토스페이먼츠 연동 문서 - 튜토리얼2. 좋은 API 만들기토스페이먼츠에는 새로운 API(Application Programming Interface)를 만들거나 스펙을 개선할 때 API 리뷰를 거쳐요. API 리뷰는 API의 설계와 구현을 검토하는 과정으로, 개발자뿐 아니라 테크니컬 라이터도 리뷰에 참여하는데요. 일방적으로 스펙 초안을 전달받아 문서를 작성하는 게 아니라 테크니컬 라이터도 사용자 입장에서 의견을 내고 반영할 수 있는 구조죠. 테크니컬 라이터 입장에서 설명하기 어려운 스펙은 코드 스멜(code smell)이 있다고 보고 피드백 해서 개선에 기여해요. 이런 경험을 반복하면서 지금은 리뷰하는 개발자들이 먼저 ‘문서에 이 스펙을 설명한다면?’이라는 질문을 던지게 됐어요.👉 토스페이먼츠 연동 문서 - API 레퍼런스3. DevRel(Developer Relations)DevRel은 PR(Public Relations)처럼 개발자와 관계 맺고 상호작용하는 것을 뜻해요. 일종의 기술 브랜딩을 하는 것이죠. 주로 토스페이먼츠의 기술적 도전과 문제 해결, 결제 연동에 대한 기술 콘텐츠를 만들고 외부에 공유해요. 콘텐츠는 개발자와 함께 쓰기도 하고, 테크니컬 라이터가 직접 제품과 관련한 주제를 발굴해서 쓰기도 해요. 현재는 트위터를 운영하고, 콘텐츠를 Velog와 toss.tech에 발행하고 있어요. DevRel은 내부 개발자들이 동기부여 될 수 있도록 돕고, 외부 개발자 고객이 토스페이먼츠에 대해 인식할 수 있는 기회를 만들죠.4. 그 밖에 팀을 위한 다양한 일들전체 팀이나 커뮤니티 차원에서 필요한 일들도 있는데요. 먼저 토스페이먼츠 팀의 핵심 도메인인 결제 관련 용어 정리를 했어요. 모든 맥락과 정보를 한곳에 모으고 서로 다르게 쓰고 있던 용어를 정리해서 팀 전체의 커뮤니케이션 비용을 줄일 수 있었어요. 토스 커뮤니티에서는 기술 행사인 SLASH에 참여해서 연사들의 장표와 스크립트 전체 검수 작업을 하고 발표 흐름이 더 자연스러울 수 있도록 피드백을 드리는 역할을 해요. 커뮤니티 내에 테크니컬 라이팅 역량을 높이기 위해서 필요한 사람 누구나 볼 수 있는 테크니컬 라이팅 가이드도 만들었답니다.테크니컬 라이터에게 필요한 역량테크니컬 라이터가 메인으로 풀어야 하는 문제는 기술 글쓰기를 넘어 ‘개발자가 기술을 학습하고 일에 적용하는 방법을 이해하고 돕는 것’이라고 생각해요. 그래서 테크니컬 라이터 스스로가 능동적인 학습자여야 하고, 그 내용을 텍스트로 잘 녹여야 하며, 전체적인 학습 여정과 일에 적용하는 경험을 디자인할 수 있어야 한다고 생각해요. 하나씩 좀 더 자세히 설명해 볼게요.능동적으로 학습하기저는 테크니컬 라이팅의 핵심은 ‘학습 기술’이라고 생각해요. 기본적인 기술 지식, 글쓰기 역량을 가지고 있으면 물론 좋아요. 그런데 현재 가진 지식이 많은 것보다, 새로운 지식에 거부감이 없고 잘 학습해 나갈 수 있는 역량이 더 중요한 것 같아요. 테크니컬 라이터가 모든 분야의 기술적 내용을 개발자만큼 알 순 없어요. 다만 새로운 기술을 배우는 데 거부감이 없고, 개발자의 문제 해결 과정에 관심을 가지면 충분히 학습할 수 있어요. 그리고 그 내용을 더 많은 사람을 위해 양질의 콘텐츠로 작성해서 공유하며 가치를 느끼는 것도 중요해요.기술에 서사를 부여하기그렇게 학습한 지식과 정보로 글을 쓰며 일종의 서사를 만들어야 해요. 기술적인 주제라고 해도 결국 사람이 뭔가를 이해하는 방식은 자연스러운 서사에서 온다고 생각해요. 물론 그 서사가 문학 같은 형식은 아니고, 기술과 관련한 주요 서사들이 따로 있어요. 예를 들어 기술과 기능은 작든 크든 결국 어떤 문제를 해결하기 위해 존재해요. 그래서 닥친 문제에 공감하고, 해결해 나가는 과정의 서사를 잘 연결해 줄 수 있어야 해요.개발자 경험 개선하기문서화를 하고, API&SDK 인터페이스를 함께 고민하고, 가까이서 기술 지원팀의 이야기와 피드백을 듣고 문서를 개선하다 보면 결국 개발자 경험 전반을 개선하고 싶어져요. 테크니컬 라이팅은 결국 개발자의 학습과 작업 성공을 돕는 게 목표이기 때문이죠. 그래서 글 외에도 영상이나 개발자도구 제공과 같이 개발자 경험의 전체 여정을 이해하고 설계할 수 있으면 좋아요.개발자 경험이란?‘개발자 경험’에 대해 들어보신 적 있나요? 이제 친숙한 개념이 된 UX(User eXperience, 사용자 경험)처럼 DX라는 개념이 있는데요. 바로 개발자에게 가장 빠르고 쉬운 경험을 제공하는 것을 뜻해요. 개발자의 생산성이나 만족도를 위한 제품이나 경험을 만드는 것이 목표예요.테크니컬 라이터에 대해서 좀 더 이해할 수 있으셨나요? 다음 글에서는 테크니컬 라이터의 역할을 확장하면서 개발자 경험을 적극적으로 개선한 이야기로 찾아올게요.함께 읽으면 좋은 콘텐츠📍결제 연동 코드없이 시작하기📍브라우저용 번들링 플러그인, 직접 만들었어요Write 한주연 Review 박수연, 여인욱 Graphic 이은호, 이나눔재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "41": "그 많은 개발 문서는 누가 다 만들었을까 (2) 개발자의 학습을 돕는 모든 것한주연ㆍ테크니컬 라이터2024. 1. 25지난 콘텐츠에서는 테크니컬 라이터의 역할과 그동안 해왔던 일을 중점적으로 소개해 드렸어요. 이번 콘텐츠에서는 토스의 테크니컬 라이터가 개발자 경험 전반으로 역할을 확장해 온 이야기를 공유해요.개발자 경험(Developer eXperience)이란?‘개발자경험’에 대해 들어보신 적 있나요? 이제 친숙한 개념이 된 UX(User eXperience, 사용자 경험)처럼 DX라는 개념이 있는데요. 바로 개발자에게 가장 빠르고 쉬운 경험을 제공하는 것을 뜻해요. 개발자의 생산성이나 만족도를 위한 제품이나 경험을 만드는 것이 목표예요.다음 그림처럼 개발자 경험은 내부와 외부로 나눌 수도 있는데요. 저희가 다루는 개발자 경험은 토스페이먼츠의 API와 SDK를 사용하는 외부 개발자 고객의 경험을 뜻해요. 개발자 경험을 이루는 요소는 아래와 같이 다양한데요. 보통 테크니컬 라이터는 문서에 관한 개발자 경험을 개선해요.개발자의 학습을 돕는 모든 것하지만 토스페이먼츠 테크니컬 라이터의 역할은 기술 문서와 기술 콘텐츠를 작성하는 것에 그치지 않아요. 지난 번 소개한 테크니컬 라이터의 역량 중 개발자 경험의 전체 여정을 이해하고 설계하는 것이 있었는데요. 이 부분을 좀 더 적극적으로 해보고 싶었어요. 그래서 저희는 테크니컬 라이터의 목표를 ’개발자의 학습을 돕는 모든 것을 하기’로 정의했어요. 개발자가 빠르게 제품을 학습하면 개발도 빨라져요.그래서 DX(Developer eXperience) 팀이 생겼고, 개발자 경험을 위한 제품을 기획하고 만들기로 했어요. 목표는 ‘토스페이먼츠 개발자센터에 진입한 개발자의 첫 번째 경험을 개선하는 것’이었어요. 개발자의 첫 번째 경험은 새로운 도구의 사용법을 익히고 테스트 해 보는 과정, 말 그대로 ‘학습’이기 때문이에요. 개발자가 API나 SDK 혹은 개발자 도구를 처음 접할 때 그 경험은 완전해야 해요. 당장 일을 하고 목표 달성을 해야 하는 입장에서 잘못된 문서를 뒤지거나 사용성이 좋지 않은 도구를 파 볼 시간은 없기 때문이죠.개발자 고객 이해하기어떤 첫 번째 경험을 제공할지 알아내기 위해 먼저 개발자의 학습 방식, 일 하는 방식을 이해하려고 노력했어요. 여러 번의 UT(Usability Test, 사용성 테스트), 유저 인터뷰, 리서치를 거치며 질문이 생겼어요. 연동을 막 시작하려는 개발자에게 문서부터 탐색하게 하는 게 좋을까? 혹은 개발 없이 바로 실행해 보는 경험이 더 좋을까? 저희는 개발자가 제한된 시간에 빠르게 문서를 탐색하고 개발을 바로 할 수 있도록 도와야 한다고 생각했어요. 테크니컬 라이터로서 조금 아이러니 하지만, 그 시작을 빠르게 하는 건 읽기 방식 하나로는 부족해 보였어요. 이전에는 ‘문서의 모든 내용을 숙지하고 시작하는 개발자’를 메인 페르소나로 가정하고 모든 정보를 제공하는 라이팅 측면에 집중했다면, ‘코드부터 보는 개발자’를 위한 무언가가 필요하다고 생각했어요.실제로 관련 논문도 있더라구요. 처음부터 문서를 아주 꼼꼼하게 본 뒤 개발을 시작하는 개발자가 있는 반면, 바로 코드를 보고 적용하고 부딪치면서 해결하는 개발자도 있었죠. 그렇다면 두 페르소나를 모두 만족시켜야 한다고 생각했어요. 보통은 두 가지 방법을 결합한 개발자들이 대부분일 거기도 하고요.토스페이먼츠의 DX 제품 소개그래서 저희는 브라우저에서 바로 시작할 수 있는 코드 에디터와 인터렉티브한 문서 두 가지를 만들었는데요, 자세히 소개해 볼게요.브라우저에서 바로 시작하는 개발샌드박스는 브라우저에서 바로 연동 흐름을 체험하고, 테스트 연동을 해볼 수 있는 개발자 도구예요. GitHub 레포지토리를 클론하거나 VS Code를 띄울 필요 없이 브라우저에서 바로 코딩하고 실행할 수 있는 코드 에디터라고 할 수 있어요.문제개발 없이 제품을 체험해 볼 수 없어요.개발자마다 로컬 환경이 각자 달라서 GitHub 코드 샘플 실행에 실패해요.해결처음부터 끝까지 쉽게 테스트 하고 연동할 수 있는 환경을 브라우저에서 제공해요.연동을 위한 준비를 0으로 만들어요.제품 데모를 제공해요.결과로그인 한 유저의 결제 요청량이 70% 상승했어요.공개키 테스트 양이 일평균 약 200회 이상 증가했어요.튜토리얼로 인입되는 유저가 약 5배 증가했어요.고객 피드백“문서에는 예제 코드만 있다보니 로컬에 따로 파일을 만드는 과정이 필요한데 그 과정이 생략되어 좋아요.”“로컬에서 개발하려면 코드 화면과 문서 화면을 왔다갔다 해야 하는데 샌드박스에서는 이미 파일이 구축되어 있어서 편해요.”“서버 로그도 남아서 좋아요.”샌드박스를 활용하면 코드와 결과물이 어떻게 서로 연결되어 있는지 한눈에 볼 수 있어 편리해요. 개발자센터에 진입해서 바로 코드를 통해 직관적으로 결제 연동을 이해하고 싶은 개발자가 타겟이죠. 그치만 개발자가 아니어도 전체 결제 과정을 체험할 수 있어요.👉 샌드박스 체험하기텍스트, 예제 코드, 미리보기가 연동되는 문서기존 튜토리얼 문서는 줄글과 예제 코드가 선형적으로 보여지고, 각 연동 단계의 결과물이 어떤 지 알 수 없었어요. 정보가 정적으로 전달되었기 때문에 결국 개발자는 한 쪽에 코드 에디터를 켜고 한 단계씩 따라서 구현을 해야만 결과물을 볼 수 있었어요. 이런 경험을 개선하기 위해 튜토리얼을 인터렉티브하게 바꿨어요. 코드와 코드에 대한 설명, 결과물이 모두 연동되어 보여지기 때문에 끝그림을 미리 그려볼 수 있어요.문제개발 없이 제품의 끝그림을 알 수 없어요.텍스트만으로는 실제 적용하는 방법을 이해하기 어려워요.예제 코드만으로는 연동해야 하는 내용을 이해할 수 없어요.튜토리얼에 있는 예제 코드는 전체 프로젝트 코드가 아니라 직접 예제 코드를 만들어야 해서 시간이 많이 소요돼요.해결텍스트, 예제 코드, 미리보기가 완전히 연동되는 문서를 제공해요.원하는 언어 조합에 따라 코드 다운로드를 해서 바로 개발할 수 있어요.하나의 문서로 개발자의 결제 흐름, 클라이언트•서버 프로젝트 구조, 트러블슈팅 과정을 모두 보여줘요.결과튜토리얼 전체 사용자가 11% 증가했어요.튜토리얼을 90% 이상 스크롤한 사용자는 65%로 증가했어요.고객 피드백“코드와 설명이 연동되어서 코드와 매칭해 내용을 파악하기 쉬워요.”“코드를 직접 에디터에 넣어보면서 어떻게 화면에 그려지는지 확인할 필요가 없어서 좋아요.”“코드와 문서, 프리뷰 화면이 동적으로 바뀌니까 이해하는데 도움이 돼요.”👉 튜토리얼 보러가기개발자 경험을 개선하면서 개발자의 학습 및 개발 경험에 적극적으로 개입하고 ‘살아있는 개발자 제품’을 만드는 역할을 테크니컬 라이터가 충분히 할 수 있다는 생각이 들었어요. 앞으로도 토스페이먼츠 테크니컬 라이터들은 양질의 기술 글쓰기 뿐만 아니라 개발자 경험까지 계속 챙겨볼게요.Write 한주연 Review 박수연, 여인욱 Graphic 이은호, 이나눔재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "42": "GitHub Actions로 개선하는 코드 리뷰 문화김성일ㆍ토스페이먼츠 서버 개발자2024. 2. 7토스페이먼츠는 결제의 다양한 맥락을 다루는 회사이기 때문에 코드 변경과 배포가 신중하게 이루어져야 하는데요. 이런 상황에서 코드 리뷰는 서비스의 안정성을 높일 수 있는 효과적인 방법 중 하나죠. 제가 속한 온보딩 플랫폼 팀에서도 코드 리뷰를 진행하고 있지만, 보다 면밀한 리뷰와 피드백 공유가 필요한 상황이었어요.저는 코드 리뷰 문화의 가장 중요한 조건은 피드백의 속도와 PR 코멘트로 이루어지는 대화의 양, 두 가지라고 생각해요. 물론 리뷰의 질도 중요하지만, 리뷰 자체가 잘 이루어지려면 피드백 루프가 빠르게 돌아가는 것이 중요해요. 그래서 저는 피드백 루프가 시작되는 첫 단계인 ‘리뷰어 할당’부터 개선했어요. ‘리뷰어가 할당되면 피드백 루프가 빨라질 것이다’라는 가설을 세우고, 리뷰어를 자동으로 할당하는 자동화를 진행한 것이죠.리뷰어 할당 자동화는 몇 가지 이유로 GitHub Actions로 개발했는데요. 먼저 GitHub Actions는 기능 구현과 유지보수가 간편하고 사용자가 많아서 접근성이 좋아요. 그리고 개발자들의 워크플로우가 GitHub Actions에서 제공하는 이벤트와 밀접하게 얽혀있어서 기능을 붙이기도 쉽고요.물론 GitHub에서 리뷰어 자동 할당 기능을 기본으로 제공하고 있지만 커스터마이즈를 못한다는 단점이 있어요. 그래서 쉽게 커스터마이징할 수 있는 GitHub Actions로 자동화 기능을 직접 개발했어요.GitHub Actions는 GitHub에서 제공하는 자동화 툴인데요. 특정 이벤트를 기반으로 워크플로우를 실행할 수 있게 해줘요. 많은 팀에서 GitHub Actions를 CI/CD에서만 사용하고 있지만, 사실 이벤트만 적절히 선택하면 그 이벤트 기반으로 많은 것을 자동화할 수 있어요.1. 리뷰어 지정하기토스페이먼츠는 내부 워크플로우를 주로 타입스크립트로 개발하는데요. 유지보수의 편의성을 위해 이 워크플로우도 타입스크립트로 개발했어요. 워크플로우의 로직은 아주 간단해요. 정해진 리뷰어 목록에서 PR 생성자를 제외하고 랜덤으로 리뷰어를 선정해요. 그런 뒤, GitHub에서 제공하는 toolkit이라는 SDK를 통해 해당 리뷰어를 할당해요. 아주 간단한 로직이죠.async function main() {\n  const selectedReviewer = selectRandomReviewer();\n\n  await githubClient.rest.pulls.requestReviewers({\n    owner: github.context.repo.owner,\n    repo: github.context.repo.repo,\n    pull_number: github.context.issue.number,\n    reviewers: [selectedReviewer.githubName]\n  });\n}\n\nfunction selectRandomReviewer() {\n  const prCreator = github.context.payload.pull_request.user.login;\n  const candidateReviewer = getCandidates().filter(\n    (person) => person.githubName !== prCreator\n  );\n\n  return candidateReviewer[\n    Math.floor(Math.random() * candidateReviewer.length)\n  ];\n}리뷰어가 될 수 있는 후보는 따로 정의한 yaml 파일에서 읽어오도록 설정했어요. yaml 파일은 리뷰어가 될 사용자들의 GitHub username과 Slack userId 쌍의 간단한 구조로 이루어져있어요. 새로운 리뷰어가 추가된다면, 코드 수정 없이 yaml 파일만 수정하면 돼요. 이 파일에 정의된 리뷰어 목록을 기반으로 PR이 생성될 때마다 워크플로우는 생성자 본인을 제외한 리뷰어 한 명을 지정해요. reviewers:\n  - githubName: 김토스\n    slackUserId: toss-slack-user-id이 워크플로우를 적용한 뒤 평균적으로 PR 리뷰가 되는 시간이 하루 내외로 짧아졌고, 코드 리뷰 자체에 더 익숙해지는 팀원도 늘어났어요. 실제로 워크플로우가 추가되기 전에 비해, 코드 리뷰의 코멘트 수도 평균적으로 2배 이상 많아졌어요.또, 온보딩 플랫폼은 ‘계약’이라는 넓은 도메인을 다루고 있어서 개발팀 중에서도 특히나 많은 맥락을 가지고 있는데요. 이렇게 리뷰어를 무작위로 할당한 후에는 자신이 개발하지 않은 기능도 리뷰하게 되어서 팀 내에서 한두 명만이 알고 있었던 맥락을 점차 줄여나갈 수 있었어요. 이러한 것이 도움을 주어서인지, 온보딩 플랫폼의 운영 이슈 대응 시간은 작년에 비해 약 90% 정도 감소했어요. 회사 내에서 좋은 사례가 되어 다른 팀도 같은 워크플로우를 도입하기도 했고요.2. 리뷰어에게 알림 보내기물론 개선이 필요한 부분도 있었어요. 처음에는 본인이 리뷰어로 지정된 걸 인지하기 어려웠어요. 리뷰어가 되면 GitHub이 메일로 알림을 보내는데요. 이메일을 자주 확인하지 않으면 본인이 리뷰어인 것을 알기 어려웠죠. 그래서 슬랙봇으로 리뷰어가 되었다는 메시지를 보내는 기능을 추가했어요.앞서 살펴본 yaml 파일에 이미 리뷰어의 GitHub username과 Slack userId 정보가 있었기 때문에, slackClient를 호출하는 코드만 간단히 추가해서 문제를 해결했어요. 실제 코드는 아래와 같아요.async function main() {\n  const selectedReviewer = selectRandomReviewer();\n\n  await githubClient.rest.pulls.requestReviewers({\n    owner: github.context.repo.owner,\n    repo: github.context.repo.repo,\n    pull_number: github.context.issue.number,\n    reviewers: [selectedReviewer.githubName]\n  });\n\n  sendDirectMessage(selectReviewer);\n}\n\nasync function sendDirectMessage(reviewer: IReviewer) {\n  slackClient.chat.postMessage({\n    text: createMessage(github.context),\n    channel: reviewer.slackUserId\n  });\n}간단한 코드 작업으로 리뷰어 할당 알림을 훨씬 효과적으로 전달했어요. 실제로 팀원들이 리뷰를 시작하는 시점이 빨라진 것도 체감할 수 있었어요.3. 오늘 안에 리뷰할 수 있게 하기리뷰 기간이 다소 지연되기도 했는데요. 여러 이유로 리뷰어가 리뷰 요청을 받은 당일에 확인하지 않으면, PR을 다시 안 보고 리뷰가 누락될 가능성이 높아졌어요. 그래서 구두로 다시 PR 리뷰를 요청하는 상황이 자주 발생하기도 했어요.그래서 리뷰되지 않은 PR을 다시 알려주는 리마인드 워크플로우를 개발했어요. 이 워크플로우는 평일 오후 2시에 팀 채널로 전달돼요. 쉽게 인지할 수 있도록 점심시간이 끝나고 업무를 시작하는 타이밍에 알려주는 것이죠.GitHub Actions는 PR에 대한 이벤트 외에도 다양한 이벤트 기능을 제공해요. 그 중 cron 표현식을 이용해서 반복적으로 워크플로우를 스케줄링할 수 있는 이벤트가 있는데요. 아래와 같은 cron 표현식을 사용하면, 월요일부터 금요일까지 매주 오후 2시에 정의한 워크플로우가 동작해요.name: review-remind\n\non:\n  schedule:\n    - cron: \"0 14 * * 1-5\"이 스케줄 이벤트 덕분에 실제 코드 구현도 정말 간단해져요. 단순하게 특정 레포지토리에 Open 상태로 되어있는 PR이 리뷰가 되어있는지 체크하고, 리뷰가 되어있지 않은 PR 정보를 모아 팀 슬랙 채널로 전송하기만 하면 돼요.async function main() {\n  const owner = process.env.GITHUB_REPOSITORY?.split(\"/\")[0];\n  const repo = process.env.GITHUB_REPOSITORY?.split(\"/\")[1];\n\n  const { data: pullRequests } = await githubClient.rest.pulls.list({\n    owner,\n    repo,\n    state: \"open\",\n    per_page: 100,\n    sort: \"updated\",\n    direction: \"desc\"\n  });\n\n  const messages = await collectMessages(pullRequests);\n  sendMessage(messages);\n}\n\nasync function collectMessages(pullRequests: IPullRequest[]): IMessage[] {\n  return pullRequests\n    .flatMap((pr) => {\n      if (isDraft(pr) || isAlreadyReviewed(pr)) return [];\n      else [constructMessage(pr)];\n    })\n}\n\nasync function sendMessage(messages: IMessage[]) {\n  if (messages.length === 0) return;\n\n  const threadStartMessage = await slackClient.chat.postMessage({\n    text: \"리뷰해주세요!\",\n    channel: process.env.SLACK_CAHNNEL_ID\n  });\n\n  Promise.all(\n    messages.map((message) => {\n      slackClient.chat.postMessage({\n        text: message.text,\n        channel: process.env.SLACK_CAHNNEL_ID,\n        thread_ts = threadStartMessage.ts\n      });\n    })\n  );\n}실제로 위 워크플로우가 동작하고 나면, 팀 슬랙 채널에는 오후 2시에 \"리뷰해주세요!\" 스레드가 만들어지고 아래와 같이 리뷰가 필요한 PR의 목록들이 스레드의 댓글로 달리면서 리뷰어가 멘션 돼요. 이렇게 개선한 뒤, 팀원들이 밀린 코드 리뷰를 잊는 일이 점점 줄었어요.번외: 공휴일에는 리뷰 건너뛰기추석 전날 10분 만에 이루어진 개선을 하나 소개해 드릴게요. PR 리마인드 워크플로우가 cron 표현식 기반이라 주말에는 동작하지 않지만, 공휴일에는 동작하는 문제도 있었는데요. 이 문제도 아주 간단하게 해결했어요. 토스페이먼츠는 PG회사이기 때문에 매일 정산을 해요. 정산할 때 공휴일 정보는 매우 중요해서 오늘이 공휴일인지 확인할 수 있는 유틸리티가 내부에 이미 있었어요.공휴일 유틸리티로 당일의 공휴일 여부를 확인하고, 공휴일이면 알림을 보내지 않도록 했어요. 코드는 아래와 같이 수정했어요.async function main() {\n  if (isHoliday(new Date())) return;\n\n  const owner = process.env.GITHUB_REPOSITORY?.split(\"/\")[0];\n  const repo = process.env.GITHUB_REPOSITORY?.split(\"/\")[1];\n\n  const { data: pullRequests } = await githubClient.rest.pulls.list({\n    owner,\n    repo,\n    state: \"open\",\n    per_page: 100,\n    sort: \"updated\",\n    direction: \"desc\",\n    baseUrl: process.env.GITHUB_BASE_URI,\n  });\n\n  const messages = await collectMessages(pullRequests);\n  sendMessage(messages);\n}\n\nfunction isHoliday(date: Date) {\n  const formattedDate = formatDate(date);\n  return utility.isHoliday(formattedDate);\n}isHoliday() 함수에서 date와 같은 값을 내부에서 초기화하는 방식으로도 사용할 수 있지만, 그렇게 되면 의존성에 의해 테스트하기 어려운 코드가 돼요. date와 같은 객체를 내부에서 생성하는 것이 아닌 외부 파라미터로 변경하게 되면 테스트하기 좋은 코드가 된답니다.이렇게 간단한 개선을 통해, 공휴일에는 리뷰 리마인드를 건너뛰게 됐어요.마치며지금까지 자동화를 통해 코드 리뷰 문화를 개선하고 편의를 높였던 경험을 공유했어요. 이렇게 팀 내부의 업무 효율화를 위한 문제를 해결했을 때 받는 긍정적인 피드백은 큰 원동력이 되는 것 같아요. 팀원들의 시간을 줄여주고 효율화하는 작업에 의미를 느끼는 분이라면 한 번 도전해 보세요.Write 김성일 Edit 한주연, 여인욱 Graphic 이나눔, 이은호재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기"
}