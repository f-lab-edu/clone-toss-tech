{
    "1": "토스 프론트엔드 챕터를 소개합니다!박서진ㆍFrontend Developer2021. 4. 28안녕하세요. 토스 프론트엔드 챕터 블로그에 오신 것을 환영합니다. 앞으로 토스팀에서 프론트엔드 개발을 하면서 생기는 이야기들을 정리하여 블로그로 전해드리려고 합니다.토스 프론트엔드 챕터는?토스에서는 같은 일을 하는 사람들을 모아서 챕터라고 합니다. 프론트엔드 챕터는 JavaScript, HTML, CSS를 이용하여 웹 프론트엔드 제품을 만드는 직군을 가리키는 말입니다.토스의 프론트엔드 개발자들은 ‘사일로’라고 하는 목적 조직에서 각자 독립적으로 일하고 있습니다. 각각의 사일로는 토스에서 하나의 제품을 전담하며, 작은 스타트업처럼 독립적으로 일합니다. 예를 들어, 송금을 담당하는 사일로는 송금 사일로, 결제를 담당하는 사일로는 결제 사일로입니다.사일로는 기본적으로 프로덕트 오너, 디자이너, 서버 개발자, 프론트엔드 개발자, 데이터 분석가 1명씩으로 구성됩니다. 사일로에서 프론트엔드 개발자는 다른 직군의 동료들과 가까운 자리에서 밀접하게 협업하면서, 주어진 반기별 OKR(목표)를 달성하기 위해 꾸준히 제품 개선에 참여하고 있습니다.→ 토스의 첫인상을 책임지는 사람들, 프론트엔드 개발자를 만나다함께 협업하기각 사일로에서 독립적으로 일하는 프론트엔드 개발자들이지만, 하나의 팀처럼 같이 협업하고 있습니다. 예를 들어서,함께 라이브러리를 개발하고 있습니다. UI 컴포넌트, 비동기 처리와 같이 프론트엔드 개발을 하면서 꼭 필요한 것들이 모두 라이브러리화되어 있습니다. 서비스를 개발하다가 적절히 추상화된 코드 조각이 생기면 라이브러리에 꾸준히 반영하고 있습니다.서로 코드를 리뷰합니다. 라이브러리 개발과 서비스 개발 모두에서 코드 리뷰가 의무화되어 있습니다. 코드 리뷰를 주고받으면서 더 나은 설계에 대해 토론하고, 새로운 기술 채택에 대해 의견을 나누고 있습니다.챕터 위클리에 참여합니다. 주기적으로 모든 프론트엔드 개발자들이 모여서 그동안 있었던 개발 업무를 주제로 이야기합니다. 챕터 위클리에서는 돌아가면서 “Tech Talk” 라고 하는 작은 발표를 열고 있는데, 개발 노하우나 새로운 기술 토픽을 소개합니다.그 이외에도 비정기적으로 기술 스터디를 열기도 합니다. 서로 자리가 떨어져 있어도 슬랙 채널이나 각자의 자리에서 활발하게 소통합니다.프론트엔드 챕터가 사용하는 기술React, TypeScript, Next.js가 토스 프론트엔드 챕터가 다루는 기술의 핵심을 이루고 있습니다.React: 토스 웹 페이지는 대부분 React로 구성되어 있습니다. 제품의 성격에 따라 React Suspense와 같은 실험적인 기술을 적극적으로 도입하고는 합니다.TypeScript: 토스에서는 모든 코드를 TypeScript로 작성합니다. 사람의 실수를 줄이고, 빌드 시간에 미리 오류를 찾아냄으로써 웹 서비스를 안정적으로 운영하는 데에 큰 도움이 되고 있습니다.Next.js: 토스 사용자들에게 더 빠른 웹을 보여주기 위해 적극적으로 SSR와 Static Export를 활용하고 있습니다.그 이외에 아래 기술을 보조적으로 사용하고 있어요.Yarn Berry: 토스에서 의존성을 관리하기 위해 사용하는 패키지 매니저입니다. NPM보다 엄격히 package.json을 관리함으로써 개발자의 실수를 더 빨리 발견할 수 있도록 해 줍니다. NPM보다 모듈을 로드하는 속도가 빠릅니다. (홈페이지)Emotion: CSS를 다루기 위해 emotion 라이브러리를 사용하고 있습니다. CSS Prop으로 생산적으로 스타일을 다룰 수 있습니다. 서버 사이드 렌더링을 했을 때 첫 렌더에 포함되는 Critical CSS만을 HTML에 포함해줌으로써 더 빠르게 화면을 보여줄 수 있도록 도와주기도 합니다.React Query, SWR: 비동기를 다루는 상황에서 대부분 사용하고 있는 라이브러리입니다. 선언적으로 비동기 자원을 관리하고 캐싱할 수 있습니다.Tech Talks토스에서는 모든 프론트엔드 챕터 구성원이 모이는 챕터 위클리에서 Tech Talk라고 하는 작은 발표를 열고 있습니다. 발표의 주제는 서비스 개발을 할 때의 꿀팁부터 React Suspense 소개까지 다양합니다. 2019년부터 지금까지 총 90개 이상의 크고 작은 발표가 있었습니다.토스 FE 블로그의 아티클은 위클리에서 있었던 Tech Talk의 내용을 간단히 정리한 것입니다.기술 블로그에서 공개된 내용보다 더 좋은 발표를 듣고 싶다면, 그리고 저희와 함께 웹 서비스의 문제를 풀어가고 싶다면, 언제든 토스팀에 지원해주세요. 모든 단계에서 최대한 빠르고 솔직하게 피드백을 드리겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "2": "웹 서비스 캐시 똑똑하게 다루기박서진ㆍFrontend Developer2021. 4. 29토스 프론트엔드 챕터에서는 웹 성능을 최대한으로 높이기 위해 HTTP 캐시를 적극적으로 사용하고 있습니다. 캐시를 잘못 관리했을 때, 원하는 시점에 캐시가 사라지지 않을 수 있습니다. 필요 이상으로 HTTP 요청이 발생하기도 합니다.HTTP 캐시를 효율적으로 관리하려면 Cache-Control 헤더를 섬세하게 조절해야 합니다. 토스 프론트엔드 챕터에서 다양한 생명 주기를 가지는 캐시를 다루면서 알게 된 노하우를 테크 블로그로 공유합니다.캐시의 생명 주기HTTP에서 리소스(Resource)란 웹 브라우저가 HTTP 요청으로 가져올 수 있는 모든 종류의 파일을 말합니다. 대표적으로 HTML, CSS, JS, 이미지, 비디오 파일 등이 리소스에 해당합니다.웹 브라우저가 서버에서 지금까지 요청한 적이 없는 리소스를 가져오려고 할 때, 서버와 브라우저는 완전한 HTTP 요청/응답을 주고받습니다. HTTP 요청도 완전하고, 응답도 완전합니다. 이후 HTTP 응답에 포함된 Cache-Control 헤더에 따라 받은 리소스의 생명 주기가 결정됩니다.캐시의 유효 기간: max-age서버의 Cache-Control 헤더의 값으로 max-age=<seconds> 값을 지정하면, 이 리소스의 캐시가 유효한 시간은 <seconds> 초가 됩니다.캐시의 유효 기간이 지나기 전한 번 받아온 리소스의 유효 기간이 지나기 전이라면, 브라우저는 서버에 요청을 보내지 않고 디스크 또는 메모리에서만 캐시를 읽어와 계속 사용합니다.메모리 캐시에서 불러온 HTTP 리소스예를 들어, 위 개발자 도구 캡처와 같이 어떤 JavaScript 파일을 요청하는 경우를 가정합시다. 이 리소스가 가지는 Cache-Control 헤더 값은 max-age=31536000 이기 때문에, 이 리소스는 1년(31,536,000초)동안 캐시할 수 있습니다.스크린샷에서는 유효한 캐시가 메모리에 남아 있기 때문에 (from memory cache) 라고 표기된 것을 확인할 수 있습니다.“서버에 요청을 보내지 않고” 라고 하는 말에 주의합시다. 한번 브라우저에 캐시가 저장되면 만료될 때까지 캐시는 계속 브라우저에 남아 있게 됩니다. 때문에 CDN Invalidation을 포함한 서버의 어떤 작업이 있어도 브라우저의 유효한 캐시를 지우기는 어렵습니다.Note: Cache-Control max-age 값 대신 Expires 헤더로 캐시 만료 시간을 정확히 지정할 수도 있습니다.캐시의 유효 기간이 지난 이후: 재검증그렇다면 캐시의 유효 기간이 지나면 캐시가 완전히 사라지게 될까요? 그렇지는 않습니다. 대신 브라우저는 서버에 조건부 요청(Conditional request)을 통해 캐시가 유효한지 재검증(Revalidation)을 수행합니다.재검증 결과 브라우저가 가지고 있는 캐시가 유효하다면, 서버는 [304 Not Modified] 요청을 내려줍니다. [304 Not Modified] 응답은 HTTP 본문을 포함하지 않기 때문에 매우 빠르게 내려받을 수 있습니다. 예를 들어, 위 스크린샷을 살펴보면 59.1KB 리소스의 캐시 검증을 위해 324바이트만의 네트워크 송수신만을 주고받았음을 볼 수 있습니다.If-None-Match와 If-Modified-Since가 포함된 요청대표적인 재검증 요청 헤더들로는 아래와 같은 헤더가 있습니다.If-None-Match: 캐시된 리소스의 ETag 값과 현재 서버 리소스의 ETag 값이 같은지 확인합니다.If-Modified-Since: 캐시된 리소스의 Last-Modified 값 이후에 서버 리소스가 수정되었는지 확인합니다.위의 ETag 와 Last-Modified 값은 기존에 받았던 리소스의 응답 헤더에 있는 값을 사용합니다.재검증 결과 캐시가 유효하지 않으면, 서버는 [200 OK] 또는 적합한 상태 코드를 본문과 함께 내려줍니다. 추가로 HTTP 요청을 보낼 필요 없이 바로 최신 값을 내려받을 수 있기 때문에 매우 효율적이죠. 😉max-age=0 주의보 정의대로라면 max-age=0 값이 Cache-Control 헤더로 설정되었을 때, 매번 리소스를 요청할 때마다 서버에 재검증 요청을 보내야 할 것입니다. 그렇지만 일부 모바일 브라우저의 경우 웹 브라우저를 껐다 켜기 전까지 리소스가 만료되지 않도록 하는 경우가 있습니다. 네트워크 요청을 아끼고 사용자에게 빠른 웹 경험을 제공하기 위해서라고 합니다.이 경우에는 웹 브라우저를 껐다 켜거나, 아래에서 소개할 no-store 값을 사용해주세요.no-cache와 no-storeCache-Control에서 가장 헷갈리는 두 가지 값이 있다면 바로 no-cache 와 no-store 입니다. 이름은 비슷하지만 두 값의 동작은 매우 다릅니다.no-cache 값은 대부분의 브라우저에서 max-age=0 과 동일한 뜻을 가집니다. 즉, 캐시는 저장하지만 사용하려고 할 때마다 서버에 재검증 요청을 보내야 합니다.no-store 값은 캐시를 절대로 해서는 안 되는 리소스일 때 사용합니다. 캐시를 만들어서 저장조차 하지 말라는 가장 강력한 Cache-Control 값입니다. no-store를 사용하면 브라우저는 어떤 경우에도 캐시 저장소에 해당 리소스를 저장하지 않습니다.캐시의 위치CDN과 같은 중간 서버를 사용할 때, 캐시는 여러 곳에 생길 수 있습니다. 서버가 가지고 있는 원래 응답을 CDN이 캐시합니다. CDN의 캐시된 응답은 사용자 브라우저가 다시 가져와서 캐시합니다. 이처럼 HTTP 캐시는 여러 레이어에 저장될 수 있기 때문에 세심히 다루어야 합니다.CDN Invalidation일반적으로 캐시를 없애기 위해서 “CDN Invalidation”을 수행한다고 이야기합니다. CDN Invalidation은 위 다이어그램에서 가운데에 위치하는 CDN에 저장되어 있는 캐시를 삭제한다는 뜻입니다. 브라우저의 캐시는 다른 곳에 위치하기 때문에 CDN 캐시를 삭제한다고 해서 브라우저 캐시가 삭제되지는 않습니다.경우에 따라 중간 서버나 CDN이 여러 개 있는 경우도 발생하는데, 이 경우 전체 캐시를 날리려면 중간 서버 각각에 대해서 캐시를 삭제해야 합니다.이렇게 한번 저장된 캐시는 지우기 어렵기 때문에 Cache-Control의 max-age 값은 신중히 설정하여야 합니다.Cache-Control: public과 privateCDN과 같은 중간 서버가 특정 리소스를 캐시할 수 있는지 여부를 지정하기 위해 Cache-Control 헤더 값으로 public 또는 private을 추가할 수 있습니다.public은 모든 사람과 중간 서버가 캐시를 저장할 수 있음을 나타내고, private은 가장 끝의 사용자 브라우저만 캐시를 저장할 수 있음을 나타냅니다.기존과 max-age 값과 조합하려면 Cache-Control: public, max-age=86400 과 같이 콤마로 연결할 수 있습니다.s-maxage중간 서버에서만 적용되는 max-age 값을 설정하기 위해 s-maxage 값을 사용할 수 있습니다.예를 들어, Cache-Control 값을 s-maxage=31536000, max-age=0 과 같이 설정하면 CDN에서는 1년동안 캐시되지만 브라우저에서는 매번 재검증 요청을 보내도록 설정할 수 있습니다.토스에서의 Cache-Control토스 프론트엔드 챕터는 리소스의 성격에 따라 세심히 Cache-Control 헤더 값을 조절하고 있습니다.HTML 파일일반적으로 https://service.toss.im/toss-card/introduction 과 같은 HTML 리소스는 새로 배포가 이루어질 때마다 값이 바뀔 수 있습니다. 때문에 브라우저는 항상 HTML 파일을 불러올 때 새로운 배포가 있는지 확인해야 합니다.이런 리소스에 대해 토스 프론트엔드 챕터는 Cache-Control 값으로 max-age=0, s-maxage=31536000 을 설정했습니다. 이로써 브라우저는 HTML 파일을 가져올 때마다 서버에 재검증 요청을 보내고, 그 사이에 배포가 있었다면 새로운 HTML 파일을 내려받습니다.CDN은 계속해서 HTML 파일에 대한 캐시를 가지고 있도록 했습니다. 대신 배포가 이루어질 때마다 CDN Invalidation을 발생시켜 CDN이 서버로부터 새로운 HTML 파일들을 받아오도록 설정했습니다.JS, CSS 파일JavaScript나 CSS 파일은 프론트엔드 웹 서비스를 빌드할 때마다 새로 생깁니다. 토스 프론트엔드 챕터는 임의의 버전 번호를 URL 앞부분에 붙여서 빌드 결과물마다 고유한 URL을 가지도록 설정하고 있습니다.고유 버전 번호가 붙어 있는 JavaScript 파일이렇게 JS, CSS 파일을 관리했을 때, 같은 URL에 대해 내용이 바뀔 수 있는 경우는 없습니다. 내용이 바뀔 여지가 없으므로 리소스의 캐시가 만료될 일도 없습니다.이런 리소스에 대해 토스 프론트엔드 챕터는 Cache-Control 값으로 max-age의 최대치인 max-age=31536000 을 설정하고 있습니다. 이로써 새로 배포가 일어나지 않는 한, 브라우저는 캐시에 저장된 JavaScript 파일을 계속 사용합니다.캐시 설정을 섬세히 제어함으로써 사용자는 더 빠르게 HTTP 리소스를 로드할 수 있고, 개발자는 트래픽 비용을 절감할 수 있습니다. 위에서 Cache-Control와 ETag 헤더를 리소스의 성격에 따라 잘 설정하는 것만으로 캐시를 정확하게 설정할 수 있다는 것을 살펴보았습니다. HTTP 캐시로 고민하고 있는 분들께 도움이 되었기를 기대합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "3": "JSCodeShift로 기술 부채 청산하기박지우ㆍFrontend Developer2021. 5. 4토스 프론트엔드 챕터에서는 100개 이상의 서비스들이 작은 패키지 단위로 쪼개져 활발하게 개발되고 있는데요. 공통으로 사용하는 라이브러리에서 인터페이스가 변경되는 Breaking Change가 발생하면, 의존하고 있는 모든 서비스의 코드를 수정해야 했습니다. 관리하는 코드베이스가 점점 커지면서 해야 하는 작업의 양도 계속 늘어나고는 했습니다.이에 프론트엔드 챕터는 JSCodeShift를 도입하여 대부분의 코드 수정 작업을 자동화할 수 있었습니다. 토스팀이 JSCodeShift를 도입하면서 알게 된 점과 노하우를 테크 블로그로 공유합니다.JSCodeShift란?JSCodeShift는 Facebook이 만든 JavaScript/TypeScript 코드 수정 도구입니다. JSCodeShift를 통해 코드를 수정하는 코드를 작성할 수 있습니다.찾아 바꾸기와의 비교JSCodeShift를 도입하기 전, 토스에서는 대량의 코드 수정이 필요할 때면 IDE의 찾아 바꾸기(Find & Replace)를 사용했습니다. 그러나 찾아 바꾸기로는 안전하게 코드를 수정하는 데에 한계가 많았습니다.예시 1: console.log() 모두 삭제하기프로젝트 전체에 있는 console.log() 호출을 모두 제거하고 싶은 상황을 생각해봅시다. 간단한 예제임에도 쉽게 고칠 수 없는 엣지 케이스들이 발생합니다. 우선 console.log 안에 들어가는 인자의 내용이 달라질 수 있습니다. console.log에 여러 인자를 넘겨서 함수 호출이 여러 줄에 걸칠 수도 있습니다.이것을 정규식을 이용하여 어느 정도 해결할 수도 있습니다. 그러나 다양한 엣지케이스에 대응하기 위해서 정규식이 점점 복잡해지는 경우가 발생했습니다. 또 정규식은 정규 언어이기 때문에 기술적으로 대응할 수 없는 경우도 존재했습니다.예시 2: default import된 객체의 프로퍼티 수정하기아래와 같은 코드가 있었다고 생각해봅시다.import A from '@tossteam/a';\n\nA.foo();어느 순간 A.foo() 함수가 A.bar() 함수로 이름이 변경되었다고 가정해봅시다.Default import의 변수 이름은 사용하는 사람마다 임의로 정할 수 있기 때문에, 어떤 사람은 이 라이브러리를 B 라고 하는 이름으로 사용하고 있을 수도 있습니다. 때문에 이 라이브러리를 B.foo() 처럼 사용하고 있던 코드가 있었다면, B.bar() 로 수정해주어야 합니다.이런 경우는 찾아 바꾸기로 쉽게 대응하기 어렵습니다.JSCodeShift 기초JSCodeShift는 추상 구문 트리(AST, Abstract Syntax Tree)를 이용하여 코드를 수정하는 방법을 제공함으로써 코드 수정 작업을 정확하고 편리하게 할 수 있도록 도와줍니다.추상 구문 트리 (AST)추상 구문 트리는 프로그램의 소스 코드를 쉽게 다룰 수 있도록 도와주는 자료구조입니다.예를 들어서, 다음 import 문을 추상 구문 트리로 옮기면 이런 모습이 됩니다.import React, { useMemo } from 'react';ImportDeclaration {\n  specifiers: [\n    ImportDefaultSpecifier {\n      local: Identifier {\n        name: \"React\"\n      }\n    },\n    ImportSpecifier {\n      local: Identifier {\n        name: \"useMemo\"\n      }\n    }\n  ],\n  source: Literal {\n    value: \"react\"\n  }\n}살펴보면 import 문이 ImportDeclaration 객체로 바뀌었습니다. 또 내부에서 사용되는 Default Import와 Named Import, 라이브러리 이름이 알맞은 객체로 옮겨진 것을 확인할 수 있습니다.ASTExplorer작성한 코드의 추상 구문 트리를 ASTExplorer로 쉽게 확인할 수 있습니다. 코드만 붙여넣으면 해당하는 구문 트리를 바로 확인할 수 있어 편리합니다. 소스 코드의 특정 부분에 커서를 옮기면 그 부분이 트리의 어떤 부분에 해당하는지 바로 볼 수 있기도 합니다. 😉 추상 구문 트리에 익숙하지 않다면, 사용해보시는 것을 권장합니다.라이브러리별 추상 구문 트리라이브러리마다 사용하는 추상 구문 트리의 모습은 다를 수 있습니다. 예를 들어서 같은 JavaScript를 다루더라도 ESLint가 사용하는 트리와 Babel이 사용하는 트리는 약간 다릅니다. JSCodeShift는 Babel이 사용하는 트리를 사용하고 있습니다.ASTExplorer 상단 메뉴에서 사용할 추상 구문 트리를 선택할 수 있습니다. JSCodeShift가 사용하는 트리는 @babel/parser 입니다.JSCodeShift 사용하기JSCodeShift로 코드를 수정하는 과정은 크게 4가지 작업으로 나눌 수 있습니다.AST로 파싱: 파일의 소스 코드를 AST로 파싱합니다.수정할 노드 선택: AST에서 수정할 노드를 선택합니다.수정하기: 검색한 노드를 JSCodeShift가 제공하는 유틸리티로 코드를 변경시킵니다.소스 코드로 내보내기: 수정된 AST를 JavaScript 소스 코드로 내보냅니다.예를 들어, 이런 형식으로 코드를 작성합니다./* transformSomeCode.js */\nfunction transformSomeCode(file, { jscodeshift }) {\n  // 1. AST로 파싱\n  const tree = jscodeshift(file.source);\n\n  // 2. 수정할 노드 선택\n  const nodes = tree.find(...);\n\n  // 3. 수정\n  jscodeshift(nodes)\n    .remove() | .replaceWith() | .insertBefore()\n\n  // 4. 소스 코드로 내보내기\n  return tree.toSource();\n}이후 JSCodeShift CLI를 이용하여 jscodeshift -t transformSomeCode.js <target> 와 같은 명령을 실행하면 <target> 에 있는 소스 코드들이 transformSomeCode.js 에 정의된 규칙에 맞게 수정됩니다.이제 본격적으로 JSCodeShift에서 자주 사용되는 메서드들을 살펴보겠습니다.수정할 노드 선택하기: find()기본적으로 수정할 노드를 선택하기 위해 find() 함수를 사용합니다.예를 들어, react 라이브러리의 useMemo 를 가져오는 import 구문들을 선택하기 위해서는 아래와 같이 코드를 작성할 수 있습니다.const nodes = tree.find(\n  /* 찾을 AST 노드 타입 */\n  jscodeshift.ImportDeclaration,\n  /* 필터링할 함수 */\n  node => {\n    return (\n      /* ImportDeclaration 중에서 */\n      node.type === 'ImportDeclaration' &&\n      /* react 라이브러리에서 */\n      node.source.value === 'react' &&\n      /* 가져오는 것 중에서 */\n      node.specifiers.some(specifier => {\n        /* useMemo를 포함하는 것을 */\n        return (\n          specifier.type === 'ImportSpecifier' &&\n          specifier.imported.name === 'useMemo'\n        );\n      })\n      /* 선택한다 */\n    )\n  }\n);노드 삭제하기: remove()선택한 노드를 삭제하기 위해 remove() 함수를 사용합니다.예를 들어서, 아래와 같이 코드를 작성함으로써 선택한 node 의 목록을 삭제할 수 있습니다.for (const node of nodes) {\n  jscodeshift(node).remove();\n}노드를 다른 노드로 치환하기: replaceWith()선택한 노드를 새로운 노드로 치환하려고 할 때 replaceWith() 함수를 사용할 수 있습니다.예를 들어서, 선택한 node 들을 다른 모습으로 치환하기 위해서는 아래와 같이 코드를 작성할 수 있습니다.for (const node of nodes) {\n  /* 노드를 만드는 방법에 대해서 아래에서 더 자세히 다룹니다. */\n  const newNode = createNode();\n\n  jscodeshift(node).replaceWith(newNode);\n}새로운 노드 만들기replaceWith() 와 같은 함수에서 사용하기 위해서 새로운 노드를 만들 때는 JSCodeShift에서 제공하는 도우미 함수들을 사용할 수 있습니다.각 노드를 만드는 방법을 모두 알 필요는 없습니다. TypeScript를 사용하는 경우, 각 함수가 어떤 인자를 받는지 바로 확인할 수 있습니다. JavaScript를 사용하는 경우, ast-types가 정의하는 타입 정보를 참고해주세요.변수 참조: foo와 같은 변수에 참조하는 노드를 만들기 위해서 jscodeshift.identifier() 를 사용할 수 있습니다.jscodeshift.identifier('foo');멤버 접근: 변수 foo의 멤버 bar 에 접근하는 노드를 만들기 위해서 jscodeshift.memberExpression() 을 사용할 수 있습니다.jscodeshift.memberExpression(\n  jscodeshift.identifier('foo'),\n  jscodeshift.identifier('bar')\n);import 문: import { useMemo } from 'react'; 와 같은 import 문을 만들기 위해서 jscodeshift.importDeclaration() 을 사용할 수 있습니다.jscodeShift.importDeclaration(\n  [\n    jscodeShift.importSpecifier(\n      jscodeshift.identifier('useMemo')\n    )\n  ],\n  jscodeshift.literal('react')\n);JSCodeShift 사용 예시토스 프론트엔드 챕터에서는 2020년 import { Adaptive } from '@tossteam/web-development-kits' 와 같은 import 문을 모두 import { adaptive } from '@tossteam/colors' 으로 수정해야 하는 필요성이 있었습니다.이런 경우는 찾아 바꾸기로 해결하는 데에 어려움이 있었습니다. 코드를 수정하는 규칙이 복잡했기 때문입니다.@tossteam/web-development-kits 라이브러리로부터 Adaptive 뿐 아니라 다른 변수나 함수를 import 하는 경우가 있었습니다. 그런 경우에는 전체 import 문을 지우는 것이 아닌, Adaptive 를 가져오는 부분만 삭제해야 했습니다.Adaptive 를 import하는 부분이 삭제된 경우에만 import { adaptive } from '@tossteam/colors'; 와 같이 새로운 import 문을 파일의 가장 처음에 추가해주어야 했습니다. 아닌 경우, 사용하지 않은 변수로 인해 컴파일 시간에 오류가 발생했습니다.Adaptive 를 import하는 부분이 삭제된 경우에만 그 파일에서 사용되는 모든 Adaptive 변수를 adaptive 로 바꿔줘야 했습니다.다행히 토스팀에서는 간단히 이 문제를 JSCodeShift로 해결할 수 있었습니다. 저희가 설계한 JSCodeShift 변환 코드의 구조는 다음과 같습니다.function transformLegacyImportToNewImport(file, { jscodeshift }) {\n  const root = jscodeshift(file.source);\n\n  /* 오래된 import 문들을 찾음 */\n  const oldImports = findOldImports(root, { jscodeshift });\n\n  /* 오래된 import 문이 없는 파일인 경우, 아무 작업을 하지 않음 */\n  if (oldImports.length === 0) {\n    return;\n  }\n\n  for (const oldImport of oldImports) {\n    /* 오래된 import 문에서 Adaptive를 가져오는 부분을 삭제 */\n    /* (Adaptive만을 가져오는 import 문인 경우, import 문 전체를 삭제) */\n    removeImportMember(root, oldImport, 'Adaptive', { jscodeshift });\n  }\n\n  /* @tossteam/colors에서 adaptive를 import하는 부분을 추가 */\n  /* (@tossteam/colors를 import하고 있지 않은 경우, import 문을 추가) */\n  addImportMember(root, '@tossteam/colors', 'adaptive', { jscodeshift });\n\n  /* Adaptive 변수를 모두 adaptive로 치환 */\n  const oldAdaptives = findIdentifiers(root, 'Adaptive', { jscodeshift });\n\n  for (const oldAdaptive of oldAdaptives) {\n    jscodeshift(oldAdaptive).replaceWith(\n      jscodeshift.identifier('adaptive')\n    );\n  }\n\n  /* 수정된 소스코드를 반환 */\n  return root.toSource();\n}\n\n이 중에서 removeImportMember 함수와 같은 경우, 아래와 같이 간단히 구현할 수 있었습니다.function removeImportMember(root, importNode, name, { jscodeshift }) {\n  const oldSpecifiers = importNode.value.specifiers;\n\n  /* name을 import하는 부분을 삭제 */\n  const newSpecifiers = oldSpecifiers.filter(specifier => {\n    return (\n      specifier.type !== 'ImportSpecifier' ||\n      specifier.imported.name !== name\n    );\n  }\n\n  /* 더 이상 import할 것이 남지 않은 경우에는, import 문을 삭제 */\n  if (newSpecifiers.length === 0) {\n    jscodeshift(importNode).remove();\n    return;\n  }\n\n  /* 그렇지 않은 경우, import 문에서 name을 가져오는 부분만 삭제 */\n  jscodeshift(importNode).replaceWith(\n    jscodeshift.importDeclaration(\n      newSpecifiers,\n      importNode.value.source\n    )\n  );\n}다른 함수의 경우에도 유사하게 JSCodeShift API를 이용하여 구현할 수 있었습니다.JSCodeShift 테스트하기JSCodeShift는 작성한 변환 코드가 잘 작동하는지 테스트할 수 있도록 testUtils 라고 하는 이름의 테스트 도구를 제공합니다. 테스트 파일의 디렉토리 구조를 JSCodeShift가 요구하는 대로 맞춰야 하지만, 손쉽게 Jest에 테스트를 붙일 수 있어서 편리합니다.테스트가 잘 붙어 있으면, JSCodeShift 코드의 문제점을 바로바로 찾을 수 있게 됩니다. 개발 속도도 절약되는 만큼, JSCodeShift를 개발할 때는 꼭 테스트와 함께 하는 것을 추천합니다.JSCodeShift 테스트와 관련된 자세한 내용은 JSCodeShift README에서 확인할 수 있습니다.토스팀과 JSCodeShift토스 프론트엔드 개발팀은 짧은 시간동안 빠르게 개발환경을 개선해오면서 대량의 레거시 코드를 최신 라이브러리와 코드 컨벤션에 맞추도록 수정해주어야 했습니다. 경우에 따라서는 작성된지 2년이 지난 오래된 코드가 수만 줄 이상 존재하기도 했습니다.이때 JSCodeShift를 사용함으로써 그런 코드도 한번에 최신 코드와 같이 일관성을 맞출 수 있었습니다. 이번 JSCodeShift 가이드가 레거시 시스템을 다루는 다른 프론트엔드 개발자 분들께 도움이 되었으면 합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "4": "node_modules로부터 우리를 구원해 줄 Yarn Berry박서진ㆍFrontend Developer2021. 5. 7토스 프론트엔드 챕터에서는 지난해부터 의존성을 관리하기 위해 Yarn Berry(v2)를 도입했습니다. 처음에는 일부 레포지토리부터 시작하여, 현재는 대부분의 레포지토리에 Yarn Berry가 적용되어 있는데요. 토스팀이 새로운 패키지 관리 시스템을 도입하게 된 배경과 사용하면서 좋았던 점을 테크 블로그를 통해 공유합니다.Yarn Berry란?Yarn Berry는 Node.js를 위한 새로운 패키지 관리 시스템으로, Yarn v1의 주요 개발자인 Maël Nison 씨가 만들었습니다. 2020년 1월 25일부터 정식 버전(v2)가 출시되어, 현재는 Babel과 같은 큰 오픈소스 레포지토리에서도 채택하고 있습니다. Yarn Berry는 GitHub yarnpkg/berry 레포지토리에서 소스코드가 관리되고 있습니다.Yarn Berry는 기존의 \"깨져 있는\" NPM 패키지 관리 시스템을 혁신적으로 개선합니다.NPM의 문제점NPM은 Node.js 설치 시에 기본으로 제공되어 범용적으로 사용되고 있으나, 비효율적이거나 깨져 있는 부분이 많습니다.비효율적인 의존성 검색NPM은 파일 시스템을 이용하여 의존성을 관리합니다. 익숙한 node_modules 폴더를 이용하는 것이 특징인데요. 이렇게 관리했을 때 의존성 검색은 비효율적으로 동작합니다.예를 들어, /Users/toss/dev/toss-frontend-libraries 폴더에서 require() 문을 이용하여 react 패키지를 불러오는 상황을 가정합시다.라이브러리를 찾기 위해 순회하는 디렉토리의 목록을 확인하려고 할 때, Node.js에서 제공하는 require.resolve.paths() 함수를 사용할 수 있습니다. 이 함수는 NPM이 검색하는 디렉토리의 목록을 반환합니다.$ node\nWelcome to Node.js v12.16.3.\nType \".help\" for more information.\n> require.resolve.paths('react')\n[\n  '/Users/toss/dev/toss-frontend-libraries/repl/node_modules',\n  '/Users/toss/dev/toss-frontend-libraries/node_modules',\n  '/Users/toss/node_modules',\n  '/Users/node_modules',\n  '/node_modules',\n  '/Users/toss/.node_modules',\n  '/Users/toss/.node_libraries',\n  '/Users/toss/.nvm/versions/node/v12.16.3/lib/node',\n  '/Users/toss/.node_modules',\n  '/Users/toss/.node_libraries',\n  '/Users/toss/.nvm/versions/node/v12.16.3/lib/node'\n]\n\n목록에서 확인할 수 있는 것처럼, NPM은 패키지를 찾기 위해서 계속 상위 디렉토리의 node_modules 폴더를 탐색합니다. 따라서 패키지를 바로 찾지 못할수록 readdir, stat과 같은 느린 I/O 호출이 반복됩니다. 경우에 따라서는 I/O 호출이 중간에 실패하기도 합니다.TypeScript 4.0까지는 node_modules를 이용한 패키지 탐색이 너무 비효율적인 나머지, 패키지를 처음으로 import 하기 전까지는 node_modules 내부의 타입 정보를 찾아보지 않기도 했습니다. (TS 4.0 Changelog)환경에 따라 달라지는 동작NPM은 패키지를 찾지 못하면 상위 디렉토리의 node_modules 폴더를 계속 검색합니다. 이 특성 때문에 어떤 의존성을 찾을 수 있는지는 해당 패키지의 상위 디렉토리 환경에 따라 달라집니다.예를 들어, 상위 디렉토리가 어떤 node_modules를 포함하고 있는지에 따라 의존성을 불러올 수 있기도 하고, 없기도 합니다. 다른 버전의 의존성을 잘못 불러올 수 있는 여지도 존재합니다.이렇게 환경에 따라 동작이 변하는 것은 나쁜 징조입니다. 해당 상황을 재현하기 까다로워지기 때문입니다.비효율적인 설치NPM에서 구성하는 node_modules 디렉토리 구조는 매우 큰 공간을 차지합니다. 일반적으로 간단한 CLI 프로젝트도 수백 메가바이트의 node_modules 폴더가 필요합니다. 용량만 많이 차지할 뿐 아니라, 큰 node_modules 디렉토리 구조를 만들기 위해서는 많은 I/O 작업이 필요합니다.node_modules 폴더는 복잡하기 때문에 설치가 유효한지 검증하기 어렵습니다. 예를 들어, 수백 개의 패키지가 서로를 의존하는 복잡한 의존성 트리에서 node_modules 디렉토리 구조는 깊어집니다.이렇게 깊은 트리 구조에서 의존성이 잘 설치되어 있는지 검증하려면 많은 수의 I/O 호출이 필요합니다. 일반적으로 디스크 I/O 호출은 메모리의 자료구조를 다루는 것보다 훨씬 느립니다. 이런 문제로 인해 Yarn v1이나 NPM은 기본적인 의존성 트리의 유효성까지만 검증하고, 각 패키지의 내용이 올바른지는 확인하지 않습니다.유령 의존성 (Phantom Dependency)NPM 및 Yarn v1에서는 중복해서 설치되는 node_modules를 아끼기 위해 끌어올리기(Hoisting) 기법을 사용합니다.예를 들어, 의존성 트리가 왼쪽의 모습을 하고 있다고 가정합시다.왼쪽 트리에서 [A (1.0)]과 [B (1.0)] 패키지는 두 번 설치되므로 디스크 공간을 낭비합니다. NPM과 Yarn v1에서는 디스크 공간을 아끼기 위해 원래 트리의 모양을 오른쪽 트리처럼 바꿉니다.오른쪽 트리로 의존성 트리가 바뀌면서 package-1 에서는 원래 require() 할 수 없었던 [B (1.0)] 라이브러리를 불러올 수 있게 되었습니다.이렇게 끌어올리기에 따라 직접 의존하고 있지 않은 라이브러리를 require() 할 수 있는 현상을 유령 의존성(Phantom Dependency)이라고 부릅니다.유령 의존성 현상이 발생할 때, package.json에 명시하지 않은 라이브러리를 조용히 사용할 수 있게 됩니다. 다른 의존성을 package.json 에서 제거했을 때 소리없이 같이 사라지기도 합니다. 이런 특성은 의존성 관리 시스템을 혼란스럽게 만듭니다.Plug'n'Play (PnP)Yarn Berry는 위에서 언급한 문제를 새로운 Plug'n'Play 전략을 이용하여 해결합니다.Plug'n'Play의 배경Yarn v1은 package.json 파일을 기반으로 의존성 트리를 생성하고, 디스크에 node_modules 디렉토리 구조를 만듭니다. 이미 패키지의 의존성 구조를 완전히 알고 있는 것입니다.node_modules 파일 시스템을 이용한 의존성 관리는 깨지기 쉽습니다. 모든 패키지 매니저가 실수하기 쉬운 Node 내장 의존성 관리 시스템을 사용해야 할까요? 패키지 매니저들이 node_modules 디렉토리 구조를 만드는 것에 그치지 않고, 보다 근본적으로 안전하게 의존성을 관리하면 어떨까요?Plug'n'Play는 이런 생각에서 출발했습니다.Plug'n'Play 켜기NPM에서 최신 버전의 Yarn을 내려받고, 버전을 Berry로 설정하면 Yarn Berry를 사용할 수 있습니다.$ npm install -g yarn\n$ cd ../path/to/some-package\n$ yarn set version berryYarn Berry는 기존 Node.js 의존성 관리 시스템과 많이 다르기 때문에 하위호환을 위해 패키지 단위로만 도입할 수 있습니다.Plug'n'Play의 동작 방법Plug'n'Play 설치 모드에서 yarn install 로 의존성을 설치했을 때, 기존과 다른 모습을 볼 수 있습니다.Yarn Berry는 node_modules를 생성하지 않습니다. 대신 .yarn/cache 폴더에 의존성의 정보가 저장되고, .pnp.cjs 파일에 의존성을 찾을 수 있는 정보가 기록됩니다. .pnp.cjs를 이용하면 디스크 I/O 없이 어떤 패키지가 어떤 라이브러리에 의존하는지, 각 라이브러리는 어디에 위치하는지를 바로 알 수 있습니다.예를 들어, react 패키지는 .pnp.cjs 파일에서 다음과 같이 나타납니다./* react 패키지 중에서 */\n[\"react\", [\n  /* npm:17.0.1 버전은 */\n  [\"npm:17.0.1\", {\n    /* 이 위치에 있고 */\n    \"packageLocation\": \"./.yarn/cache/react-npm-17.0.1-98658812fc-a76d86ec97.zip/node_modules/react/\",\n    /* 이 의존성들을 참조한다. */\n    \"packageDependencies\": [\n      [\"loose-envify\", \"npm:1.4.0\"],\n      [\"object-assign\", \"npm:4.1.1\"]\n    ],\n  }]\n]],\n\nreact 17.0.1 버전 패키지의 위치와 의존성의 목록을 완전하게 기술하고 있는 것을 확인할 수 있습니다. 이로부터 특정 패키지와 의존성에 대한 정보가 필요할 때 바로 알 수 있습니다.Yarn은 Node.js가 제공하는 require() 문의 동작을 덮어씀으로써 효율적으로 패키지를 찾을 수 있도록 합니다. 이 때문에 PnP API를 이용하여 의존성 관리를 하고 있을 때에는 node 명령어 대신 yarn node 명령어를 사용해야 합니다.$ yarn node일반적으로 Node.js 앱을 실행할 때에는 package.json의 scripts 에 실행 스크립트를 등록하여 사용하게 됩니다. 이때 Yarn v1에서 사용하던 것처럼 Yarn으로 스크립트를 실행하기만 하면 자동으로 PnP로 의존성을 불러옵니다.$ yarn devZipFS (Zip Filesystem)zip으로 묶인 라이브러리가 저장된 .yarn/cache 폴더Yarn PnP 시스템에서 각 의존성은 Zip 아카이브로 관리됩니다. 예를 들어, Recoil 0.1.2 버전은 recoil-npm-0.1.2-9a0edbd2b9-c69105dd7d.zip과 같은 압축 파일로 관리됩니다.이후 .pnp.cjs 파일이 지정하는 바에 따라 동적으로 Zip 아카이브의 내용이 참조됩니다.Zip 아카이브로 의존성을 관리하면 다음과 같은 장점이 생깁니다.더 이상 node_modules 디렉토리 구조를 생성할 필요가 없기 때문에 설치가 신속히 완료됩니다.각 패키지는 버전마다 하나의 Zip 아카이브만을 가지기 때문에 중복해서 설치되지 않습니다. 각 Zip 아카이브가 압축되어 있음을 고려할 때, 스토리지 용량을 크게 아낄 수 있습니다.실제로 토스팀에서 의존성이 차지하는 크기를 대폭 감축할 수 있었습니다.한 서비스의 경우 NPM을 이용했을 때 node_modules 디렉토리가 약 400MB를 차지했지만, Yarn PnP를 사용했을 때 의존성 디렉토리의 크기는 120MB에 불과했습니다.의존성을 구성하는 파일의 수가 많지 않으므로, 변경 사항을 감지하거나 전체 의존성을 삭제하는 작업이 빠릅니다.없는 의존성이나 더 이상 필요 없는 의존성을 쉽게 찾을 수 있습니다.Zip 파일의 내용이 변경되었을 때에는 체크섬과 비교하여 쉽게 변경 여부를 감지할 수 있습니다.Plug'n'Play 도입 결과토스 프론트엔드 챕터가 Plug'n'Play를 도입한 결과, 다양한 장점을 느낄 수 있었습니다.의존성을 검색할 때의존성을 검색할 때, 더 이상 node_modules 폴더를 순회할 필요가 없습니다. .pnp.cjs 파일이 제공하는 자료구조를 이용하여 바로 의존성의 위치를 찾기 때문입니다. 이로써 require()에 걸리는 시간이 크게 단축되었습니다.재현 가능성패키지의 모든 의존성은 .pnp.cjs 파일을 이용하여 관리되기 때문에 더 이상 외부 환경에 영향받지 않습니다. 이로써 다양한 기기 및 CI 환경에서 require() 또는 import 문의 동작이 동일할 것임을 보장할 수 있게 되었습니다.의존성을 설치할 때더 이상 설치를 위해 깊은 node_modules 디렉토리를 생성하지 않아도 됩니다. 또 NPM이 설치하는 것처럼 같은 버전의 패키지가 여러 번 복사되어 설치 시간을 극단적으로 단축할 수 있습니다. 이에 더해 Zero-install을 사용하면 대부분 라이브러리를 설치 없이 사용할 수 있습니다.이를 이용하면 CI와 같이 반복적으로 의존성 설치 작업이 이루어지는 곳에서 시간을 크게 절약할 수 있습니다. 토스팀에서는 원래 CI에서 60초씩 걸리던 설치 작업을 Yarn PnP를 도입함으로써 수 초 이내로 단축했습니다.엄격한 의존성 관리Yarn PnP는 node_modules에서와 같이 의존성을 끌어올리지 않습니다. 이로써 각 패키지들은 자신이 package.json에 기술하는 의존성에만 접근할 수 있습니다. 기존에 환경에 따라 우연히 작동할 수 있었던 코드들이 보다 엄격히 관리되는 것입니다. 이로써 예기치 못한 버그를 쉽게 일으키던 유령 의존성 현상을 근본적으로 막을 수 있었습니다.의존성 검증node_modules를 사용하여 의존성을 관리했을 때에는 올바르게 의존성이 설치되지 못해서 의존성 폴더 전체를 지우고 다시 설치해야 하는 경우가 발생하고는 했습니다. node_modules 폴더를 검증하기 어려웠기 때문입니다. 전체 재설치를 수행할 때 node_modules 디렉토리 구조를 다시 만드느라 1분 이상의 시간이 허비되기도 했습니다.Yarn PnP에서는 Zip 파일을 이용하여 패키지를 관리하기 때문에 빠진 의존성을 찾거나 의존성 파일이 변경되었음을 찾기 쉽습니다. 이로써 의존성이 잘못되었을 때 쉽게 바로잡을 수 있습니다. 이로써 올바르게 의존성이 설치되는 것을 100%에 가깝게 보장할 수 있습니다.Zero-Install위에서 Yarn Berry의 PnP를 도입함으로써 얻을 수 있는 다양한 장점들을 살펴보았습니다. 여기에서 한 발 더 나아간 생각을 해 볼 수 있습니다. 바로 의존성도 Git 등을 이용하여 버전 관리를 하면 어떨까? 라고 하는 생각인데요.Yarn PnP은 의존성을 압축 파일로 관리하기 때문에 의존성의 용량이 작습니다. 또한 각 의존성은 하나의 Zip 파일로만 표현되기 때문에 의존성을 구성하는 파일의 숫자가 NPM만큼 많지 않습니다. 예를 들어, 일반적인 node_modules 는 1.2GB 크기이고 13만 5천개의 파일로 구성되어 있는 반면, Yarn PnP의 의존성은 139MB 크기의 2천개의 압축 파일로 구성됩니다.이처럼 용량과 파일의 숫자가 적기 때문에 Yarn Berry를 사용하면 의존성을 Git으로 관리할 수 있습니다. 그리고 이렇게 의존성의 버전을 관리할 때 더욱 큰 장점들을 발견할 수 있습니다.이렇게 Yarn Berry에서 의존성을 버전 관리에 포함하는 것을 Zero-Install이라고 합니다.Yarn Berry Git 레포지토리에서 사용하는 Zero-install의존성을 버전 관리에 포함하면 많은 장점들이 생깁니다.새로 저장소를 복제하거나 브랜치를 바꾸었다고 해서 yarn install을 실행하지 않아도 됩니다. 일반적으로 다른 의존성을 사용하는 곳으로 브랜치를 변경했을 때, 잊지 않고 의존성을 설치해주어야 했습니다. 경우에 따라서는 잘못된 의존성 버전이 사용됨으로써 웹 서비스가 알 수 없는 이유로 오동작하기도 했습니다. Zero-Install을 사용했을 때 이런 문제는 완전히 해결됩니다. 더해서 네트워크가 끊어진 곳에서는 오프라인 캐시 기능을 해주기도 합니다.CI에서 의존성 설치하는 시간을 크게 절약할 수 있습니다. 토스에서는 일반적으로 캐시가 존재하지 않을 때 의존성을 설치하기 위해서 60초~90초의 시간이 필요했습니다. Zero-Install을 사용하면 Git Clone으로 저장소를 복제했을 때 의존성들이 바로 사용 가능한 상태가 되어, 의존성을 설치할 필요가 없습니다. 이로써 CI 시간을 크게 절약할 수 있었습니다.토스 프론트엔드 챕터에서는 Zero-install 기능을 적극적으로 레포지토리에 도입함으로써 빌드와 배포 시간을 크게 단축할 수 있었습니다.그 외 Yarn Berry에서 좋았던 점이 외에 Yarn Berry는 다양한 개발자 친화적인 기능을 제공합니다.플러그인 시스템: Yarn Berry는 핵심 기능도 플러그인을 이용하여 개발되어 있을 만큼 플러그인 친화적인 환경을 자랑합니다. 필요한 만큼 Yarn의 기능을 확장하여 손쉽게 CLI로 사용할 수 있습니다.토스 프론트엔드 챕터에서는 이현섭님께서 변경된 워크스페이스를 계산하는 플러그인을 며칠만에 만들어주시기도 하셨습니다. 이처럼 Yarn Berry의 기능이 부족하다면 손쉽게 플러그인을 만들 수 있습니다.워크스페이스: Yarn Berry는 Yarn v1와 비교할 수 없을 정도로 높은 완성도의 워크스페이스 기능을 제공합니다. Yarn Berry의 Git 레포지토리에서 대표적으로 사용하는 모습을 확인할 수 있습니다. TypeScript를 사용함에도 한 패키지의 소스 코드의 변경사항이 즉시 다른 패키지에 반영되는 모습이 인상적입니다.토스 프론트엔드 챕터에서도 적극적으로 워크스페이스 기능을 사용하고 있습니다.패치 명령어 기본 지원: 경우에 따라서 NPM에 배포된 라이브러리의 일부분만 수정해서 사용하고 싶은 니즈가 있습니다. Yarn Berry는 yarn patch 명령어를 제공함으로써 쉽게 라이브러리의 일부분을 수정해서 사용할 수 있도록 합니다. 이렇게 만든 패치 파일은 patch: 프로토콜을 이용해서 쉽게 의존성 설치에 사용할 수 있습니다.토스팀은 이렇게 Yarn Berry를 도입함으로써 JavaScript 의존성을 효율적이고 안전하게 다룰 수 있었습니다. 오래 걸리던 CI 속도를 60초 이상 단축하기도 했습니다.다음 Yarn Berry 아티클에서는 실제로 거대한 서비스 모노레포를 Yarn Berry로 이전한 경험을 소개드리면서 실사용에서 주의할 점에 대해 보다 자세히 소개드리겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "5": "Template Literal Types로 타입 안전하게 코딩하기박서진ㆍFrontend Developer2021. 5. 142020년 11월 TypeScript 4.1이 출시되면서 \"Template Literal Type\"을 사용할 수 있게 되었습니다. TypeScript로 JSON Parser를 만들거나, document.querySelector 의 결과 타입을 추론할 수 있게 되어 화제가 되었는데요. 이번 아티클에서는 Template Literal Type이란 무엇인지, 이를 바탕으로 어떻게 그런 결과물을 만들 수 있었는지 간단히 예시로 소개드리고자 합니다.Template Literal Type이란?간단히 말해, Template Literal Type이란 기존 TypeScript의 String Literal Type을 기반으로 새로운 타입을 만드는 도구입니다. 구체적인 예시로 Template Literal Type에 대해 자세히 살펴보겠습니다.예시 1: 가장 간단한 형태type Toss = 'toss';\n\n// type TossPayments = 'toss payments';\ntype TossPayments = `${Toss} payments`;TypeScript Playground가장 간단한 형태로, 원래 있던 'toss' 라고 하는 타입을 바탕으로 'toss payments' 라고 하는 타입을 만드는 경우를 생각할 수 있습니다.TypeScript 4.1 이전에는 이런 문자열 작업이 불가능했지만, Template Literal Type을 이용함으로써 보다 넓은 타입 연산이 가능해졌습니다.예시 2: 하나의 Union Typetype Toss = 'toss';\ntype Companies = 'core' | 'bank' | 'securities' | 'payments' | 'insurance';\n\n// type TossCompanies = 'toss core' | 'toss bank' | 'toss securities' | ...;\ntype TossCompanies = `${Toss} ${Companies}`TypeScript PlaygroundTemplate Literal Type을 Union type(합 타입)과 함께하면, 결과물도 Union Type이 됩니다.예를 들어, 위 예시에서 'toss' 타입과 'core' | 'bank' | 'securities' | ... 타입을 Template Literal Type으로 연결하면 'toss core' | 'toss bank' | 'toss securities' | ... 와 같이 확장되는 것을 확인할 수 있습니다.예시 3: 여러 개의 Union Typetype VerticalAlignment = \"top\" | \"middle\" | \"bottom\";\ntype HorizontalAlignment = \"left\" | \"center\" | \"right\";\n\n// type Alignment =\n//   | \"top-left\"    | \"top-center\"    | \"top-right\"\n//   | \"middle-left\" | \"middle-center\" | \"middle-right\"\n//   | \"bottom-left\" | \"bottom-center\" | \"bottom-right\"\ntype Alignment = `${VerticalAlignment}-${HorizontalAlignment}`;TypeScript Playground여러 개의 Union Type을 연결할 수도 있습니다.예를 들어, 위에서는 VerticalAlignment 타입과 HorizontalAlignment 타입을 연결하여, ${VerticalAlignment}-${HorizontalAlignment} 타입을 만들었습니다.원래라면 중복해서 Alignment 타입을 다시 정의해야 했겠지만, Template Literal Type을 사용함으로써 중복 없이 더욱 간결히 타입을 표현할 수 있게 되었습니다.예시 4: 반복되는 타입 정의 없애기문제 상황// 이벤트 이름이 하나 추가될 때마다....\ntype EventNames = 'click' | 'doubleClick' | 'mouseDown' | 'mouseUp';\n\ntype MyElement = {\n    addEventListener(eventName: EventNames, handler: (e: Event) => void): void;\n\n    // onEvent() 도 하나씩 추가해줘야 한다\n    onClick(e: Event): void;\n    onDoubleClick(e: Event): void;\n    onMouseDown(e: Event): void;\n    onMouseUp(e: Event): void;\n};이벤트에 대한 핸들러를 등록할 때, addEventListener('event', handler) 와 onEvent = handler 의 두 가지 형식을 모두 사용할 수 있는 MyElement 타입을 생각해봅시다.// 두 가지 방법 모두 사용할 수 있는 경우\nelement.addEventListener('click', () => alert('I am clicked!'));\nelement.onClick = () => alert('I am clicked!');예를 들어, click 이벤트를 구독할 때, 위의 두 가지 방법을 모두 사용할 수 있는 것입니다.요소에 추가할 수 있는 이벤트의 종류는 자주 변경되고는 합니다. 예를 들어, 브라우저 API가 바뀌면서 'pointerDown' 과 같은 이벤트가 새로 추가될 수 있습니다.이런 경우, TypeScript 4.1 이전에는 매번 수동으로 여러 곳의 타입을 수정해야 했습니다. 우선 addEventListener의 인자로 사용되는 이벤트 이름 EventNames 타입에 'pointerDown' 을 넣어야 했습니다. 또 onPointerDown 메서드를 명시해야 했습니다. 잊지 않고 두 곳을 수정해야 했기 때문에, 실수하기 쉬웠습니다.하지만 Template Literal Type을 이용하면 한 곳만 수정해도 모두에 반영되도록 할 수 있습니다.type EventNames = 'click' | 'doubleClick' | 'mouseDown' | 'mouseUp';\n\n// CapitalizedEventNames = 'Click' | 'DoubleClick' | ...;\ntype CapitalizedEventNames = Capitalize<EventNames>;\n\n// type HandlerNames = 'onClick' | 'onDoubleClick' | 'onMouseDown' | 'onMouseUp';\ntype HandlerNames = `on${CapitalizedEventNames}`;\n\ntype Handlers = {\n  [H in HandlerNames]: (event: Event) => void;\n};\n\n// 원래 MyElement 그대로 작동!\ntype MyElement = Handlers & {\n  addEventListener: (eventName: EventNames, handler: (event: Event) => void) => void;\n};위 코드를 한번 자세히 살펴봅시다.CapitalizedEventNames 타입을 정의할 때, TypeScript 4.1에서 추가된 Capitalize<T> 타입을 이용하여 EventNames의 첫 글자를 대문자로 만들었습니다.HandlerNames 타입을 만들 때, Template Literal Type으로 onClick 과 같이 on 접두사를 붙였습니다.Handlers 타입에서는 기존의 onClick, onMouseDown 과 같은 이벤트 핸들러를 메서드로 가지도록 했고,마지막으로 MyElement 에서는 addEventListener 메서드를 가지는 객체와 연결하여 원래와 동일한 동작을 하는 타입을 만들 수 있었습니다.이제 EventNames 만 수정하면 MyElement 에서 이벤트를 구독하는 양쪽 모두 대응이 되므로, 코드가 깔끔해지고 실수의 여지가 적어졌습니다. ✨Conditional Type과 더 강력한 추론하기Template Literal Type은 Conditional Type과 함께 더욱 강력하게 사용할 수 있습니다.Conditional Type 되짚어보기Conditional Type은 JavaScript의 삼항 연산자와 비슷하게 분기를 수행하면서, 타입을 추론하는 방법인데요. 고급 TypeScript 사용에서 강력한 타입 연산을 하기 위해서 빠지지 않습니다.Template Literal Type을 더 잘 다루기 위해 반드시 필요한 개념이므로, 간단한 예시로 Conditional Type을 사용하는 방법에 대해 살펴보겠습니다.예시 1: 제네릭 타입 인자 꺼내오기Conditional Type을 가장 자주 사용하는 경우로,  Promise<number>와 같은 타입에서 number 를 꺼내오고 싶은 상황을 생각해봅시다.type PromiseType<T> = T extends Promise<infer U> ? U : never;\n\n// type A = number\ntype A = PromiseType<Promise<number>>;\n\n// type B = string | boolean\ntype B = PromiseType<Promise<string | boolean>>;\n\n// type C = never\ntype C = PromiseType<number>;TypeScript Playground위 코드를 살펴보면, PromiseType<T> 타입에 Promise<number> 타입을 인자로 넘기면 number 타입을 얻고 있습니다.Conditional Type이 동작하는 방식을 간단히 알아봅시다.삼항 연산자처럼 생긴 부분 가운데 X extends Y 와 같이 생긴 조건 부분은 X 타입의 변수가 Y 타입에 할당될 수 있는지에 따라 참값이 평가됩니다.예시:true extends boolean: true 는 boolean 에 할당될 수 있으므로 참으로 평가됩니다.'toss' extends string: 'toss' 는 string 에 할당될 수 있으므로 참으로 평가됩니다.Array<{ foo: string }> extends Array<unknown>: 마찬가지로 참으로 평가됩니다.string extends number: 문자열은 숫자 타입에 할당될 수 없으므로 거짓입니다.boolean extends true: boolean 타입 가운데 false 는 true 에 할당될 수 없으므로 거짓입니다.조건식이 참으로 평가될 때에는 infer 키워드를 사용할 수 있습니다. 예를 들어, Promise<number> extends Promise<infer U> 와 같은 타입을 작성하면, U 타입은 number 타입으로 추론됩니다. 이후 참인 경우에 대응되는 식에서 추론된 U 타입을 사용할 수 있습니다.예를 들어, Promise<number> extends Promise<infer U> ? U : never 에서는 조건식이 참이고 U 타입이 number로 추론되므로, 이를 평가한 타입의 결과는 number 가 됩니다.반대로 number extends Promise<infer U> ? U : never 에서는 조건식이 거짓이므로 이를 평가한 결과는 never가 됩니다.예시 2: Tuple 다루기[string, number, boolean] 과 같은 TypeScript의 Tuple Type에서 그 꼬리 부분인 [number, boolean] 과 같은 부분만 가져오고 싶은 상황을 생각해봅시다.Conditional Type과 Variadic Tuple Type을 활용함으로써 이를 간단히 구현할 수 있습니다.type TailOf<T> = T extends [unknown, ...infer U] ? U : [];\n\n// type A = [boolean, number];\ntype A = TailOf<[string, boolean, number]>;TypeScript Playground첫 요소를 제외하고 ...infer U 구문을 이용하여 뒤의 요소들을 모두 선택한 것을 확인할 수 있습니다.이 외에 간단한 형태로 특정한 튜플이 비어 있는지 검사하기 위해서, 아래와 같은 IsEmpty<T> 타입을 정의할 수도 있습니다.type IsEmpty<T extends any[]> = T extends [] ? true : false;\n\n// type B = true\ntype B = IsEmpty<[]>;\n\n// type C = false\ntype C = IsEmpty<[number, string]>;TypeScript PlaygroundConditional Type에 대해 더 궁금하신 분은 TypeScript 공식 문서를 참고하시기 바랍니다.이제 Conditional Type과 Template Literal Type을 함께 사용했을 때 어떤 결과를 얻을 수 있는지 살펴봅시다.초급 예시 1: 간단한 추론type InOrOut<T> = T extends `fade${infer R}` ? R : never;\n\n// type I = \"In\"\ntype I = InOrOut<\"fadeIn\">;\n// type O = \"Out\"\ntype O = InOrOut<\"fadeOut\">;가장 간단한 예시로, 'fadeIn' | 'fadeOut' 과 같은 타입에서 앞의 fade 접두사를 버리고 'In' | 'Out' 만 가져오고 싶은 상황을 생각해봅시다.Promise<number> 에서 number 를 가져오는 것과 유사하게, Conditional Type을 이용하여 접두사를 제외할 수 있습니다.중급 예시 1: 문자열에서 공백 없애기위의 예시를 응용하면 문자열의 공백을 없애는 타입을 정의할 수 있습니다. 예를 들어, 아래와 같이 오른쪽의 공백을 모두 제거한 타입을 만들 수 있습니다.// type T = \"Toss\"\ntype T = TrimRight<\"Toss      \">;TrimRight<T> 타입은 재귀적 타입 선언을 활용합니다.type TrimRight<T extends string> =\n  T extends `${infer R} `\n    ? TrimRight<R>\n    : T;TypeScript Playground위 코드를 살펴보시면, infer R 문 뒤에 하나의 공백이 있는 것을 확인하실 수 있습니다.즉, T 타입의 오른쪽에 공백이 하나 있다면, 공백을 하나 빠뜨린 것을 R 타입으로 추론하고, 다시 TrimRight<R> 을 호출합니다.만약 공백이 더 이상 존재하지 않는다면, 원래 주어진 타입 그대로를 반환합니다.TypeScript에는 if 문이 존재하지 않지만, 만약 존재한다고 가정했을 때 아래와 같이 작성해볼 수 있습니다.type TrimRight<T extends string> =\n  if (T extends `${infer R} `) {\n    return TrimRight<R>;\n  } else {\n    return T;\n  }보다 재귀적인 구조를 잘 확인할 수 있습니다.중급 예시 2: 점으로 연결된 문자열 Split하기재귀적 타입 정의를 활용하면 'foo.bar.baz' 와 같은 타입을 ['foo', 'bar', 'baz'] 로 나누는 타입을 정의할 수 있습니다.type Split<S extends string> =\n  S extends `${infer T}.${infer U}`\n    ? [T, ...Split<U>]\n    : [S];\n\n// type S = [\"foo\", \"bar\", \"baz\"];\ntype S = Split<\"foo.bar.baz\">;TypeScript Playground주어진 S 타입에서 첫번째 점(.) 을 찾고, 그 앞 부분을 T, 뒷 부분을 U 로 추론합니다. 이후 이를 [T, ...Split<U>]와 같이 재귀적으로 하나씩 값을 이어 나가면서 원하는 결과 타입을 만들어 나갑니다.이 경우에도 if 문이 있다는 가정 하에 pseudo-code로 정리해볼 수 있습니다.type Split<S extends string> =\n  if (S extends `${infer T}.${infer U}`) {\n    return [T, ...Split<infer U>];\n  } else {\n    return [S];\n  }고급 예시: lodash.set() 함수 타입 추론하기lodash.set()는 아래와 같이 문자열로 된 접근자를 이용하여 객체의 깊은 프로퍼티까지 수정할 수 있는 함수입니다.const someObject = {\n  toss: {\n    core: {\n      client: {\n        platform: \"foo\"\n      }\n    }\n  }\n};\n\n// OK!\nlodashSet(someObject, \"toss.core.client\", { platform: 'bar' });\n\n// Error: 'bar' is not assignable to type '{ platform: string }';\nlodashSet(someObject, 'toss.core.client', 'bar');Template Literal Type이 있기 전, 이런 함수는 타입 안전하게 사용할 수 없어 세 번째 인자를 any 로 규정해야 했습니다. 그러나 위에서 살펴본 타입 정의를 조합하면 lodash.set() 를 더욱 안전하게 타이핑할 수 있습니다. 💯lodash.set() 함수를 정확하게 타이핑하기 위해서는 아래의 ValueOf<T, P> 타입이 필요합니다. ValueOf<T, P> 타입은 객체 T 와 접근 경로 P가 주어졌을 때, T 를 P 경로로 순서대로 접근했을 때 결과로 나오는 타입을 나타냅니다.interface Foo {\n  foo: {\n    bar: {\n      baz: string;\n    }\n  }\n}\n\n// type A = { bar: { baz: string } };\ntype A = ValueOf<Foo, ['foo']>;\n\n// type B = { baz: string };\ntype B = ValueOf<Foo, ['foo', 'bar']>;\n\n// type C = string;\ntype C = ValueOf<Foo, ['foo', 'bar', 'baz']>;만약에 위와 같은 ValueOf<T, P> 이 있다면, 위에서 만들었던 Split<S> 과 조합하여 쉽게 lodash.set() 함수에 타입을 부여할 수 있을 것입니다.function lodashSet<Type, Path>(\n  obj: Type,\n  path: Path,\n  value: ValueOf<Type, Split<Path>>\n): void;이제 ValueOf<T, P> 타입을 만들어봅시다. if 문과 내부 타입 선언이 있는 pseudo-code로 나타낸다면, 아래와 같이 코드를 작성할 수 있습니다.type ValueOf<Type, Paths> =\n  type Head = Paths[0];\n  type Tail = TailOf<Paths>;\n\n  if (/* Tail의 길이가 0이다 */) {\n    return Type[Head];\n  } else {\n    return ValueOf<Type[Head], Tail>;\n  }ValueOf<T, P> 타입이 그렇게 동작한다면, 위의 Foo 예시에서는 아래와 같이 차례대로 값이 계산될 것입니다.ValueOf<Foo, ['foo', 'bar']>\n== ValueOf<Foo['foo'], ['bar']>\n== ValueOf<Foo['foo']['bar'], []>\n== Foo['foo']['bar']작성했던 의사 코드를 유효한 TypeScript 코드로 나타내면 다음과 같습니다.type ValueOf<Type, Paths extends any[]> =\n  /*\n   * IsEmpty<TailOf<Paths>>가 참이면\n   * == TailOf<Paths>가 빈 Tuple이면\n   */\n  IsEmpty<TailOf<Paths>> extends true\n    ? Type[HeadOf<Paths>]\n    : ValueOf<Type[HeadOf<Paths>], TailOf<Paths>>;위 내용을 모두 조합하면 lodash.set()을 안전하게 다룰 수 있는데요. 실제로 동작하는 방식을 TypeScript Playground에서 확인해보실 수 있습니다. 😉Template Literal Type의 응용위에서 살펴본 바와 같이, Template Literal Type을 Conditional Type과 사용하면 더욱 많은 코드를 안전하게 사용할 수 있습니다. awesome-template-literal-types 레포지토리에는 상상력을 자극하는 Template Literal Type의 사용 예시들이 모여 있습니다.대표적으로 화제가 되었던 예시들에 대한 링크를 남기고 글을 맺습니다.1. TypeScript로 JSON 파서 만들기// type Json = { key1: ['value1', null]; key2: 'value2' };\ntype Json = ParseJson<'{ \"key1\": [\"value1\", null], \"key2\": \"value2\" }'>;코드와 같이 JSON 문자열을 바로 TypeScript 타입으로 옮길 수 있다는 Proof-of-concept로 화제가 되었습니다.2. document.querySelector를 타입 안전하게 사용하기const a = querySelector('div.banner > a.call-to-action'); //-> HTMLAnchorElement\nconst b = querySelector('input, div'); //-> HTMLInputElement | HTMLDivElement\nconst c = querySelector('circle[cx=\"150\"]') //-> SVGCircleElement\nconst d = querySelector('button#buy-now'); //-> HTMLButtonElement\nconst e = querySelector('section p:first-of-type'); //-> HTMLParagraphElementa 태그를 선택했을 때 결괏값이 HTMLAnchorElement가 되는 것을 확인하실 수 있습니다.3. Express의 Route Parameter로부터 타입 추론하기Express에서 사용하는 경로 문자열에서 Route Parameter의 타입을 추론할 수 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "6": "개발자의 애질리티강병훈ㆍHead of Technology2021. 10. 14이 글은 토스페이먼츠에 입사하신, 혹은 입사를 고려 중인 개발자분들을 위해 작성된 글입니다.애자일하게 일하기애자일하게 일한다는 것은 어떠한 의미일까요? 한 시간을 일하면 한 시간 만큼의 가치를 만들어 내는 방식이 아닐까 합니다. 예를 들어, 동작하는 함수를 구현하거나 난해한 개념을 이해하는 식으로요. Big up-front 설계과거에는 프로젝트 진행 초기부터 분석과 설계에 많은 시간을 투자했습니다. 전체 프로젝트의 ⅓ 이상이 분석과 설계일 정도로요. 문제는 프로젝트 기간의 ½ 시점에 요구사항이 크게 변한다면 결과물 하나 없이 다시 새로 시작해야 한다는 점이었습니다. 세상의 변화속도가 엄청나게 빨라진 지금, 초기 설계 비용이 큰 big up-front 설계가 항상 잘 들어맞지 않는다는 사실을 이제는 대부분의 사람이 알게 되었습니다.애자일 개발 방법론애자일 개발 방법론에서는 애초에 크게 설계하지 않습니다. 간단한 사용 시나리오를 작성하고 그것의 구현을 목표로 하는 짧은 주기를 가집니다. 거대한 아키텍처를 설계하지 않고 이번 주기(이터레이션(Iteration) 혹은 스프린트(Sprint))에서 달성할 수 있는 만큼의 설계를 추구합니다. (앞으로는 주기가 아닌 스프린트라고 표현하겠습니다.)예를 들어 카드 승인이 100가지의 하위 기능으로 구분할 수 있다면, 이번 스프린트의 목표를 그중에 3가지로 결정할 수 있습니다. 나머지 기능을 함께 확인하고 싶으면 어떻게 하냐구요? 애초에 3가지 기능 외에는 개발하지 않는다는 목표로 일을 진행하기 때문에 스프린트 목표 달성 여부를 확인할 때에도 오직 이 3가지 기능만 확인합니다. 이번 스프린트를 성공적으로 달성했다면 팀은 최소한 3개의 작은 기능을 획득할 수 있게 됩니다. 개발자들은 요구사항 전체가 아닌 이번 스프린트에 필요한 요구사항에 더 집중할 수 있게 되고, 만약 스프린트가 성공적으로 진행되었다면 3개의 기능을 제공하는 소프트웨어를 가지게 됩니다. 팀은 점진적으로 비즈니스에 대한 학습을 진행하면서 작지만 동작하는 일부 기능을 확실히 정복해 갈 수 있게 되는 것이죠. 이런 방식으로 팀은 매 스프린트 목표에 맞추어 성장하고, 그에 맞추어 제품도 계속 성장해 나가게 됩니다. 나아가 다음 스프린트에 전혀 다른 기능을 개발하더라도 충분히 다시 쓸 수 있는 동작하는 코드를 확보하게 됩니다. 이러한 점에서 애자일 개발 방법론은 투자한 만큼의 가치를 만드는 방식이라고 볼 수 있습니다.그림: 애자일 프로젝트에서의 기능과 시간의 관계그림: big up-front 설계가 적용된 프로젝트에서의 기능과 시간의 관계애자일 프로젝트는 시간에 비례하여 기능의 수가 증가하고, 비-애자일 프로젝트에서는 상대적으로 후반부에 기능이 집중적으로 증가함을 표현해 봤습니다.품질과 생산성어떤 코드를 보고 최고의 품질인지 판단하는 것은 매우 어려운 일입니다. 피카소조차도 더 탁월한 화가를 만난다면 자신의 작품에 몇 점을 매겨야 할지 애매할 수 있습니다.평범한 화가가 심혈을 기울인다고 하여 피카소 만큼 훌륭한 그림을 그릴 확률 역시 극히 낮을 것입니다. 이러한 논리는 개발자에게도 그대로 적용 가능합니다. 개인의 품질 역량을 10이라고 가정했을 때 평균적으로 8정도의 품질을 보여줄 확률이 높으며, 상당한 노력을 투입해야 10의 품질을 만들어 낸다고 생각할 수 있습니다. 따라서 11의 품질을 추구하게 된다면 생산성이 극적으로 저하될 수 있습니다. 기존의 코드를 개선하여 품질을 높이고자 한다면 어느 정도의 품질 개선을 목표로 해야 할까요? 만약 전체 코드를 복제한다면 품질 개선율 0%, 공정률 100%를 즉시 달성할 수 있습니다. 만약 품질 개선율 1%, 생산 공정률 100%를 달성하고자 한다면 전체 코드의 1%를 개선하는 만큼의 시간을 더 사용해야 합니다.품질을 더 개선하고 싶은데 스스로 해내기가 쉽지 않다고 판단된다면 다양한 주변 환경(동료, 메이트, 멘토, 팀, 단위 테스트 등)을 활용하여 성장 환경을 만들어 가는 것이 중요한 것 같습니다. 특히 토스페이먼츠에는 기술 논의를 즐기는 기술 덕후들이 꽤 많습니다. 내 주변이 나 때문에 활기 넘치게 만들어 보세요.핵심은 코드 리딩의 생산성대부분의 회사에는 다른 개발자들이 생산한 코드가 항상 산적해 있습니다. 어떤 기능을 개선하고 싶다면 다른 사람이 작성한 코드를 읽어야 하죠. 그래서 보통은 코드를 읽는 시간이 작성하는 시간보다 훨씬 깁니다. 따라서 읽기 좋은 코드를 만드는 것은 개발자의 삶에 굉장히 중요합니다.기존 코드를 읽는 것에 과도한 시간을 써야 한다면 기능 개선을 위한 준비 작업에만 상당한 시간을 소비하게 되어 생산성이 떨어지게 됩니다.읽기 좋은 코드를 만들어서 코드 리딩의 생산성을 향상시키는 것이 중요한 이유입니다.가능하다면 코드를 읽을 때 리팩토링 기술(Rename Method, Extract Method 등)을 활용하는 것이 좋습니다. 이러한 리팩토링을 Michael Feathers는 ‘탐색적 리팩토링(Exploratory Refactoring)’ 이라고 부르며, 이 과정에서 수정된 코드가 최종적으로 코드 저장소에 반영되지 않는다고 하더라도 충분히 가치있는 일입니다. 제가 느끼기에 Exploratory Refactoring은 정말로 효과적인 학습 프로세스이기 때문입니다.Exploratory Refactoring을 수행하게 되면 코드를 읽은 즉시 나의 해설을 표시하기 때문에 굉장히 적극적으로 코드 리딩이 되며, 코드 리딩의 주도권을 자연스럽게 리더(reader)가 가져가게 됩니다. 책을 읽었는데도 이해가 안되서 다시 읽어야 하는 것과 같은 수동적인 상태에서 벗어날 수 있게 됩니다. 따라서 Exploratory Refactoring은 탁월한 개발자가 탁월해지게 만들어주는 진정한 OP 기술입니다.리팩토링의 가치개발자들이 리팩토링의 욕구를 강하게 느낄 때는 보통 유지보수 비용이 과도한 경우입니다. 유지보수 비용이 높은 이유는 기능을 수정해야 하는데 어느 코드를 수정해야 할지, 몇 줄의 코드를 바꿔야 할지, 변경을 했다면 올바르게 변경했는지 등을 파악하기 어렵기 때문입니다.따라서 요구사항에 대응하는 코드가 어디인지 명확하게 찾아낼 수 있고, 수정한 부분의 동작이 정확한지 파악하기 쉽다면 유지보수 비용을 낮출 수 있습니다. 리팩토링의 목적은 이러한 부분을 달성하는 데 있습니다.따라서 리팩토링이 잘 수행되었다면 새로운 개발자(혹은 미래의 자신)가 기능 변경 요청을 받았을 때 아래의 3가지를 쉽게 해낼 수 있습니다.코드 위치 파악코드 수정 기능 테스트리팩토링으로 잘 설계된 코드는 이러한 핵심 과업을 쉽게 이행하는데 큰 도움이 됩니다.결론애자일 기법은 요구사항이라는 큰 덩어리를 작지만 동작하는 작은 기능으로 나누고 매 스프린트 마다 목표한 바를 착실히 정복해가는 방식이라고 할 수 있습니다.이 때 안전하게 기능을 수정 혹은 추가하기 위해서는 3개의 핵심 과업을 잘 수행해야 하는데요,코드로부터 도메인 파악하기(a.k.a. 코드 고고학)수정에 필요한 코드 파악하기수정된 코드 쉽게 검증하기토스페이먼츠에서는 애자일하게 일하는 방법을 동료들과 함께 일 하면서 쉽게 터득할 수 있는 좋은 문화와 프로세스를 만들기 위해 서툰 실험을 계속하고 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "7": "조금만 신경써서 초기 렌더링 빠르게 하기 (feat. JAM Stack)한재엽ㆍFrontend Developer2022. 2. 9들어가면서SPA(Single Page Application) 구조로 웹 프론트엔드 애플리케이션이 개발되면서 초기 렌더링 속도는 프런트엔드 개발자에게 중요한 과제 중 하나가 되었습니다. 사용자 경험에 영향을 줄 수 있는 가장 큰 요소 중 하나가 바로 속도이기 때문입니다. 이번 개선은 Web Vitals 지표를 중심으로 측정했습니다.주어진 과제들과제 1. 번들 사이즈애플리케이션에 기능이 추가되면서 번들 사이즈가 커졌고 이로 인해 초기 렌더링이 늦어지는 문제가 발생하게 됩니다. 네트워크 비용을 줄이기 위해 Webpack으로 번들링했던 소스코드를 다시 적절한 단위로 코드 스플리팅(Code Splitting)을 하기도 하고 사용되지 않는 코드, 불필요한 코드들을 덜어내기 위한 트리 세이킹(Tree Shaking)을 위한 작업을 하기도 합니다.→ [SLASH 21] 이한 – JavaScript Bundle Diet이러한 노력을 하더라도 개선할 수 있는 부분엔 한계가 존재했습니다. 초기에 렌더링되는 index.html 자체가 비어있는 문서(Document)이기 때문에 스크립트가 실행되어 실제로 렌더링이 되기까지의 시간이 존재하기 때문입니다.과제 2. 렌더링 시점그렇다면 이제 렌더링 시점을 어떻게 앞당길 것인가에 대한 문제를 해결해야 됩니다. 사용자가 tosspayments.com 에 접근했을 때, 사용자가 최종적으로 볼 수 있는 화면을 서버에서 미리 그리고 그 화면을 브라우저에 전달해주면 초기 렌더링 시점이 앞당겨지지 않을까요?물론 인터랙션이 가능해지기 까지는 하이드레이트(Hydrate) 시간이 필요하지만, 사용자 입장에서는 우선 화면이 보여지는 것이 중요합니다. 초기에 렌더링 되는 index.html이 비어있는 문서가 아니라 무언가 렌더링되어 있는 문서라면 LCP(Largest Contentful Paint) 시점을 크게 앞당길 수 있을 것입니다.JAM Stack서론이 길었는데요, 토스페이먼츠에서 만들고 있는 일부 제품에서 SSR(Server Side Rendering)없이 초기 렌더링 속도를 개선해 보았습니다. 어떤 결과를 낳았으며 어떻게 개선했는지 이야기하고자 합니다.JAM Stack이란 JavaScript와 Markup에 해당하는 HTML, CSS 정적 리소스들을 활용하여 웹 애플리케이션을 구성하는 스택을 말합니다. 그리고 이 정적 리소스들을 CDN(Content Delivery Network)에 배포하여 서버 관리를 최소화 할 수 있습니다.토스페이먼츠에서는 AWS S3, CloundFront, Lambda@edge 를 사용하여 인프라를 운영하고 있습니다.→ JAM Stack에 대해 더 알아보기SSGStatic Site Generation이라는 개념인데요, 앱을 빌드하는 시점에 미리 그려두고 이를 서빙(serving)하는 방식을 말합니다. JAM Stack에서 정적 리소스를 생성하는 용도로 사용합니다.컴파일 단계에서 미리 그릴 수 있는 부분을 최대한 그려서 사용자에게 도달하는 최초 index.html 파일이 비어있지 않도록 합니다.미리 그릴 수 있다는 것은 말 그대로 컴파일 단계에서 리액트 코드를 읽어 HTML로 렌더링 할 수 있는 부분을 말합니다. 정적인 부분을 포함하여 인증이 필요하지 않은 데이터 또한 서버로부터 가져와 미리 그릴 수 있습니다.결과 (지표)구체적인 내용을 다루기에 앞서 어느 정도의 개선이 있었는지 먼저 소개하고자 합니다. 기대한 것 이상의 결과가 나와서 매우 즐거웠던 경험이었습니다.토스페이먼츠 상점관리자 초기 로딩 화면Lighthouse 지표before개선 하기 전 지표After개선 후 지표구체적인 지표 측정Chrome Browser에서 FP(First Paint)부터 LCP(Largest Contentful Paint)까지 걸린 시간을 측정해봤습니다.before(FP → LCP: 484ms)after(FP → LCP: 0ms)Large Contents에 해당하는 것을 일단 그려버리고 시작하니 0ms입니다.최대한 그릴 수 있는 영역을 미리 그림으로써 사용자는 흰 화면을 마주하지 않고 바로 제품을 만나는 것과 같은 느낌을 받을 수 있습니다.How?Next.js토스페이먼츠의 프런트엔드 애플리케이션은 Next.js 라는 프레임워크를 사용하고 있습니다. Next.js는 서버 사이드 렌더링은 물론이고 앞서 설명드린 Static Site Generate 또한 지원합니다. (Next.js Automiatic Static Optimization)Suspense우선 토스 대부분의 프런트엔드 애플리케이션 제품은 React의 Suspense를 통해 비동기를 제어하고 있으며 토스페이먼츠 제품 또한 예외가 아니었습니다. 이와 동시에 에러 핸들링 또한 ErrorBoundary를 통해 제어하면서 비동기 상황을 제어하고 있습니다.→ [SLASH 21] 박서진 – 프론트엔드 웹 서비스에서 우아하게 비동기 처리하기→ 선언적으로 에러 상황 제어하기이 Suspense를 Next.js와 함께 사용하기 위해선 약간의 추가 작업이 필요한데요, 앞서 설명드렸다시피 Next.js는 서버사이드 렌더링 또한 지원하는 프레임워크이기 때문에 Isomophic한 코드를 작성해야 합니다. 아쉽게도 Suspense는 서버사이드 렌더링이 지원되지 않습니다. (글을 작성하는 시점에 알파로 공개되어 있는 React 18에서 개선될 예정)그래서 다음과 같이 Suspense를 한번 감싸서 사용해줄 수 있습니다.import { useState, useEffect, Suspense as ReactSuspense } from 'react';\n\nexport function Suspense({ fallback, children }: ComponentProps<typeof ReactSuspense>) {\n  const [mounted, setMounted] = useState(false);\n\n  useEffect(() => {\n    setMounted(true);\n  }, []);\n\n  if (mounted) {\n    return <ReactSuspense fallback={fallback}>{children}</ReactSuspense>;\n  }\n  return <>{fallback}</>\n\n이렇게 수정된 Suspense로 제어하고 있는 컴포넌트를 SSG로 빌드하게 되면 fallback이 렌더링됩니다.다음과 같은 코드일 경우, SSG 시점엔 <Loading /> 컴포넌트만 그려지게 됩니다.function UserPage() {\n  return (\n    <Suspense fallback={<Loading />}> // <- Render!\n      <UserProfile />\n      <UserDetailInfo />\n    </Suspense>\n\n즉, 빌드 단계에서 SSG로 미리 그려주고자 했던 UserPage에는 Loading 컴포넌트만 렌더링 될 뿐, UserProfile , UserDetailInfo 컴포넌트는 전혀 렌더링 되지 않습니다. 미리 렌더링하는 것에 대한 이점을 전혀 얻지 못하게 되는 것입니다.번들 사이즈를 아무리 줄여도 사용자는 일단 로딩만 돌고 있는 흰 화면을 마주하게 되는 것입니다.컴포넌트 배치 되돌아보기우선 Suspense가 정말 필요한 컴포넌트인지, 레이아웃 영역인지 되돌아 볼 필요가 있습니다.정말 Suspense가 필요한 영역이라면 fallback 컴포넌트를 정의해줄 때 로딩 컴포넌트만 정의해주지 않는다면 어떨까요? API 응답이 돌아오고 결국 그려질 컴포넌트와 응답이 오지 않았을 경우 보여줄 이 fallback 컴포넌트를 최대한 비슷하게 구성해주는 겁니다. 그렇다면 컴파일 시점에 그릴 수 있는 영역이 늘어나지 않을까요?즉, 위와 같이 Loading 컴포넌트만 렌더링하지 않으려면 API 응답이 돌아왔을 때 그려져야 할 컴포넌트와 응답이 아직 돌아오지 않았을 때 보여줄 컴포넌트 두 벌이 최대한 비슷하게 구성되어 있어야 합니다.컴포넌트와 API를 가깝게처음 보셨던 화면에서는 총 16개의 API call이 존재합니다. 너무나 당연하게도 이 모든 API 응답은 제각각으로 올 것이고 모든 응답이 돌아오기를 기다렸다가 그려주는 것은 정말 낭비입니다.각각의 API들을 따로 격리시켜 서로의 렌더링을 block하지 않도록 합니다.데이터가 필요한 곳에서 가장 가까운 곳에서 API를 호출합니다. client caching이 이젠 너무나도 자연스럽기 때문에 이를 최대한 활용해줍니다.UserPage 컴포넌트의 구조를 다음과 같이 변경해 볼 수 있습니다.function UserPage() {\n  return (\n    <Layout>\n      <h1>사용자 정보</h1>\n      <dl>\n        <dt>이름</dt>\n        <Suspense fallback={<dd>Loading</dd>}>\n          <UserName />\n        </Suspense>\n      </dl>\n      <h2>사용자 상세 정보</h2>\n      <Suspense fallback={<div>Loading</div>}>\n        <UserDetailInfo />\n      </Suspense>\n    </Layout>\n  )\n}페이지 컴포넌트(UserPage) 전체를 감싸고 있던 Suspense 컴포넌트가 사라지고 비동기로 처리되는 영역이 좁게 정의가 되었습니다. 또한 비동기 처리 과정 중 노출되는 컴포넌트의 모습도 원래 보여질 컴포넌트와 비슷하게 정의해줬습니다.디자인이 필요한 영역이 늘었어요. API를 호출하고 기다리는 순간에 대해서도 디자인이 필요해요. 그대로 컴포넌트도 만들어줘야 하고 그만큼 손도 많이 갑니다. 하지만 서버 관리하는 비용보다 더 신경써줄 필요는 없다고 생각합니다.조금만 신경쓰더라도 많은 개선을 볼 수 있는 방법입니다.더 나아가기지난 Next.js Conf에서 공식적으로 React의 Server Component를 사용한 렌더링 방식이 공개되었습니다. React 18도 알파 단계이니 프런트엔드 애플리케이션을 개발하면서 성능 상 이점을 많이 챙길 수 있는 환경으로 뒤바꿈 될 것 같습니다. ISR 방식과 컴포넌트 단위의 캐싱이 적용되어 웹이 더 빨라질 수 있을 것이라 기대합니다.마무리초기 로딩 속도가 중요한 것은 비즈니스에도 영향을 미치기 때문입니다. web.dev에서 초기 로딩 속도를 개선하여 성과가 개선된 사례가 소개된 바 있습니다.(https://web.dev/vitals-business-impact/)당장에 SSR 도입이 쉽지 않은 상황이라면 SSG를 통한 초기 렌더링을 최적화 할 수 있습니다.토스 팀은 계속해서 초기 로딩 속도를 계속해서 개선 중입니다. 곧 있을 SLASH22에서는 ‘매달, 유저가 기다리는 시간을 2.3년씩 아낄 수 있는 초기 렌더링 개선기 (feat. SSR)’라는 제목으로 초기 렌더링 개선 경험을 공유할 예정이니 많은 관심 부탁드립니다.👉 토스페이먼츠 프런트엔드 챕터에 대해 더 알아보기감사합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "8": "Kotlin으로 DSL 만들기: 반복적이고 지루한 REST Docs 벗어나기한규주ㆍServer Developer2022. 4. 11REST Docs 테스트 코드량을 70% 줄여주는 DSL 개발기읽는 데 걸리는 시간: 6분DSLDomain Specific Languages(DSL)은 코드의 내부 로직을 숨기고 재사용성을 올려줍니다. 어떤 경우는 비 개발자가 사용하도록 고안되는 경우도 있어서, 일반적인 프로그래밍 언어보다 훨씬 쉬운 사용성을 가집니다. 핵심은 해당 도메인을 아는 사람이면 누구나 쉽게 해당 도메인을 제어할 수 있도록 DSL을 제공하는것이 목적이며, 그렇기 때문에 프로그래밍 언어가 아닌 일반적인 언어에 가깝도록 호출 방식을 설계합니다. 때문에 DSL 호출 내부에서 어떤 로직이 작동하는지는 사용자가 알도록 할 필요가 없으며 훨씬 더 간결하고 빠르게 코드를 작성할 수 있습니다.Spring REST Docs, 더 쉽고 간결하게 쓸 수 없을까토스페이먼츠에서는 API docs를 REST Docs를 사용해서 작성할 수 있도록 권장하고 있습니다. docs를 작성하는 행위 자체에서부터 API를 통합테스트할 수 있다는 점이 매력적이며, 인터페이스의 의도치 않은 변경을 감지할 수 있다는 장점이 있습니다. 문제는 독스를 작성할 때마다 테스트 코드를 작성해줘야 하기 때문에 Swagger 보다 더 번거롭게 작업하게 된다는 문제가 있습니다.이 글에서는 DSL을 통해서 API 인터페이스의 안정성과 개발자의 생산성을 모두 가져갈 수 있는 방법을 소개합니다.REST Docs DSL먼저 기존의 작성법(AS-IS)과 DSL을 이용한 작성법(TO-BE)을 비교해보겠습니다.AS-IS.TO-BE.한 눈에 봐도 간결해보이지 않나요? AS-IS에서 볼 수 있듯, 기존의 작성법은 여러 문제가 있습니다.반복적인 코드 호출이 많음. 기존 작성법으로 작성할 때마다 생산성 저하를 느꼈습니다. API를 만드는 시간만큼이나 docs를 생성하는 시간이 걸린다니, 이것 참 비효율이지 않나요?코드가 장황하여 읽히지 않음. 인터페이스에 변화가 생기면 REST Docs 테스트 코드를 수정해야 하는데, 어떤 코드를 수정해야 하는지 빠르게 찾기가 어려웠습니다. 즉 해당 코드가 무엇을 수행하는지 한번에 읽기가 힘들고, 이 코드 수행 결과가 어떤 docs를 만들어낼지 단번에 떠올리기 어렵다는 단점이 있었습니다.첫 번째 단점은 기존의 다른 코드로부터 복붙으로 시간을 좀 줄여낼 수는 있었지만, 두 번째 단점은 참 신경 쓰였습니다. 저는 JSON과 같은 간결한 구조로부터 docs를 테스트하는 코드가 만들어지길 원했습니다.Kotlin으로 DSL 만들기다행히도 Kotlin은 여러 함수 선언 방식이 존재하여서, 이런 문제를 풀기에 매우 좋습니다. Kotlin의 테스트 코드 라이브러리인 Kotest와 MockK이 대표적인 사례라고 생각합니다.infix 함수Infix Notation (kotlinlang.org)잘 만들어진 DSL은 인간의 자연어를 사용하듯이 자연스럽게 쓰고 읽힐 수 있어야 한다고 생각합니다. Kotlin의 infix notation은 이 목표를 달성하기에 최적의 도구입니다.\"data.businessId\" type NUMBER는 \"data.businessId\".type(NUMBER)와 동일한 효과를 낳습니다.infix fun String.type(                    // (1)\n    docsFieldType: DocsFieldType\n): Field {                                // (2)\n    ...                                   // (3)\n}(1):infix notation으로 해당 함수를 선언해줍니다.type이라는 함수는 String을 receiver로 받는 함수입니다.파라미터는 docsFieldType 하나만 받습니다 (DocsFieldType는 아래에서 서술합니다.)(2): 원래 restdocs가 제공하던 FieldDescriptor를 유연하게 다루기 위해 Field라는 Wrapper 클래스를 정의합니다.(3): 원래의 RestDocs를 만들던 동작을 수행합니다infix 함수를 사용할때는 제한사항이 있습니다.호출할때는 receiver와 parameter가 명시적으로 있어야 함 (this로 암시적인 전달 불가능)parameter는 하나여야 함 (default value도 지정할 수 없음)그래야만 \"data\" type OBJECT 처럼 간결한 구조를 만들어 낼 수 있기 때문입니다.DocsFieldTypeREST Docs에서는 응답, 요청 필드의 type을 JsonFieldType으로서 구분합니다.여기에 저는 자주 사용하는 format인 Date, DateTime을 쉽게 정의할 방법을 찾고 싶었고, enum class도 간단히 전달하여 어떤 필드가 사용될 수 있는지 docs에 쉽게 표기하고 싶었습니다. date, datetime, enum은 모두 JsonFieldType.STRING이지만 format과 sample이 다르게 표시될 필요가 있는 특이 케이스이기 때문입니다.이런 식으로 정의한다면 아래 예시와 같이 간단하게 Field를 생성해내면서 DocsFieldType을 정의해낼 수 있습니다.\"data\" type OBJECT\n\"id\" type NUMBER\n\"createdAt\" type DATETIMEDocsFieldType - enum다만 enum을 정의하고 싶을때는 조금 디테일이 필요합니다.\"companyType\" type STRING example CompanyType::class로도 선언할 수는 있지만 매번 example을 호출해주는 건 조금 귀찮습니다. 어차피 enum이 string이라는건 누구나 다 아는 사실인데 두 함수 호출을 나눠야 할까요?\"companyType\" type ENUM(CompanyType::class)훨씬 간결해졌습니다.다음과 같이 DocsFieldType을 확장한 sealedSubclass를 만든다면 위와 같은 dsl 작성이 가능합니다.data class ENUM<T : Enum<T>>(val enums: Collection<T>) : DocsFieldType(JsonFieldType.STRING) {\n  constructor(clazz: KClass<T>) : this(clazz.java.enumConstants.asList())   // (1)\n}(1): secondary constructor 덕분에 모든 enum값이 아니라 특정 조건에 맞는 enum 값을 collection으로 넘길수도 있습니다.ex) 개인사업자에 해당하는 companyType만 해당 필드에 존재할 수 있을 때 \"individualCompanyType\" type ENUM(CompanyType.values().filter { it.isIndividual() })이로써 type infix 함수는 아래와 같이 완성할 수 있습니다.infix fun String.type(docsFieldType: DocsFieldType): Field {\n    val field = createField(this, docsFieldType.type)\n    when (docsFieldType) {\n        is DATE -> field formattedAs RestDocsUtils.DATE_FORMAT\n        is DATETIME -> field formattedAs RestDocsUtils.DATETIME_FORMAT\n        else -> {}\n    }\n    return field\n}\n\ninfix fun <T : Enum<T>> String.type(enumFieldType: ENUM<T>): Field {\n    val field = createField(this, JsonFieldType.STRING, false)\n    field.format = EnumFormattingUtils.enumFormat(enumFieldType.enums)\n    return field\n}\n\nprivate fun createField(value: String, type: JsonFieldType, optional: Boolean): Field {\n    val descriptor = PayloadDocumentation.fieldWithPath(value)\n        .type(type)\n        .attributes(RestDocsUtils.emptySample(), RestDocsUtils.emptyFormat(), RestDocsUtils.emptyDefaultValue())\n        .description(\"\")\n\n    if (optional) descriptor.optional()\n\n    return Field(descriptor)\n}\n\nField 클래스에서 DSL 확장하기이제 좀 더 욕심을 내봅시다. 위 예시처럼 얼마든지 함수 호출을 chaining할 수 있습니다.어떤가요? 괄호로 계속 호출하는 것보다 좀 더 직관적이지 않나요?type이라는 infix function이 Field를 반환할 수 있도록 했으니, Field에서 더 많은 DSL을 호출하도록 확장할 수 있게 되었습니다.open class Field(\n    val descriptor: FieldDescriptor,\n) {\n    val isIgnored: Boolean = descriptor.isIgnored\n    val isOptional: Boolean = descriptor.isOptional\n\n    protected open var default: String\n        get() = descriptor.attributes.getOrDefault(RestDocsAttributeKeys.KEY_DEFAULT_VALUE, \"\") as String\n        set(value) {\n            descriptor.attributes(RestDocsUtils.defaultValue(value))\n        }\n\n    protected open var format: String\n        get() = descriptor.attributes.getOrDefault(RestDocsAttributeKeys.KEY_FORMAT, \"\") as String\n        set(value) {\n            descriptor.attributes(RestDocsUtils.customFormat(value))\n        }\n\n    protected open var sample: String\n        get() = descriptor.attributes.getOrDefault(RestDocsAttributeKeys.KEY_SAMPLE, \"\") as String\n        set(value) {\n            descriptor.attributes(RestDocsUtils.customSample(value))\n        }\n\n  \topen infix fun means(value: String): Field {\n        return description(value)\n    }\n\n    open infix fun attributes(block: Field.() -> Unit): Field {\n        block()\n        return this\n    }\n\n    open infix fun withDefaultValue(value: String): Field {\n        this.default = value\n        return this\n    }\n\n    open infix fun formattedAs(value: String): Field {\n        this.format = value\n        return this\n    }\n\n    open infix fun example(value: String): Field {\n        this.sample = value\n        return this\n    }\n\n    open infix fun isOptional(value: Boolean): Field {\n        if (value) descriptor.optional()\n        return this\n    }\n\n    open infix fun isIgnored(value: Boolean): Field {\n        if (value) descriptor.ignored()\n        return this\n    }\n}\n\n이렇게 얼마든지 코드를 확장해나갈 수 있을뿐더러, 해당 프로젝트에서 사용하는 REST Docs snippet의 attribute를 코드 상으로 좀 더 명확하게 정의할 수 있게 되었습니다.마무리이 글은 REST Docs의 반복적인 코드를 제거하고, docs의 생성이라는 본래의 목적을 달성하고자 기존 MockMvc 테스트코드 작성법에서 벗어나, REST Docs DSL을 만드는 방식으로 문제를 해결하고자 했습니다.우리가 흔히 쓰는 gradle configuration 작성 방식인 build.gradle.kts 또한 org.gradle.kotlin.dsl에서 그 선언 방식을 찾아볼 수 있고, MockK이나 Kotest에서도 다양한 방식으로 Kotlin의 장점을 최대한 끌어낸 모습을 확인할 수 있습니다.build.gradle.kts (https://github.com/gradle/kotlin-dsl-samples)MockK의 every(https://mockk.io/#dsl-examples),Kotest의 여러 Testing Styles(https://kotest.io/docs/framework/testing-styles.html)혹시나 여러분도 반복적인 작업을 일일히 복붙으로 하고 있다면 여러분의 팀만을 위한 DSL을 만들어보는 건 어떨까요?이 REST Docs DSL은 토스페이먼츠 *엔지니어링 데이에 장태영(Server Developer, taeyoung.jang@tosspayments.com)님과 함께 만들었습니다.토스페이먼츠에서는 매주 목요일에 엔지니어링 데이를 진행하고 있어요. 이 시간에는 평소 업무에 병목이 되는 문제들을 해결하거나, 인프라를 개선하는 등의 작업을 진행합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "9": "에러 핸들링을 다른 클래스에게 위임하기 (Kotlin 100% 활용)한규주ㆍServer Developer2022. 5. 14TL;DRResult를 이해한다면, MSA 환경에서 에러가 전파되지 않도록 막을 수 있습니다.runCatching과 Result를 사용하면 에러 핸들링을 클라이언트에게 위임할 수 있습니다.예제: 로그인 요청을 전달하는 서비스 흐름에서 에러 처리하기아래와 같은 서비스 호출 흐름이 있다고 가정해보겠습니다.Server A 입장에서는 Server B에서 발생하는 에러 처리를 해야하는 고민에 빠집니다.API를 호출하는 코드에서 API의 에러 응답에 따른 비즈니스 로직을 다르게 가져가고 싶은 경우가 있습니다. 예를 들어 위 사례에서 비밀번호가 틀리거나 이메일 주소가 틀린 경우 이 에러를 캐치해서 다른 메세지를 던지고 싶을 수 있고, 어떤 코드에서는 그 에러를 무시하고 다른 로직을 수행하고 싶을 수 있습니다.에러 처리를 API Client 단에서 하지 않고 다른 클래스에 위임을 하고 싶은 이런 경우에는 어떤 방법을 사용할 수 있을지 아래 코드 예시로 알아보겠습니다.// API client\n@FeignClient\ninternal interface LoginApi {\n  @PostMapping\n  fun login(\n    @RequestBody request: LoginRequestDto\n  ): OtherServiceResponse<LoginResponseDto>\n}\n\n@Component\nclass LoginApiClient internal constructor(\n  private val loginApi: LoginApi\n) {\n  fun login(request: LoginRequestDto): LoginResult {\n    return loginApi.login(request).result.toResult()\n  }\n}\n\n@Service\nclass LoginService(\n  private val loginApiClient: LoginApiClient\n) {\n  fun login(id: String, pw: String): LoginResult {\n    return try {\n      loginApiClient.login(LoginRequestDto(id, pw))\n    } catch {\n      // 에러 핸들링\n    }\n  }\n}\n\n이 경우에 아래와 같은 두 케이스를 해결하고 싶어집니다.이 API를 사용하는 쪽(ex. LoginService)에서 에러 핸들링을 강제하고 싶습니다.API 호출 로직마다 에러 핸들링을 다른 방식으로 가져가게 하고 싶습니다.LoginService가 아닌 다른 호출 로직에서는 에러를 다르게 처리하고 싶을 수 있습니다.위 고민을 해결할 방법이 있습니다. 바로 Result입니다.@Component\nclass LoginApiClient internal constructor(\n  private val loginApi: LoginApi\n) {\n  fun login(request: LoginRequestDto): Result<LoginResult> {\n    return runCatching {\n      loginApi.login(request).result.toResult()\n    }\n  }\n}\n\n@Service\nclass LoginService(\n  private val loginApiClient: LoginApiClient\n) {\n  fun login(id: String, pw: String): LoginResult {\n    return loginApiClient.login(LoginRequestDto(id, pw))\n      .onFailure {\n        // 에러 핸들링\n      }\n  }\n}코틀린의 runCatching💡 이미 runCatching을 잘 사용하고 있다면 넘겨도 좋습니다.위 코드를 이해하기에 앞서서 runCatching을 알아둘 필요가 있습니다. 코틀린은 물론 자바의 try ... catch를 동일하게 지원하지만 이와는 조금 다른 방법으로 에러 핸들링을 할 수도 있습니다.예제아래 요구사항이 있다고 가정합시다.LoginApiClient 호출 시 LoginException이 발생했는데,errorCode가 INVALID_PASSWORD 인 경우 예외를 발생시키지 않고 null을 반환한다.그 외 모든 에러 상황에서는 예외를 발생시킨다.try ... catch를 사용했을때try {\n  loginApiClient.login(request)\n} catch (e: LoginException) {\n  if (e.errorCode == \"INVALID_PASSWORD\") {\n    return null\n  } else {\n    throw e\n  }\n}Java에서 위와 같이 작성하는 코드를 runCatching을 사용하면 아래처럼 표현할 수 있습니다.runCatching을 사용했을 때return runCatching {\n  loginApiClient.login(request)\n}.onFailure { e ->\n  if (e.errorCode != \"INVALID_PASSWORD\") throw e\n}.getOrNull()kotlin.runCatching@InlineOnly\n@SinceKotlin(\"1.3\")\npublic inline fun <R> runCatching(block: () -> R): Result<R> {\n  return try {\n    Result.success(block())\n  } catch (e: Throwable) {\n    Result.failure(e)\n  }\n}try..catch 로직을 그대로 사용하지만 Result로 감싸서 반환하는 것을 알 수 있습니다.에러가 발생하지 않았을 때에는 Result.success 반환에러가 발생했을 때에는 Result.failure 반환Result가 뭔가요?Result가 무엇인지 알아보기 위해서 Kotlin 1.3 표준 라이브러리의 코드를 살펴봅시다.@SinceKotlin(\"1.3\")\n@JvmInline\npublic value class Result<out T> @PublishedApi internal constructor(\n  @PublishedApi\n  internal val value: Any?\n) : Serializable {\n\n  public val isSuccess: Boolean get() = value !is Failure\n\n  public val isFailure: Boolean get() = value is Failure\n\n  /* ... */\n\n  public companion object {\n    @Suppress(\"INAPPLICABLE_JVM_NAME\")\n    @InlineOnly\n    @JvmName(\"success\")\n    public inline fun <T> success(value: T): Result<T> =\n      Result(value)\n\n    @Suppress(\"INAPPLICABLE_JVM_NAME\")\n    @InlineOnly\n    @JvmName(\"failure\")\n    public inline fun <T> failure(exception: Throwable): Result<T> =\n      Result(createFailure(exception))\n  }\n\n  internal class Failure(\n    @JvmField\n    val exception: Throwable\n  ) : Serializable {\n    /* ... */\n  }\n}\n\n즉, Result의 value는성공일 경우 T를 타입으로 하는 값을 가지게 되고실패일 경우는 Failure를 wrapper class로 하는 exception을 값으로 가지게 됩니다.Result가 제공하는 함수들은 다음과 같습니다.inline fun <T> Result<T>.getOrThrow(): T\n\ninline fun <R, T : R> Result<T>.getOrElse(\n  onFailure: (exception: Throwable) -> R\n): R\n\ninline fun <R, T : R> Result<T>.getOrDefault(defaultValue: R): R\n\ninline fun <R, T> Result<T>.fold(\n  onSuccess: (value: T) -> R,\n  onFailure: (exception: Throwable) -> R\n): R\n\ninline fun <R, T> Result<T>.map(transform: (value: T) -> R): Result<R>\n\nfun <R, T> Result<T>.mapCatching(transform: (value: T) -> R): Result<R>\n\ninline fun <R, T : R> Result<T>.recover(transform: (exception: Throwable) -> R): Result<R>\n\ninline fun <T> Result<T>.onFailure(action: (exception: Throwable) -> Unit): Result<T>\n\ninline fun <T> Result<T>.onSuccess(action: (value: T) -> Unit): Result<T\n\nResult 사용 예시runCatching은 Result<T>를 반환하게 되는데, Result가 제공하는 함수를 이용해서 다양하게 활용할 수 있습니다.에러를 무시하고 null 반환val response = runCatching {\n  login()\n}.getOrNull()기본값 반환val response = runCatching {\n  login()\n}.getOrDefault(emptyList())에러 발생 시 다른 동작 수행val response = runCatching {\n  login()\n}.getOrElse { ex ->\n  logger.warn(ex) { \"에러 발생\" }\n\n  // 에러를 던지고 싶다면\n  throw ex\n}에러가 발생한 경우에만 해당 에러 객체 반환val exception = runCatching {\n  login()\n}.exceptionOrNull()\n\n// 위에서 받은 에러로 로직 수행\nwhen (exception) {\n  /* ... */\n}에러가 발생하는지 아닌지만 확인하고 싶을 때에도 유용할 수 있습니다.val isValidCredential = runCatching { tryLogin() }.exceptionOrNull() != null성공/에러 시 각각 특정 동작 수행 후 에러 던지기val response = runCatching {\n  login()\n}.onSuccess {\n  logger.info(\"성공!\")\n}.onFailure {\n  logger.info(\"실패!\")\n}.getOrThrow()runCatching으로 try .. finally 구현하기runCatching {\n  request()\n}.also {\n  doSomething()\n}.getOrThrow()Result를 사용해서 예외 처리를 다른 클래스에 위임하기runCatching을 사용하면 Result가 제공하는 다양한 함수의 편의에 기댈 수 있다는 것을 배웠습니다.Result에 대한 처리를 즉시 하지 않고 함수의 반환 값으로 반환하게 된다면, Result에 대한 핸들링을 다른 클래스에 위임할 수도 있습니다.LoginApiClient@Component\nclass LoginApiClient internal constructor(\n  private val loginApi: LoginApi\n) {\n  fun login(request: LoginRequestDto): Result<LoginResult> {\n    return runCatching {\n      loginApi.login(request).result.toResult()\n    }\n  }\n}\n\nResult를 반환하여 다른 클래스가 에러 핸들링을 하도록 위임합니다.LoginService@Service\nclass LoginService(\n  private val loginApiClient: LoginApiClient\n) {\n  fun login(id: String, pw: String): LoginResult? {\n    return loginApiClient.login(LoginRequestDto(id, pw))\n      .getOrNull()\n  }\n}\n\n에러가 발생한 경우 에러를 무시하고 기본값으로 null을 반환합니다.하지만 아래처럼 다른 컴포넌트에서는 에러를 핸들링하고 싶을 수도 있습니다.PasswordChangeService@Component\nclass PasswordChangeService(\n  private val loginApiClient: LoginApiClient,\n  private val errorStatusWriter: ErrorStatusWriter,\n  private val passwordChanger: PasswordChanger\n) {\n  fun change() {\n    loginApiClient.login(request)\n      .onFailure { exception ->\n        errorStatusWriter.write(exception)    // (1)\n      }.onSuccess { loginResult ->\n        passwordChanger.change(loginResult)   // (2)\n      }.getOrThrow()                          // (3)\n  }\n}[1] 에러가 발생한 경우 에러를 기록합니다.[2] 성공한 경우 해당 값을 받아서 다른 컴포넌트를 호출합니다.→ [1], [2]번 두 케이스는 배타적이고 동시에 일어날 수 없습니다.[3] 그리고 에러인 경우 예외를 발생시킵니다.결론정리하자면 Result(runCatching)는 다음의 용도에서 사용할 수 있습니다.외부 서비스에 의존하는 로직이라 예외 발생 가능성이 빈번한 컴포넌트해당 컴포넌트에서 에러가 발생할 수 있다는 것을 클라이언트에게 알려주고 싶을 때, 에러 핸들링을 다른 컴포넌트에 강제하고 위임하고 싶을 때try ... catch를 쓰고 싶지 않을 때재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "10": "테스트 의존성 관리로 높은 품질의 테스트 코드 유지하기양권성ㆍServer Developer2022. 6. 9테스트 코드는 애플리케이션 코드 못지 않게 높은 품질을 유지해야 합니다.낮은 품질(이해하기 어려운 코드, 여기저기 깨져있는 테스트)의 테스트는 유지보수가 어렵고 기술부채에 못지 않은 부채로 다가옵니다.그래서 테스트 코드의 높은 품질을 유지하기 위해 다양한 Builder, Helper 클래스들이 나오게 되고, 테스트 전용으로 의존성을 추가하기도 합니다. 하지만 이 또한 관리의 대상이며 제대로 관리하지 않으면 중복 코드와 얼기설기 얽힌 의존성 지옥을 맛보게 됩니다.이 포스트에서는 Gradle의 java-test-fixtures 플러그인을 사용하여 위 문제를 해결하는 방법에 대해 설명합니다.TL;DRGradle의 java-test-fixtures 플러그인을 사용하면 테스트용으로 작성한 Builder, Helper 클래스 등등을 다른 모듈과 공유할 수 있습니다.추가적으로 해당 모듈의 테스트 전용 의존성까지 전파시킬 수 있어 각 모듈마다 불필요한 테스트 전용 의존성들을 일일이 추가할 필요가 사라집니다.프로젝트 구조예제를 이해하기 쉽게 하기 위해 프로젝트 구조(멀티 모듈)를 가정하고 이야기를 진행하겠습니다.domain 모듈: 핵심 비즈니스 로직에만 관심이 있는 모듈, 외부(써드파티 라이브러리, DB, HTTP 등등)에 의존하지 않고 온전히 비즈니스 로직에만 관심을 갖고 있는 모듈로써 어떠한 의존성도 가지지 않습니다.db 모듈: 데이터의 CRUD(저장, 조회, 수정, 삭제)에만 관심이 있는 모듈, 클라이언트의 요구사항을 처리하기 위해 domain 모듈에 의존(implementation)하고 있습니다.이미지 출처: Gradle Docs// db 모듈의 build.gradle.kts\ndependencies {\n    implementation(project(\":domain\"))\n    // 기타 디펜던시들...\n}application 모듈: 클라이언트의 요청을 받아 처리하는 모듈, 클라이언트의 요구사항을 처리하기 위해 domain 모듈에 의존(implementation)하고 있으며, application 모듈에 main 함수가 존재하기 때문에 데이터 조작(저장, 조회 등등)을 위해 db 모듈에도 의존(runtimeOnly)하고 있습니다.이미지 출처: Gradle Docs// application 모듈의 build.gradle.kts\ndependencies {\n    implementation(project(\":domain\"))\n    runtimeOnly(project(\":db\"))\n    // 기타 디펜던시들...\n}테스트 전용으로 작성한 클래스를 다른 모듈에게 노출시키기domain 모듈에 아래와 같은 객체가 있다고 가정해보겠습니다.class Order(\n    val id: String,\n    val description: String,\n    val amount: Long\n)테스트에서 위 클래스를 사용해야할 때 객체를 생성하려고 생각하면 매우 번거로워집니다. (공감이 되지 않는다면 파라미터가 10개 정도 된다고 생각해보면 됩니다.)이 때 모든 파라미터에 기본값을 넣는 절충안도 존재하는데, 객체의 필수값이 기본값으로 채워진 채 객체가 생성되면 불안정하게 동작할 수 있습니다. 누군가의 실수로 프로덕션에서 객체의 필수값 중 일부가 기본값으로 생성된다면 의도치 않은 동작을 하게 될 수도 있기 때문입니다.따라서 테스트에서 사용할 목적으로 디폴트 값이 들어간 빌더 객체를 만들게 됩니다.참고로 IntelliJ IDEA에서 코틀린 클래스의 빌더를 만들어주는 플러그인은 kotlin-builder-generator를 사용하면 손 쉽게 만들 수 있습니다.data class OrderBuilder(\n    val id: String = \"\",\n    val description: String = \"\",\n    val amount: Long = 0L\n) {\n    fun build(): Order {\n        return Order(\n            id = id,\n            description = description,\n            amount = amount\n        )\n    }\n}하지만 빌더는 테스트에서만 사용해야하기 때문에 domain/src/test 디렉토리 밑에 생성해야합니다. test가 아닌 main 디렉토리 밑에 존재하게 되면 프로덕션 코드에서 누가 해당 빌더로 온전치 않은 상태의 객체를 생성하고 사용하는 실수를 할 수 있기 때문입니다.이런 Builder나 Helper 같이 테스트 전용으로 만든 클래스들을 해당 클래스가 존재하는 모듈(domain 모듈)이 아닌 해당 모듈을 의존하고 있는 다른 모듈(domain 모듈에 의존하고 있는 application, db 모듈)의 테스트에서 사용하고 싶다는 니즈가 생겼다고 가정해보겠습니다.하지만 application과 db 모듈에서 domain 모듈에 의존하고 있다고 할지라도 각 모듈의 테스트에서는 OrderBuilder를 import 할 수 없습니다.build된 jar 파일의 압축을 해제했을 때 나오는 결과물을 보면 main 디렉토리 밑에 있는 Order 클래스는 포함하고 있지만, test 디렉토리 밑에 있는 OrderBuilder 클래스는 포함하고 있지 않기 때문입니다.어떻게 생각해보면 당연한 결과입니다.domain 모듈을 테스트하는데 필요한 정보들은 프로덕션 코드에서는 필요가 없고, 그렇기 때문에 굳이 불필요하게 테스트 전용 클래스들까지 포함시킬 필요는 없기 때문입니다.이제 문제를 해결하기 위한 간단한 방법 두 가지를 떠올리게 됩니다.각 모듈의 test 디렉토리에 빌더를 복사/붙여넣기 합니다. 하지만 이는 코드의 중복을 유발하며 Order 클래스의 변경사항이 생겼을 때 각 모듈에 존재하는 OrderBuilder 클래스를 각각 수정해야한다는 번거로움이 존재합니다.Builder/Helper를 모아놓은 별도의 test-data 같은 테스트 전용 모듈을 만들고, 각 모듈에서 test-data 클래스에 의존(testImplementation)하게 만듭니다.// application/db 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    testImplementation(project(\":test-data\"))\n}하지만 이는 실제 소스코드(Order는 domain 모듈에 존재)와 거리가 멀어지게 만들어(OrderBuilder는 test-data 모듈에 존재) 응집도가 떨어지는 모듈이 나오게 됩니다.또한 테스트 전용임에도 불구하고 test-data 모듈의 클래스들을 외부에 노출시켜야하기 때문에 test 디렉토리가 아닌 main 디렉토리에 둬야 하는 점도 약간의 혼란(’main 디렉토리에 있으니까 프로덕션 레벨에서 사용하는 건가…?’ 하는 정도의)을 유발할 수 있습니다.둘 다 좋은 방법은 아니라는 생각이 듭니다. 이 문제를 해결하기 위한 빛과 소금과 같은 존재가 있습니다.구세주: java-test-fixtures 플러그인Gradle에는 이런 문제를 해결하고자 java-test-fixtures 플러그인이 존재합니다.우선 외부에 노출시키고자 하는 Builder나 Helper 클래스가 존재하는 domain 모듈의 build.gradle.kts 파일에 플러그인을 추가해주고 프로젝트를 reload 하면 됩니다.// domain 모듈의 build.gradle.kts\nplugins {\n    // 기타 플러그인들...\n    `java-test-fixtures`\n}java-test-fixtures 플러그인이 적용된 모듈에서 디렉토리를 생성하려고 하면 IntelliJ IDEA에서는 testFixtures 디렉토리가 자동완성 됩니다.그럼 아까 생성했던 OrderBuilder 클래스는 test가 아닌 testFixtures 디렉토리로 이동시켜준 후 build를 했을 때 수행되는 Gradle Task들을 보게 되면 testFixture 관련된 task가 추가된 걸 알 수 있습니다../gradlew :domain:build\n\n...\n> Task :domain:compileTestFixturesKotlin\n> Task :domain:compileTestFixturesJava NO-SOURCE\n> Task :domain:processTestFixturesResources NO-SOURCE\n> Task :domain:testFixturesClasses UP-TO-DATE\n> Task :domain:testFixturesJar\n\n그리고 빌드된 결과물을 보면 test-fixtures.jar가 추가된 걸 볼 수 있습니다.plain.jar는 plain에, test-fixtures.jar는 test에 각각 풀었는데 OrderBuilder는 test에 존재하는 걸 보니 test-fixtures.jar에 존재한다는 걸 알 수 있습니다.여기서 또 java-test-fixtures 플러그인의 장점이 나오게 되는데 다른 모듈에서 불필요하게 여기는 클래스들(test 디렉토리에 있는 @Test 어노테이션이 붙은 테스트 코드들 등등)은 노출되지 않고, 필요한 클래스들(testFixtures 디렉토리에 있는 Helper나 Builder 클래스 등등)만 노출된다는 점입니다.하지만 이렇게 했다고 해서 아직 application이나 db 모듈에서 OrderBuilder를 import 할 수 있는 건 아닙니다. application과 db 모듈에서는 plain.jar에 의존하고 있는 것이지, test-fixtures.jar에 의존하고 있는 건 아니기 때문입니다.따라서 application과 db 모듈에서 test-fixtures.jar에 의존하도록 각 모듈의 build.gradle.kts에 추가해줘야합니다.// application/db 모듈의 build.gradle.kts\ndependencies {\n    implementation(project(\":domain\"))\n    testImplementation(testFixtures(project(\":domain\")))\n    // 기타 디펜던시들...\n}위와 같이 의존성을 추가해줘야 비로소 application과 db 모듈의 테스트 코드에서도 domain 모듈의 testFixtures에 존재하는 OrderBuilder를 사용할 수 있게 됩니다.이해하기 쉽게 모듈 간의 디렉토리 관계를 좀 더 세분화해서 표현해보았습니다.테스트 전용으로 추가한 의존성을 다른 모듈에게 노출시키기db 모듈의 통합테스트를 위해 인메모리 DB인 H2를 테스트 전용으로 의존성을 추가했다고 가정해보겠습니다.이미지 출처: Gradle Docs// db 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    testRuntimeOnly(\"com.h2database:h2\")\n}이 상태에서 db 모듈의 통합테스트를 돌리게 되면 H2 DB를 사용하여 실제 DB와 격리된 환경에서 테스트가 돌아가는 것을 볼 수 있습니다.그리고 application 모듈은 아래와 같이 db 모듈에 의존하고 있기 때문에 통합테스트를 작성할 때도 인메모리 DB를 쓸 것이라 희망하게 되는데 실제로 테스트를 짜고 돌려보면 그렇지 않습니다.// application 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    runtimeOnly(project(\":db\"))\n}gradle 모듈의 디펜던시를 보게 되면 db 모듈의 testRuntimeClasspath에는 H2가 존재하지만, application 모듈의 testRuntimeClasspath에 존재하는 db 모듈에는 H2가 존재하지 않기 때문입니다.이 때도 application 모듈의 build.gradle.kts에 H2를 의존성으로 추가하는 방법이 있겠지만 관심사 문제가 있습니다. application 모듈의 관심사는 ‘어떻게 클라이언트와 커뮤니케이션해서 요구사항을 만족시킬 것인가?’이지 세부적인 내용(’저장소는 무엇을 쓸까? 데이터는 어디서 저장하고 어떻게 불러올까?’ 같은)은 관심사가 아닙니다. 따라서 H2를 직접적으로 의존성을 추가하는 순간 관심사 분리가 제대로 되지 않게 됩니다.이 문제를 해결하기 위해 또 우리의 구세주 java-test-fixtures 플러그인이 필요합니다.testFixturesComplieClasspath와 testFixturesRuntimeClasspath우선 외부에 테스트 전용 의존성(H2)을 노출시키고 싶은 db 모듈에 java-test-fixtures 플러그인을 추가하고, testRuntimeOnly로 추가했던 H2 의존성을 testFixturesRuntimeOnly로 변경해줘야 합니다.// db 모듈의 build.gradle.kts\nplugins {\n    // 기타 플러그인들...\n    `java-test-fixtures`\n}\n\ndependencies {\n    // 기타 디펜던시들...\n    testFixturesRuntimeOnly(\"com.h2database:h2\")\n}그리고 나서 다시 db 모듈의 디펜더시를 보면 기존에 보지 못했던 testFixturesCompileClasspath와 testFixturesRuntimeClasspath가 추가된 게 보입니다.사실 두 가지 클래스패스는 java-test-fixtures 플러그인을 추가하기만 해도 추가되는 클래스패스입니다.여기서 눈여겨봐야할 것은 기존에는 testRuntimeClasspath에만 존재하던 H2 의존성이 testFixturesRuntimeClasspath에도 추가된 점입니다.이에 대한 해답은 java-test-fixtures 플러그인 문서를 보다보면 아래와 같은 내용에 나오게 됩니다.Test fixtures are configured so that: • they can see the main source set classes • test sources can see the test fixtures classes두 번째로 나와있는 테스트 소스(test 디렉토리에 있는 내용들)에서 test fixture(testFixtures 디렉토리에 있는 내용들)에 있는 내용을 참조(can see)할 수 있도록 구성된다는 내용이 핵심입니다.따라서 testFixturesRuntimeOnly로만 추가(testFixturesRuntimeClassPath)했지만 testRuntimeOnly로도 추가된 것과 동일한 효과(testRuntimeClasspath에 추가된 효과)를 같이 보게 됩니다. 따라서 db 모듈의 통합테스트를 돌렸을 때는 여전히 H2 DB를 사용하게 됩니다.하지만 H2를 db 모듈에 testFixturesRuntimeClasspath에 추가했지만, 여전히 application 모듈의 testRuntimeClasspath를 보면 아직도 db 모듈에는 H2 의존성이 추가되지 않은 모습을 볼 수 있습니다.그 이유는 application 모듈의 build.gradle.kts를 보면 알 수 있습니다.// application 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    runtimeOnly(project(\":db\"))\n}이미지 출처: Gradle Docs바로 정답은 runtimeOnly 키워드에 있습니다.runtimeOnly로 추가한 디펜던시는 testRuntimeClasspath에도 추가됩니다. (물론 runtimeClasspath에도 추가됩니다.)하지만 testRuntimeClasspath에 추가된 의존성은 외부 모듈에 노출되지 않는다는 특성이 있습니다.따라서 우리는 db 모듈의 testRuntimeClasspath가 아닌 testFixturesRuntimeClasspath에 추가된 의존성들에 주목해야하며 해당 의존성들이 추가되도록 application 모듈의 build.gradle.kts를 수정해야 합니다.// application 모듈의 build.gradle.kts\ndependencies {\n    // 기타 디펜던시들...\n    runtimeOnly(project(\":db\"))\n    testRuntimeOnly(testFixtures(project(\":db\")))\n}마지막 부분이 db 모듈의 testFixturesRuntimeClasspath에 있는 의존성을 testRuntimeOnly로 추가(testRuntimeClasspath에 추가)하는 내용입니다.이제 application 모듈의 testRuntimeClasspath에도 db 모듈의 testFixutresRuntimeClasspath에 있는 H2 의존성이 추가된 걸 볼 수 있습니다.이 상태에서 application 모듈의 통합테스트를 돌리더라도 H2 DB를 사용하는 걸 볼 수 있습니다.결론테스트 코드는 실제 프로덕션에 영향을 미치지 않으므로 신경을 덜 쓰기 마련입니다. 그러다보면 중복이 난무하고 관심사 분리도 제대로 되지 않고 의존성 지옥에 빠지기 십상입니다. 하지만 테스트 코드는 우리의 소프트웨어를 좀 더 나은 설계로 유도하며 안정감도 주기 때문에 품질을 관리해야하는 소프트웨어임에는 분명합니다.혹시 해당 포스트를 보고 ‘어, 그거 그렇게 하는 거 아닌데…’라는 생각이 들었다면 토스페이먼츠에 와서 신나게 토론할 준비가 되어있으니 언제든 환영합니다!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "11": "CommonJS와 ESM에 모두 대응하는 라이브러리 개발하기: exports field장호승ㆍFrontend Developer2022. 10. 4토스 프론트엔드 챕터에서는 개발 생산성을 극대화하기 위해 코드를 지속적으로 라이브러리로 만들고 있습니다. 그 결과 지금은 100개가 넘는 라이브러리를 운영하고 있습니다.Node.js 12부터 ECMAScript Modules라는 새로운 Module System이 추가되면서, 기존의 CommonJS라는 Module System까지, 라이브러리는 두 가지 Module System을 지원해야 하게 되었습니다.토스팀에서는 그것을 package.json의 exports field를 통해 지원하고 있습니다. 각각의 모듈 시스템과 exports field에 대해 자세히 알아봅시다.Node.js에는 CommonJS, ECMAScript Modules(이하 CJS, ESM)라는 두 가지 모듈 시스템이 존재합니다.CommonJS (CJS)// add.js\nmodule.exports.add = (x, y) => x + y;\n\n// main.js\nconst { add } = require('./add');\n\nadd(1, 2);ECMAScript Modules (ESM)// add.js\nexport function add(x, y) {\n  return x + y\n}\n\n// main.js\nimport { add } from './add.js';\n\nadd(1, 2);CJS는 require / module.exports 를 사용하고, ESM은 import / export 문을 사용합니다.CJS module loader는 동기적으로 작동하고, ESM module loader는 비동기적으로 작동합니다.ESM은 Top-level Await을 지원하기 때문에 비동기적으로 동작합니다.따라서 ESM에서 CJS를 import 할 수는 있지만, CJS에서 ESM을 require 할 수는 없습니다. 왜냐하면 CJS는 Top-level Await을 지원하지 않기 때문입니다.이 외에도 두 Module System은 기본적으로 동작이 다릅니다.따라서 두 Module System은 서로 호환되기 어렵습니다.왜 두 Module System을 지원해야해요?서로 호환되기 어려운 두 Module System을 지원해야하는 이유는 뭘까요? 그냥 하나로 통일하면 안될까요? 토스팀에서는 왜 그것을 중요하게 생각할까요?토스팀에서는 Server-side Rendering(이하 SSR)을 적극적으로 사용하고 있기 때문에, Node.js의 CJS를 지원하는 것이 중요했습니다.그리고 Module System의 지원은 브라우저 환경에서의 퍼포먼스와도 관련이 있습니다. 브라우저 환경에서는 페이지 렌더링을 빠르게 하는 것이 중요한데, 이 때 JavaScript는 로딩되어 실행되는 동안 페이지 렌더링을 중단시키는 리소스들 중 하나 입니다.따라서 JavaScript 번들의 사이즈를 줄여서 렌더링이 중단되는 시간을 최소화 하는 것이 중요합니다. 이를 위해 필요한 것이 바로 Tree-shaking입니다. Tree-shaking이란 필요하지 않은 코드와 사용되지 않는 코드를 삭제하여 JavaScript 번들의 크기를 가볍게 만드는 것을 말합니다.이 때, CJS는 Tree-shaking이 어렵고, ESM은 쉽게 가능합니다.왜냐하면 CJS는 기본적으로 require / module.exports 를 동적으로 하는 것에 아무런 제약이 없습니다.// require\nconst utilName = /* 동적인 값 */\nconst util = require(`./utils/${utilName}`);\n\n// module.exports\nfunction foo() {\n  if (/* 동적인 조건 */) {\n    module.exports = /* ... */;\n  }\n}\nfoo();따라서 CJS는 빌드 타임에 정적 분석을 적용하기가 어렵고, 런타임에서만 모듈 관계를 파악할 수 있습니다.하지만 ESM은 정적인 구조로 모듈끼리 의존하도록 강제합니다. import path에 동적인 값을 사용할 수 없고, export는 항상 최상위 스코프에서만 사용할 수 있습니다.import util from `./utils/${utilName}.js`; // 불가능\n\nimport { add } from \"./utils/math.js\"; // 가능\n\nfunction foo() {\n  export const value = \"foo\"; // 불가능\n}\n\nexport const value = \"foo\"; // 가능따라서 ESM은 빌드 단계에서 정적 분석을 통해 모듈 간의 의존 관계를 파악할 수 있고, Tree-shaking을 쉽게 할 수 있습니다.위와 같은 배경으로 토스팀에서는 CJS/ESM 모두 지원하는 라이브러리를 운영하게 되었습니다.파일이 CJS인지 ESM인지 어떻게 알아요?Module System이 두 개가 존재하며 둘 다 지원해야할 필요성은 알겠는데, .js 파일이 CJS인지 ESM인지 어떻게 알 수 있을까요? package.json의 type field 또는 확장자를 보고 알 수 있습니다..js 파일의 Module System은 package.json의 type field에 따라 결정됩니다.type field의 기본값은 \"commonjs\" 이고, 이 때 .js 는 CJS로 해석됩니다.다른 하나는 \"module\" 입니다. 이 때 .js 는 ESM으로 해석됩니다..cjs 는 항상 CJS로 해석됩니다..mjs 는 항상 ESM으로 해석됩니다.TypeScript도 4.7부터 tsconfig.json 의 moduleResolution 이 nodenext 또는 node16 으로 설정된 경우, 위 규칙이 똑같이 적용됩니다.type field가 \"commonjs\" 인 경우, .ts 는 CJS로 해석됩니다.type field가 \"module\" 인 경우, .ts 는 ESM으로 해석됩니다..cts 는 항상 CJS로 해석됩니다..mts 는 항상 ESM으로 해석됩니다.CJS와 ESM의 차이, 패키지의 기본 Module System을 설정하는 방법과 확장자 모두 알아봤는데, 그래서 어떻게 하면 하나의 패키지가 CJS/ESM을 동시에 매끄럽게 제공할 수 있을까요?정답은 exports field입니다. exports field는 무슨 문제를 해결해줄까요? 어떤 역할을 할까요?패키지 entry point 지정기본적으로는 package.json의 main field와 같은 역할을 합니다. 패키지의 entry point를 지정할 수 있습니다.subpath exports 지원기존에는 filesystem 기반으로 동작했기 때문에, 패키지 내부의 임의의 JS 파일에 접근할 수 있었고, 또한 실제 filesystem 상의 위치와 import path를 다르게 둘 수 없었습니다.// 디렉토리 구조\n/modules\n  a.js\n  b.js\n  c.js\nindex.jsrequire(\"package/a\"); // 불가능\nrequire(\"package/modules/a\"); // 가능이 때, exports field를 사용해 subpath exports를 사용하면, 명시된 subpath 외에는 사용할 수 없고, filesystem 상의 위치와 import path를 다르게 지정할 수 있습니다.// CJS 패키지\n{\n  \"name\": \"cjs-package\",\n  \"exports\": {\n    \".\": \"./index.js\",\n    \"./a\": \"./modules/a.js\",\n  },\n}// ./a.js가 아니라\n// ./modules/a.js를 불러온다.\nrequire(\"cjs-package/a\");\n\n// 에러\n// ./b는 exports field에 명시하지 않은 subpath이다.\nrequire(\"cjs-package/b\");conditional exports 지원기존에는 filesystem 기반으로 동작했기 때문에, Dual CJS/ESM 패키지를 자연스럽게 운영하기가 어려웠습니다.exports field를 사용하면, 똑같은 import path에 대해 특정 조건에 따라 다른 모듈을 제공할 수 있습니다.{\n  \"name\": \"cjs-package\",\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.cjs\",\n      \"import\": \"./esm/index.mjs\"\n    }\n  }\n}// CJS 환경\n// ./dist/index.cjs를 불러온다.\nconst pkg = require(\"cjs-package\");\n\n// ESM 환경\n// ./esm/index.mjs를 불러온다.\nimport pkg from \"cjs-package\";올바른 exports fieldDual CJS/ESM 패키지의 exports field를 올바르게 작성하기 위해 주의해야할 점을 알아봅시다.상대 경로로 표시하기exports field는 모두 . 으로 시작하는 상대 경로로 작성되어야 합니다.// X\n{\n  \"exports\": {\n    \"sub-module\": \"dist/modules/sub-module.js\"\n  }\n}\n\n// O\n{\n  \"exports\": {\n    \".\": \"./dist/index.js\",\n    \"./sub-module\": \"./dist/modules/sub-module.js\"\n  }\n}Module System에 따라 올바른 확장자 사용하기conditional exports를 사용할 때, 패키지가 따르는 Module System에 따라, 즉 package.json의 type field에 따라 올바른 JS 확장자를 사용해야 합니다.CJS 패키지일 때// ESM은 .mjs로 명시해야함\n{\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.js\",\n      \"import\": \"./dist/index.mjs\"\n    }\n  }\n}ESM 패키지일 때// CJS는 .cjs로 명시해야함\n{\n  \"type\": \"module\"\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.cjs\",\n      \"import\": \"./dist/index.js\"\n    }\n  }\n}\n\n이 규칙을 지키지 않고 전부 .js 확장자를 사용했을 때는 어떤 일이 발생할까요? 아래와 같이 상황을 가정하겠습니다.cjs-package 는 CJS 패키지이다.type field가 \"commonjs\" 이기 때문이다../dist/index.js 는 CJS 문법(require / module.exports)으로 작성된 모듈이다../esm/index.js 는 ESM 문법(import / export)으로 작성된 모듈이다.{\n  \"name\": \"cjs-package\",\n  \"type\": \"commonjs\",\n  \"exports\": {\n    \".\": {\n      \"require\": \"./dist/index.js\",\n      \"import\": \"./esm/index.js\"\n    }\n  }\n}CJS 환경에서 cjs-package 를 require 했을 땐 잘 동작합니다. ./dist/index.js 는 CJS 모듈이고, 확장자가 .js 이므로, 가장 가까운 package.json의 type field를 따라 CJS Module Loader가 사용될 것이기 때문입니다.// 잘 동작한다.\n// ./dist/index.js를  CommonJS Module Loader로 불러온다.\nconst pkg = require(\"cjs-package\");하지만 ESM 환경에서 cjs-package 를 import 했을 땐 에러가 발생합니다. ./esm/index.js 는 ESM 모듈이지만, 확장자가 .js 이므로 가장 가까운 package.json의 type field를 따라 CJS Module Loader가 사용됩니다.ESM 문법으로 작성된 JavaScript를 CJS Module Loader로 읽기 때문에 당연히 에러가 발생합니다.(예시: import 문은 ESM에서만 사용 가능하다는 에러가 발생)// 에러가 발생한다.\n// ./esm/index.js를 CJS Module Loader로 읽었다.\nimport * as pkg from \"cjs-package\";TypeScript 지원하기TypeScript에서 module import시, 항상 Type Definition을 찾게 되는데요. 기존에는 filesystem 기반으로 Type Definition을 탐색했습니다.// ./sub-module.d.ts를 찾는다.\nimport subModule from \"package/sub-module\";\n\n하지만 TypeScript 4.7부터 moduleResolution 옵션에 node16 과 nodenext 가 정식으로 추가되었고, node16 과 nodenext 는 filesystem 기반이 아닌 exports field로부터 Type Definition을 탐색합니다. 또한, CJS TypeScript( .cts )와 ESM TypeScript( .mts )를 구분합니다.TypeScript는 conditional import의 조건 중 types 를 참조하며, 이 때 JavaScript와 마찬가지로 package.json의 type field에 따라 알맞은 확장자 ( .cts / .mts )를 사용해야 합니다.CJS 패키지// ESM TS는 mts로 명시해야함\n{\n  \"exports\": {\n    \".\": {\n      \"require\": {\n        \"types\": \"./index.d.ts\",\n        \"default\": \"./index.js\"\n      },\n      \"import\": {\n        \"types\": \"./index.d.mts\",\n        \"default\": \"./index.mjs\"\n      }\n    }\n  }\n}ESM 패키지// CJS TS는 cts로 명시해야함\n{\n  \"type\": \"module\",\n  \"exports\": {\n    \".\": {\n      \"require\": {\n        \"types\": \"./index.d.cts\",\n        \"default\": \"./index.cjs\"\n      },\n      \"import\": {\n        \"types\": \"./index.d.ts\",\n        \"default\": \"./index.js\"\n      }\n    }\n  }\n}그럼 TypeScript의 경우에는 위 규칙을 지키지 않으면 어떻게 될까요? 아래와 같이 상황을 가정하겠습니다.esm-package 는 ESM 패키지이다.type field가 \"module\" 이기 때문이다..cts (CJS TypeScript)에서 esm-package 를 사용한다.{\n  \"name\": \"esm-package\",\n  \"type\": \"module\",\n  \"exports\": {\n    \".\": {\n      \"types\": \"./index.d.ts\",\n      \"require\": \"./index.cjs\",\n      \"import\": \"./index.js\"\n    }\n  }\n}이 때 .cts (CJS TypeScript)에서 esm-package 를 require하면 타입 에러가 발생합니다.esm-package 는 Type Definition을 ./index.d.ts 만 지원합니다. 즉, ESM/CJS TypeScript 모두 ./index.d.ts 를 바라보게 됩니다.이 때, esm-package 는 ESM 패키지이기 때문에 index.d.ts 는 ESM TypeScript로써 해석됩니다.따라서 esm-package 는 CJS TypeScript 입장에서 Pure ESM Module이고, CJS는 ESM을 불러올 수 없기 때문에 esm-package 가 순수 ESM으로만 확인된다는 타입 에러가 발생합니다.// index.cts\n\n// Type Error: esm-package는 동기적으로 가져올 수 없는 ES 모듈로만 확인됩니다.\n// CJS TypeScript를 위한 .d.cts를 지원하지 않았기 때문에 발생하는 에러\nimport * as esmPkg from \"esm-package\";\n\n최근 토스팀 내부 라이브러리들은 위처럼 올바르게 exports field를 작성하여 배포되고 있습니다. CJS/ESM JavaScript는 물론 TypeScript 지원까지 잘 되있습니다.JavaScript/TypeScript 생태계는 계속해서 발전하고 있지만, TypeScript까지 잘 지원하는 라이브러리는 정말 유명한 라이브러리들 중에서도 찾아보기가 많이 힘듭니다.그렇다면 우리가 그 시작점이 되면 어떨까요? 토스팀에서는 이런 기술적인 문제를 함께 풀어가고 싶으신 분들을 언제나 환영합니다. 함께 좋은 생태계를 만들어 나가고 싶어요.Node.js의 CJS/ESM에 대해CJSESMDetermining Module Systemexports field에 대해package.json export fieldSubpath exportsConditional exportsTypeScript의 CJS/ESM 지원에 대해4.7 릴리즈 노트재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "12": "토스증권 QA Team을 소개합니다곽수정ㆍQA Manager2022. 10. 25안녕하세요. 저는 3년 전 토스의 첫 QA Manager로 입사해서 현재 토스증권 QA팀에서 일하고 있는 곽수정입니다. 길다면 긴 3년이란 시간 동안 즐겁고 재밌게 일할 수 있었던 토스와 토스증권의 QA팀을 소개해보려고 해요.QA가 없었던 토스팀, 왜 첫 번째 QA Manager를 채용하게 되었을까요?초창기의 토스팀은 사용자에게 빠른 제품/서비스를 제공하는데 많은 노력을 기울이고 있었어요. 제품의 퀄리티 역시 PO, 개발자 등 제품을 만드는 ‘메이커(Maker)’들이 간단한 테스트를 통해 관리하고 있었죠. 버그가 발생하더라도 누구보다 빠르게 대응할 수 있다는 강한 자신감과 팀원 개개인의 오너십이 있었기 때문에 가능한 일이었다고 생각해요.이러한 동력에 힘입어 토스는 시장에서 빠른 속도감을 무기로 존재감을 높여갔어요. 그만큼 우리가 제공하는 제품과 서비스의 수도 증가했죠. 이에 따라 메이커들이 제품을 만들면서, 동시에 높은 수준의 제품 안정성을 체크하고 유지하는 데 한계가 생기기 시작했어요. 이 시기에 토스팀에서도 QA 담당자의 필요성이 논의되기 시작했지만, QA가 진행하는 품질 테스트가 우리의 속도에 영향을 주지는 않을지 우려가 공존하기도 했답니다.시장에서의 빠른 속도만큼이나, 고객들에게 안정적이고 높은 퀄리티의 서비스를 제공하는 것도 중요하다고 믿었던 토스팀은 2019년 첫 번째 QA Manager를 채용하게 됩니다. 그게 바로 저고요!그럼 토스팀의 QA는 어떻게 일할까요?QA가 없던 회사에서 QA가 일하는 방식을 만들어 나가는 것은 아주 어려움이 많았어요. 일반적인 IT 회사에서는 서비스 출시 전에 테스트하는 것이 너무 당연한데요. 처음 토스에 입사했을 땐 이것도 알려야 할 만큼 어려움이 있었답니다.이전 회사의 경우, 기획팀에서 기획서를 작성한 후에 개발/QA팀에 공유해서 서비스 출시 일정을 결정했어요. 그래서 QA팀에서는 QA 중에 발견한 버그로 인해 출시 일정이 뒤로 밀리거나 개발이 늦어지지 않도록 Sanity Test(BVT)라는 절차를 만들어서 QA 가용 리소스를 확보하는 장치를 만들기도 했고요. 오히려 이러한 장치가 개발자와 QA 모두에게 업무적인 비효율을 초래해서 서비스 출시일이 미뤄지는 경우가 많다는 점이 아쉬웠어요.하지만 토스의 방식은 달라요.QA Manager는 다양한 툴을 통해 서비스와 관련된 크고 작은 변경 사항들을 미리 확인할 수 있어요. 접근 권한 또한 열려 있어서 개발자의 작업 상태를 확인할 수 있고요. 자체적으로 QA계획을 수립해서 작은 단위부터 미리 테스트를 시작할 수도 있고, 통합테스트(Integration Test) 및 모니터링과 같은 QA 단계도 직접 조율할 수 있어요.QA Manager가 테스트를 진행하지만, 살충제 패러독스(Pesticide Paradox)에 빠지지 않도록 개발자도 배포를 위한 Regression Test Case를 별도로 수행하고 있어요. 매주 랜덤으로 수행자를 지정한 뒤 테스트가 완료되어야만 앱 심사 등록 요청이 가능하도록 하고 있어요. 서비스를 만들어가는 담당자 모두가 안정적인, 높은 품질의 서비스를 제공하고자 하는 마음이 크기 때문에 가능한 방식이라고 생각해요.토스증권이 서비스를 처음 출시했을 땐, 어떻게 QA를 했을까요?토스증권 출범을 준비하던 시기에는 토스증권 소속 QA Manager가 전무한 상태였어요. 당시 토스 소속이었던 제가 파견을 나가 QA 업무를 지원해주고 있었죠.조직 내 QA 매니저가 없는 상태에서 새로운 분을 모셔오는건 정말 어려운 일이었어요. 그래서 외주업체에서 QA 테스트 전문가분들을 모셔와 MTS(모바일 트레이딩 시스템) 출시를 위한 테스트를 진행했어요.다시 말하면, 새로운 법인에서 메인 서비스를 출시하는데 단 3명의 QA 인력이 그 모든 품질 테스트를 담당했다는 것이죠.👀 “3명이라고요?? 그게 가능한 일인가요?”당연히, 저 혼자만 QA를 진행할 수는 없었어요.많은 팀원분들이 적극적으로 도움을 주셔서 가능했어요.토스증권이 서비스를 성공적으로 런칭할 수 있도록 같은 마음으로 바라봐주는 든든한 팀원분들이 없었다면 힘들었을 것 같아요. 당시 토스증권 팀원 총 70명 정도를 대상으로 클로즈 베타 테스트를 1~2회 정도 진행하였고, 토스커뮤니티의 타 계열사 팀원분들께도 사전 신청을 받아 150명을 대상으로 하는 추가 테스트도 진행했어요.이러한 과정을 통해 Edge Case를 발견하고 고객의 피드백도 미리 체험해볼 수 있었습니다. 제품에 진심인 팀원분들이 꼼꼼하게 피드백을 주시다 보니 서비스를 빠르게 개선해서 출시할 수 있었어요.(토스증권 서비스 출시 과정과 관련된 에피소드는 다음 포스팅에서 자세히 공개할께요)토스증권 QA팀은 어떤 업무를 하나요?홀로 QA Manager로 근무하던 때에는 리소스가 부족하니 고객 접점이 높은 프론트 서비스를 기준으로 QA 범위를 정했었어요.이제는 QA팀이 되어서 함께하는 든든한 동료분들이 있습니다. 각각 제품을 만드는 Silo/Team에 소속되어 프로덕트를 만드는 전 과정을 함께하고 있어요.QA가 일할 때 필요한 공통 정책을 정의하기도 하고 프로덕트의 QA 커버리지 확대를 위해 노력하고 있기도 합니다. QA Manager는 본인이 담당하고 있는 서비스를 대상으로 팀원들에게 탐색적 테스팅이나 Bug Bash를 제안하기도 해요. 주기적으로 QA Study도 진행하면서 자기 계발도 함께해요.최근에는 Test Automation Engineer를 채용하기도 했어요. UI Automation Testing 설계뿐만 아니라 QA팀에서 필요한 도구를 개발하는 업무까지 수행하고 계세요.토스증권 QA팀은 테스트 커버리지를 계속해서 넓혀가고 있습니다. 장기적으로는 QA 품질 서비스 지표를 만들어서 변하지 않는 북극성 같은 목표를 만들고자 해요.토스증권에서 QA로 근무하면서 가장 만족하는 점은 무엇인가요?제가 재직했던 여러 회사에서는 QA의 권한이 곧 QA 품질 보고서(Sign Off)라고 생각하는 경우가 많았어요. QA의 중요성을 주장하기 위해 보고서를 작성하느라 야근하는 일도 잦았죠. 품질팀에서는 버그나 장애에 대한 책임을 피하고자 테스트를 아웃소싱에 맡기는 경우도 많았습니다. QA에서 발견한 버그의 Priority, Severity를 고려하지 않고 품질 확보라는 명목으로 서비스 출시일 자체를 미뤄버리는 경우도 있었어요.서비스 출시일이 계속 뒤로 밀리고 있는 게 과연 회사를 위한 일이 맞는지는 아직도 공감되지 않아요.토스증권은 이러한 비효율적인 부분을 가장 지양합니다. 테스트케이스 수행 결과를 취합하고, Sign Off 결과에 대한 근거를 제공하기 위해 추가 이터레이션, Regression Test 일정을 확보하기 위해 문서 작업을 하는 등의 비효율이 없어요.서비스 출시를 할 때 QA가 허들이 되는 것이 아니라, 안정적인, 좋은 품질의 서비스를 제공하여 고객 만족을 추구한다는 점에서 토스증권 QA Manager로 근무하고 있는 것이 만족스러워요.토스증권에서는 어떠한 사람을 찾고 있나요? 🙋‍♂️🙋‍♀️토스증권과 함께 성장하는 QA Manager가 되기 위해서는 다음과 같은 부분이 중요하다고 말씀드리고 싶어요.QA에 대한 열정과 긍정적인 마음가짐을 바탕으로 토스증권 서비스의 품질을 향상시키기 위해 함께 고민할 수 있는, 고민하는 것을 즐기는 분QA 업무를 하시는 분들은 대부분 팀이 세팅되어있고 안정적으로 Iteration하고 있는 회사를 선호하시는 것 같아요. 새롭게 설립되는 QA팀의 경우, QA가 회사에서 꼭 필요한 역할이라는 것을 내부적으로 설득하는 것이 어렵기 때문일 수 있겠죠.토스증권은 팀원들이 QA의 중요성을 잘 알고 있고 QA팀이 설정해둔 목표 또한 명확합니다. 빠르게 성장하고 있는 서비스와 함께 QA 커버리지를 확대하기 위해 함께할 동료분들을 찾고 있어요.토스증권에서 메이커와 같은 시선으로 제품의 전 과정을 함께하는 QA Manager가 되어보세요!토스증권 QA Manager 채용공고 👉 바로가기토스증권 Test Automation Engineer 채용공고 👉 바로가기재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "13": "TypeScript 타입 시스템 뜯어보기: 타입 호환성김병묵ㆍNode.js Developer2022. 10. 26토스 Node.js 챕터에서는 높은 코드 가독성과 품질을 위해 TypeScript의 타입 시스템을 적극적으로 활용하고 있고 이에 대한 이해도를 높이기 위해 스터디를 꾸준히 진행하고 있습니다. TypeScript의 타입 시스템에 대해 공부해보던 중 알게된 흥미로운 몇가지 토픽들을 소개하려 합니다. 그 중 한가지로 이번글에서는 “타입 호환성 (type compatibility)”에 대해 알아보고자 합니다.TypeScript 공식문서 타입 호환성에 관한 글을 보면 아래와 같이 소개하고 있습니다.TypeScript의 타입 호환성은 구조적 서브타이핑(structural subtyping)을 기반으로 합니다. 구조적 타이핑이란 오직 멤버만으로 타입을 관계시키는 방식입니다. 명목적 타이핑(nominal typing)과는 대조적입니다. TypeScript의 구조적 타입 시스템의 기본 규칙은 y가 최소한 x와 동일한 멤버를 가지고 있다면 x와 y는 호환된다는 것입니다.위 내용에 대해 하나씩 이해해봅시다. 우선 강한 타입 시스템을 통해 높은 가독성과 코드 품질을 지향하는 TypeScript가 왜 타입 호환성을 지원하는 것일까요? 이 경우 타입 안정성에 문제가 생기게 되는 것은 아닐까요? 아래 예시를 통해 타입 호환성이 왜 필요한지 살펴보겠습니다.위와 같이 음식 Food 타입의 객체를 인자로 받아 간단한 칼로리 계산 공식으로 주어진 음식의 칼로리를 구하는 calculateCalorie 함수가 있습니다. 타입과 함수는 아래와 같이 구현되어 있습니다.type Food = {\n  /** 각 영양소에 대한 gram 중량값 */\n  protein: number;\n  carbohydrates: number;\n  fat: number;\n}\n\nfunction calculateCalorie(food: Food){\n  return food.protein * 4\n    + food.carbohydrates * 4\n    + food.fat * 9\n}한편, 개발자가 코드를 작성하는 과정에서 (의도했거나 혹은 실수로) calculateCalorie 함수 인자에 여러가지 타입의 객체를 전달해본다고 가정해봅시다. 이 경우 TypeScript 타입 시스템은 프로그램이 타입 오류를 일으킬 가능성을 검사하게 됩니다.위 3가지 케이스에 대해 Type Checker가 어떻게 판단하는 것이 좋을까요?개발자가 정의한 Food 타입과 동일한 타입인 경우 (1번) 오류 없음이 명확하며, Computer 타입과 같이 다른 타입이며 칼로리 계산이 불가능한 경우 (2번) 오류로 판단하는 것이 명확합니다. 하지만, 햄버거를 의미하며 음식의 한 종류인 Burger 타입이 전달되는 경우 (3번) 어떻게 판단하는 것이 맞을까요?type Burger = Food & {\n  /** 햄버거 브랜드 이름 */\n  burgerBrand: string;\n}심지어 Burger 타입이 위와 같이 Food 타입을 상속하며 칼로리 계산에 필요한 모든 프로퍼티를 포함하고 있어 런타임 상에서 정상적으로 동작한다면 이를 타입 오류라고 판단하는게 올바른 걸까요?이처럼 실제로 정상적으로 동작할 수 있는 올바른 코드라면 타입 시스템은 개발자의 의도에 맞게 유연하게 대응하여 타입 호환성을 지원하는 것이 더 좋을 수 있습니다. 이러한 유연성을 위해 TypeScript 타입 시스템은 부분적으로 타입 호환을 지원하고 있습니다.한편 위에 예시에서 Computer 타입 사례처럼 타입오류로 판단하는 것이 명확한 경우가 있으며, 타입 안정성을 해치면서까지 유연함을 제공하는 것은 바람직하지 못합니다. 이를 위해서는 어떠한 경우에 호환을 허용할 것인지에 대한 명확한 규칙이 필요합니다. 이러한 규칙 중 프로그래밍 언어들에서 널리 활용되는 방식으로 명목적 서브타이핑(nominal subtyping)과 구조적 서브타이핑(structural subtpying)이 있습니다.명목적 서브타이핑은 아래와 같이 타입 정의 시에 상속 관계임을 명확히 명시한 경우에만 타입 호환을 허용하는 것입니다. 이 방법을 통해 타입 오류가 발생할 가능성을 배제하고, 개발자의 명확한 의도를 반영할 수 있습니다./** 상속 관계 명시 */\ntype Burger = Food & {\n  burgerBrand: string;\n}\n\nconst burger: Burger = {\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n}\n\nconst calorie = calculateCalorie(burger)\n/** 타입검사결과 : 오류없음 (OK) */\n\n한편, 구조적 서브타이핑은 아래와 같이 상속 관계가 명시되어 있지 않더라도 객체의 프로퍼티를 기반으로 사용처에서 사용함에 문제가 없다면 타입 호환을 허용하는 방식입니다. 아래 예시를 보면 비록 상속 관계임을 명시하지는 않았지만 burger 변수는 Food 타입의 프로퍼티를 모두 포함하고 있고 따라서calculateCalorie 함수 실행과정에서 오류가 발생하지 않습니다.const burger = {\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n}\n\nconst calorie = calculateCalorie(burger)\n/** 타입검사결과 : 오류없음 (OK) */구조적 서브타이핑 방식은 타입 시스템이 객체의 프로퍼티를 체크하는 과정을 수행해주므로써, 명목적 서브타이핑과 동일한 효과를 내면서도 개발자가 상속 관계를 명시해주어야 하는 수고를 덜어주게 됩니다. 참고로, 구조적 서브타이핑은 “만약 어떤 새가 오리처럼 걷고, 헤엄치고, 꽥꽥거리는 소리를 낸다면 나는 그 새를 오리라고 부를 것이다.” 라는 의미에서 덕 타이핑 (duck typing) 이라고도 합니다.TypeScript Type Checker는 구조적 서브타이핑을 기반으로 타입 호환을 판단합니다.TypeScript는 구조적 서브타이핑을 지원하며, 명목적 서브타이핑만 지원하는 C#, Java 등의 언어는 명시적으로 상속 관계를 명시해주어야 타입 호환이 가능합니다.💡  한편, 여기서부터 좀 더 본격적인 이야기를 다루어 보겠습니다.위 구조적 서브타이핑 예시의 코드는 타입 호환성에 따라 타입 오류가 발생하지 않지만, 아래 코드의 경우 컴파일 과정에서 Argument is not assignable to parameter of type 'Food' 라는 타입 오류가 발생하게 됩니다. 글을 더 읽으시기에 앞서 실제로 TS Playground를 통해 오류를 확인해보시고 다양하게 테스트해보시는 것도 추천합니다.const calorie = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n})\n/** 타임검사결과 : 오류 (NOT OK)*/왜 위 코드는 타입 호환이 지원되지 않는 것일까요? 처음에 이 오류를 마주쳤을 때 이런저런 테스트를 해보며 함수에 값을 바로 인자로 전달하는 경우만 타입 호환이 지원되지 않는 것 같다고 유추하기는 했으나 조금 더 구체적인 규칙과 이렇게 예외가 발생하는 이유에 대해 이해해보고자 했습니다.결과적으로 TypeScript 컴파일러 코드 상의 구현로직과 위 이슈와 연관된 TypeScript Github PR을 통해 이해할 수 있었습니다. 이에 대해 알아보기 위해 우선 TypeScript 컴파일러가 동작하는 방식에 대해 간략히 살펴보겠습니다.TypeScript 컴파일러가 동작하는 방식에 관해 아래 영상에 자세히 소개되어 있으며, 이 중 몇가지 내용만 요약하여 살펴보겠습니다.https://www.youtube.com/watch?v=X8k_4tZ16qUTypeScript 컴파일러의 역할은 TypeScript 소스코드를 AST (Abstract Syntax Tree)로 변환한 뒤, 타입 검사를 수행하고, 그 후 JavaScript 소스코드로 변환하는 과정을 담당합니다.TypeScript 소스코드를 AST로 변환하는 과정은 parser.ts, scanner.ts , 타입 검사를 수행하는 과정은 binder.ts, checker.ts, AST를 JavaScript 소스코드로 변환하는 과정은 emitter.ts, transformer.ts 등의 파일이 담당하고 있습니다.실제로 TypeScript Github의 compiler 디렉토리에 가면 위 코드 파일이 어떤식으로 구현되어 있는지 확인해볼 수 있으며, 이번 글에서 다루고 있는 주제인 구조적 서브타이핑과 타입 호환에 관한 부분은 타입 검사와 가장 연관이 높은 checker.ts 파일의 hasExcessProperties() 함수에서 처리하고 있었습니다.아래는 checker.ts 코드 중 타입 호환의 예외가 발생하는 지점의 코드를 주요한 부분만 남기고 간소화한 것입니다. 주석과 함께 봐주시면 좋을 것 같습니다./** 함수 매개변수에 전달된 값이 FreshLiteral인 경우 true가 됩니다. */\nconst isPerformingExcessPropertyChecks =\n    getObjectFlags(source) & ObjectFlags.FreshLiteral;\n\nif (isPerformingExcessPropertyChecks) {\n    /** 이 경우 아래 로직이 실행되는데,\n     * hasExcessProperties() 함수는\n     * excess property가 있는 경우 에러를 반환하게 됩니다.\n     * 즉, property가 정확히 일치하는 경우만 허용하는 것으로\n     * 타입 호환을 허용하지 않는 것과 같은 의미입니다. */\n    if (hasExcessProperties(source as FreshObjectLiteralType)) {\n        reportError();\n    }\n}\n/**\n * FreshLiteral이 아닌 경우 위 분기를 skip하게 되며,\n * 타입 호환을 허용하게 됩니다. */지면상 다소 간소화한 코드만 남겨두었지만, 함수에 인자로 들어온 값이 FreshLiteral 인지 아닌지 여부에 따라 조건분기가 발생하여 타입 호환 허용 여부가 결정된다는 것을 확인할 수 있었습니다.그렇다면 Fresh Literal 이란 무엇이며, 왜 이 경우에는 타입 호환의 예외가 발생하도록 되어 있는 것일까요?TypeScript는 구조적 서브타이핑에 기반한 타입 호환의 예외 조건과 관련하여 신선도 (Freshness) 라는 개념을 제공합니다. 모든 object literal은 초기에 “fresh” 하다고 간주되며, 타입 단언 (type assertion) 을 하거나, 타입 추론에 의해 object literal의 타입이 확장되면 “freshness”가 사라지게 됩니다. 특정한 변수에 object literal을 할당하는 경우 이 2가지 중 한가지가 발생하게 되므로 “freshness”가 사라지게 되며, 함수에 인자로 object literal을 바로 전달하는 경우에는 “fresh”한 상태로 전달됩니다.한편, TypeScript Github PR (2015년 7월) 의 논의에 따르면, fresh object인 경우에는 예외적으로 타입 호환을 허용하지 않기로 했음을 확인할 수 있습니다. 그러한 이유에 대해 살펴보겠습니다./** 부작용 1\n * 코드를 읽는 다른 개발자가 calculateCalorie 함수가\n * burgerBrand를 사용한다고 오해할 수 있음 */\nconst calorie1 = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n})\n\n/** 부작용 2\n * birgerBrand 라는 오타가 발생하더라도\n * excess property이기 때문에 호환에 의해 오류가\n * 발견되지 않음 */\nconst calorie2 = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  birgerBrand: '버거킹'\n})구조적 서브타이핑에 기반한 타입 호환은 유연함을 제공한다는 이점이 있지만, 위 코드 사례와 같이 코드를 읽는 다른 개발자의 입장에서 함수가 실제 다루는 것보다 더 많은 데이터를 받아들인다는 오해를 불러일으킬 수 있고, 프로퍼티 키에 대한 오타가 발생하더라도 오류가 확인되지 않는 부작용이 있습니다.한편, fresh object를 함수에 인자로 전달한 경우, 이는 특정한 변수에 할당되지 않았으므로 어차피 해당 함수에서만 사용되고 다른 곳에서 사용되지 않습니다. 이 경우 유연함에 대한 이점보다는 부작용을 발생시킬 가능성이 높으므로 굳이 구조적 서브타이핑을 지원해야할 이유가 없습니다.TypeScript Type Checker는 구조적 서브타이핑을 기반으로 타입 호환을 판단하되,Freshness에 따라 예외를 둡니다.이처럼 타입 호환성은 유연함이라는 이점을 제공하지만 그로 인해 부작용이 발생할 수 있으므로, 이에 대한 절충안으로 타입 호환을 제공해서 얻는 이점이 거의 없는 fresh object에 대해서는 호환성을 지원하지 않기로 논의되어 TypeScript 컴파일러 코드에 반영된 것을 확인해볼 수 있었습니다.한편, 그럼에도 개발자가 fresh object에 대해서 타입 호환을 허용하고자 한다면 아래와 같이 함수 매개변수 타입에 index signature를 포함시켜두어 명시적으로 타입 호환을 허용시키는 것이 가능합니다. 또는 tsconfig 상에 suppressExcessPropertyErrors 를 true로 설정하는 방식도 가능합니다. (이 또한 동일한 PR 논의에 정의되어 있습니다.)type Food = {\n  protein: number;\n  carbohydrates: number;\n  fat: number;\n  [x: string]: any                  /** index signature */\n}\n\nconst calorie = calculateCalorie({\n  protein: 29,\n  carbohydrates: 48,\n  fat: 13,\n  burgerBrand: '버거킹'\n})\n/** 타임검사결과 : 오류없음 (OK) */\n\n또한 반대로 모든 경우에 대해 타입 호환을 허용하지 않도록 강제하는 것도 가능한데 이를 위해 사용할 수 있는 기법이 Branded type (또는 Branding type) 입니다. 아래와 같이 의도적으로 __brand 와 같은 프로퍼티를 추가시켜, 개발자가 함수의 매개변수로 정의한 타입 외에는 호환이 될 수 없도록 강제하는 기법입니다. 온도(섭씨, 화씨)나 화폐단위(원, 달러, 유로)와 같이 같이 number 타입이지만 서로 다를 의미를 가질 수 있어 명시적인 구분이 필요할 때 사용해볼 수 있습니다.type Brand<K, T> = K & { __brand: T};\ntype Food = Brand<{\n  protein: number;\n  carbohydrates: number;\n  fat: number;\n}, 'Food'>\n\nconst burger = {\n  protein: 100,\n  carbohydrates: 100,\n  fat: 100,\n  burgerBrand: '버거킹'\n}\n\ncalculateCalorie(burger)\n/** 타임검사결과 : 오류 (NOT OK) */\n\n앞선 글을 통해 이해한 타입 호환의 이점과 부작용에 대한 이해를 바탕으로 개발자는 자신의 프로젝트를 진행하는 과정에서 필요에 맞게 index signature, tsconfig > suppressExcessPropertyErrors, branded type 등을 통해 타입 호환성의 범위를 선택하여 개발하는 것이 가능할 것입니다.TypeScript Type Checker는 내부적인 규칙에 따라 타입 호환을 판단하지만,개발자가 필요에 따라 선택하는 것이 가능합니다.이번글의 내용을 모두 요약하면 아래와 같습니다.타입 검사의 안정성과 유연함 사이에서 절충안으로 도입된 개념이 타입 호환성입니다. 그리고 타입 호환성을 지원하는 방법과 관련하여 개발자에게 명시적 선언을 어디까지 요구할 것인지에 대한 선택지가 존재합니다.TypeScript는 구조적 서브타이핑에 기반한 타입 호환을 통해 개발자의 명시적 선언을 줄여주는 한편 이로 인한 부작용을 개선하고자 freshness에 기반한 예외조건을 두었고, Index Signature와 Branded type 등의 방식을 통해 개발자가 명시적으로 선택할 수 있는 선택지를 만들어두었습니다.프로그래밍 언어마다 타입 검사가 동작하는 방식이 다르며 이는 해당 언어를 개발한 커뮤니티의 논의와 의사결정에 따라 선택된 결과라고 볼 수 있습니다. 본 주제 외에도 TypeScript 컴파일러 코드와 Github PR을 살펴보면 흥미로운 논의와 토픽들을 확인해볼 수 있습니다.토스 Node.js 챕터는 토스의 다양한 제품과 라이브러리 개발을 위해 팀원들의 지속적인 성장이 중요하다고 믿으며, 이를 위해 꾸준히 공부하고 공유하는 자리를 가지고 있으니 많은 관심 부탁드립니다.토스 Node.js Chapter 채용 공고 👉 바로가기감사합니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "14": "NestJS 환경에 맞는 Custom Decorator 만들기송현지ㆍNode.js Developer2022. 11. 22데코레이터는 비즈니스와 상관 없는 로직들을 숨기면서 기능을 변경하거나 확장할 수 있게 합니다. 또한 여러 클래스에서 반복되는 공통 관심사가 있을 때 데코레이터를 사용하면 중복된 코드를 줄이고 코드를 모듈 단위로 관리하는 효과를 거둘 수 있습니다.이런 이유로 저희 Node.js Chapter에서도 데코레이터를 적극 활용하고 있습니다. 하지만 NestJS에서는 데코레이터를 만들 때 다음과 같은 질문들이 있었습니다.데코레이터에서 Provider를 사용해야할 때 어떻게 Provider에 접근할 수 있을까?메타데이터를 쓰는 NestJS 데코레이터를 일반 데코레이터와 사용해도 괜찮을까?NestJS에서 데코레이터를 만들기 위해서는 NestJS의 DI와 메타 프로그래밍 환경 등을 고려해야 합니다. 그래서 이 글을 통해 NestJS에서는 어떻게 데코레이터를 만드는지 살펴보고, 앞의 두 질문들을 고려하여 NestJS 환경에 맞는 데코레이터를 만들어보려고 합니다.들어가기 전에, 만약 데코레이터나 메타데이터가 생소하시다면 아래 문서들을 읽어보시는 걸 추천드립니다.Typescript Decoratorreflect-metadata@Injectable()\nclass TestService {\n  @Cacheable('key')\n  test() {\n      // 비즈니스 로직\n  }\n}TestService가 있을 때, 캐싱 로직을 Cacheable 데코레이터를 사용해 비즈니스 로직과 분리하려고 합니다.Cacheable 데코레이터에서 CacheManager라는 Provider를 사용하려면 어떻게 접근해야 할까요?@Module{\n  imports: [CacheModule.register(...)]\n  providers: [TestService]\n}\nclass TestModule {}CacheManager Provider를 export하는 CacheModule을 import 해봅시다.function Cacheable(key: string, ttl: number) {\n  return function (target: any, _key: string, descriptor: PropertyDescriptor) {\n    const methodRef = descriptor.value;\n\n    descriptor.value = async function (...args: any[]) {\n    console.log(this) // TestService {}\n\n    // TypeError: Cannot read properties of undefined (reading 'get')\n    const value = await this.cache.get(key);\n    if (value) {\n      return value;\n    }\n\n    const result = await methodRef.call(this, ...args);\n    await this.cache.set(key, result, ttl);\n    console.log(result)\n    return result;\n    };\n  };\n}TestModule에서 CacheModule을 import하고 있긴 하지만 TestService에서 CacheManager 를 주입하지 않는 이상 Cacheable에서 CacheManager에 접근할 방법이 없습니다. Cacheable 데코레이터를 사용하려면 클래스에 항상 CacheManager를 주입해주어야 하는 불편함이 있습니다.게다가 CacheManager를 넣어준다고 해도 멤버 이름을 cache 로 강제해야 합니다. 가능한 방법이지만 휴먼 에러가 발생할 수 있어 좋은 방법은 아닙니다.그렇다면 NestJS 메서드 데코레이터는 어떻게 되어있을까요?NestJS가 데코레이터를 등록하는 과정은 ‘마킹 - 조회 - 등록’로 크게 세 단계로 나뉩니다. Cron 메서드 데코레이터를 예로 들어보겠습니다.마킹 - SetMetadata라는 함수로 특정 메서드에 CRON 심볼을 메타데이터 키로 등록합니다.조회 - 모듈이 초기화되는 시점에 DiscoveryServiced와 MetadataScanner로 모든 Provider 클래스를 순회하며 CRON 심볼을 메타데이터로 가지고 있는 메서드들을 찾습니다.등록 - 메서드를 찾았으면 해당 메서드를 크론 잡으로 등록합니다.NestJS에서 제공하는 SetMetadata와 DiscoverService, 그리고 MetadataScanner를 사용하면, 특정 클래스나 메서드만 필터링하여 IoC 내 다른 Provider를 사용해 원하는 로직들을 적용할 수 있습니다.SetMetadataSetMetadata는 타겟(클래스, 메서드)에 메타데이터를 마킹하는 데코레이터를 반환하는 함수입니다. NestJS의 코드를 보면 아래와 같습니다. setMetadata 코드export const SetMetadata = <K = string, V = any>(\n  metadataKey: K,\n  metadataValue: V,\n): CustomDecorator<K> => {\n  const decoratorFactory = (target: object, key?: any, descriptor?: any) => {\n    // method or class에 메타데이터 등록\n    Reflect.defineMetadata(metadataKey, metadataValue, class or method);\n    return target;\n  };\n  decoratorFactory.KEY = metadataKey;\n  return decoratorFactory;\n};Reflect.defineMetadata(metadataKey, metadataValue, class or method);SetMetadata 함수 내부에서는 *Reflect.defineMetadata 메서드를 통해 타겟 객체에 metadataKey를 키, metadataValue를 값으로 하는 내부 슬롯을 정의합니다. ([[Metadata]] )*Reflect 는 reflect-metadata 라이브러리가 설치되어있는 경우 사용할 수 있습니다. 메타데이터를 정의하거나 조회하는 데 사용합니다.SetMetadata(KEY, value) -> CustomDecorator;SetMetadata의 리턴값은 클래스, 메서드 데코레이터로 사용 가능합니다. 해당 데코레이터로 타겟 클래스나 메서드에 대한 메타데이터를 설정할 수 있습니다.const SOMETHING = Symbol('SOMETHING')\n\nfunction CustomDecorator(key: string | symbol) {\n  // SetMetadata(SOMETHING, key)와 다른 데코레이터를 합성할 수 있습니다.\n  return applyDecorators(SetMetadata(SOMETHING, key), AnotherDecorator)\n}\n\n@CustomDecorator('KEY1')\nclass DecoratedClass {}DecoratedClass에 SOMETHING 심볼을 메타데이터 키, 'KEY1'을 메타데이터 값으로 등록합니다.DiscoveryServiceNestJS는 DiscoveryModule 을 제공합니다. DiscoveryModule의 DiscoveryService에서는 내부적으로 modulesContainer를 사용하여 모든 모듈의 Controller와 Provider 클래스를 조회할 수 있습니다.DiscoverService를 사용하여 모든 Provider 클래스를 순회하며, SetMetadata로 등록했던 메타데이터 키로 특정 Provider를 필터링할 수 있게 됩니다.DiscoveryService 코드@Injectable()\nexport class DiscoveryService {\n  constructor(private readonly modulesContainer: ModulesContainer) {}\n\n  getProviders(\n    options: DiscoveryOptions = {},\n    modules: Module[] = this.getModules(options),\n  ): InstanceWrapper[] {\n    return modules.flatMap(item => [...item.providers.values()]);\n  }\n\n  // ...생략\n}CustomDecorator 가 붙은 메서드를 찾는 과정을 예로 들어보겠습니다. 메타데이터 키는 CUSTOM_DECORATOR  심볼이고, 메타데이터 값은 test-value 입니다.export const CUSTOM_DECORATOR = Symbol(\"CUSTOM_DECORATOR\");\nexport const CustomDecorator = SetMetadata(CUSTOM_DECORATOR, 'test-value');\n\n@CustomDecorator\n@Injectable()\nclass TestService {\n  test() {}\n}아래의 explorerService.find(CUSTOM_DECORATOR) 메서드를 실행하면 어떻게 될까요?import { Injectable } from '@nestjs/common';\nimport { DiscoveryService, MetadataScanner, Reflector } from '@nestjs/core';\n\n@Injectable()\nexport class ExplorerService {\n  constructor(\n    private readonly discoveryService: DiscoveryService,\n  ) {}\n\n  find(metadataKey: string | symbol) {\n    const providers = this.discoveryService.getProviders();\n\n    return providers\n      .filter((wrapper) => wrapper.isDependencyTreeStatic())\n      .filter(({ metatype, instance }) => {\n        if (!instance || !metatype) {\n          return false;\n        }\n        return Reflect.getMetadata(metadataKey, metatype);\n      })\n      .map(({ instance }) => instance);\n  }\n}첫번째 필터: filter((wrapper) => wrapper.isDependencyTreeStatic())request scope가 아닌 싱글톤 프로바이더만 필터링합니다.두번째 필터: Reflect.getMetadata(metadataKey, metatype)해당 필터는 메타데이터가 등록된 클래스만 필터링합니다.metatype 은 class TestService 와 같이 해당 Provider의 클래스를 의미합니다.Reflect.getMetadata(metadataKey, metatype) 은 metatype(클래스)에 metadataKey로 등록된 메타데이터의 값을 가져옵니다. TestService 클래스의 경우 메타데이터 키는 CUSTOM_DECORATOR 이고 값은 test-value 입니다.만약 등록된 메타데이터가 없으면 undefined를 반환하고 해당 Provider는 필터링됩니다.MetadataScanner앞의 DiscoverService의 예시에서는 데코레이팅된 메서드를 가진 인스턴스에 접근하는 데 그쳤습니다. 실제 데코레이팅된 메서드에 접근하기 위해서는 DiscoveryModule에서 제공하는 MetadataScanner 를 사용해야 합니다.MetadataScanner 코드export class MetadataScanner {\n  public scanFromPrototype<T extends Injectable, R = any>(\n    instance: T,\n    prototype: object,\n    callback: (name: string) => R,\n  ): R[] {\n    const methodNames = new Set(this.getAllFilteredMethodNames(prototype));\n    return iterate(methodNames)\n      .map(callback)\n      .filter(metadata => !isNil(metadata))\n      .toArray();\n  }\n\n  *getAllFilteredMethodNames(prototype: object): IterableIterator<string> {\n    // prototype에 등록된 method 이름들을 가져온다.\n\nscanFromPrototype 는 getAllFilteredMethodNames 메서드로 인스턴스의 모든 메서드 이름들을 가져와 인자로 받은 callback을 실행시킵니다. 이 중에서 메타데이터가 있는 메서드만 필터링합니다.scanFromPrototype 의 callback 파라미터에서 인스턴스 메서드에 접근할 수 있습니다. 이제 메서드에 접근해 데코레이팅 함수로 덮어씌울 수 있습니다.SetMetadata, DiscoveryService, MetadataScanner 모든 재료들이 모였으니 Provider에 접근 가능한 메서드 데코레이터를 만들어봅시다.Cacheable 데코레이터메서드에 CACHEABLE 심볼을 메타데이터 키로, ttl을 메타데이터 값으로 설정합니다.export const CACHEABLE = Symbol('CACHEABLE');\nexport const Cacheable = (ttl: number) => SetMetadata(CACHEABLE, ttl);\n\n@Injectable()\nclass TargetClass {\n  @Cacheable(0)\n  test() {}\n}CacheDecoratorRegister 클래스@Injectable()\nexport class CacheDecoratorRegister implements OnModuleInit {\n  constructor(\n    private readonly discoveryService: DiscoveryService,\n    private readonly metadataScanner: MetadataScanner,\n    private readonly reflector: Reflector,\n    private readonly cache: Cache,\n  ) {}\n\n  onModuleInit() {\n    return this.discoveryService\n      .getProviders() // #1. 모든 provider 조회\n      .filter((wrapper) => wrapper.isDependencyTreeStatic())\n      .filter(({ instance }) => instance && Object.getPrototypeOf(instance))\n      .forEach(({ instance }) => {\n        this.metadataScanner.scanFromPrototype(\n          instance,\n          Object.getPrototypeOf(instance),\n          (methodName) => {\n\t    // #2. 메타데이터 value\n            const ttl = this.reflector.get(CACHEABLE, instance[methodName]);\n            if (!ttl) {\n              return;\n            }\n\n            const methodRef = instance[methodName];\n\n            // #3. 기존 함수 데코레이팅\n            instance[methodName] = async function (...args: any[]) {\n              const name = `${instance.constructor.name}.${methodName}`;\n              const value = await this.cache.get(name, args);\n              if (value) {\n                return value;\n              }\n\n              const result = await methodRef.call(instance, ...args);\n              await this.cache.set(name, args, result, ttl);\n              return result;\n            };\n          },\n        );\n      });\n  }\n}해당 클래스를 모듈의 provider에 등록하면, onModuleInit 단계에서 @Cacheable로 데코레이팅된 메서드를 찾아 기존 메서드를 덮어씌웁니다.메서드 데코레이터를 만드는 과정은 다음과 같습니다.#1. 모든 Provider 클래스를 순회하며#2. 특정 메타데이터가 등록된 메서드를 찾아#3. 기존 메서드를 덮어씌웁니다.#3의 과정에서, CacheDecoratorRegister 생성자에 주입한 CacheManager를 사용할 수 있습니다.그런데 메서드 데코레이터를 만들 때마다 매번 이렇게 복잡한 과정을 거쳐야하는 걸까요? 저희 챕터에서는 메서드 데코레이터마다 반복되는 과정을 AopModule이라는 모듈로 해결했습니다.해당 모듈은 2022년 12월에 오픈소스로 공개되었습니다. 현재 npm에서 @toss/nestjs-aop 라이브러리를 다운 받아 사용해보실 수 있습니다.관련해서 NestJS 밋업에서 발표한 자료도 있으니 함께 참고하시면 좋을 듯 합니다. :)AopModuleAopModule이 데코레이터들을 등록하는 과정은 이렇습니다.간단히 설명하면Aspect 데코레이터가 붙은 클래스를 찾고 (CacheableDecorator)Cacheable 데코레이터가 붙은 함수를 찾아 (FooService.foo)1번 클래스의 wrap 함수로 2번의 함수를 감쌉니다. (CacheableDecorator.wrap)코드를 보며 좀 더 자세히 설명해볼게요.1. Aspect 데코레이터 사용Aspect 데코레이터import { applyDecorators, Injectable } from '@nestjs/common';\n\nexport const ASPECT = Symbol('ASPECT_CLASS');\n\nexport function Aspect() {\n  return applyDecorators(SetMetadata(ASPECT, 'ASPECT_CLASS'), Injectable);\n}데코레이터 사용@Aspect()\nexport class CacheLazyDecorator {}데코레이터 로직을 실행할 클래스에 ASPECT 라는 심볼을 메타데이터로 설정합니다.2. 데코레이터 생성export const CACHEABLE = Symbol('CACHEABLE');\nexport const Cacheable = (ttl: number) => SetMetadata(CACHEABLE, ttl);\n\n\nclass FooService {\n\t@Cacheable(1000)\n\tfoo() {}\n}특정 심볼(또는 문자열)을 메타데이터 키로 하여 SetMetadata로 원하는 데코레이터를 만듭니다.3. LazyDecorator 구현AopModule에 등록되는 모든 데코레이터들은 LazyDecorator 인터페이스를 구현해야 합니다. 데코레이팅 하는 시점을 모듈이 초기화되는 시점으로 미루기 때문에 LazyDecorator라고 합니다.LazyDecorator 인터페이스export interface LazyDecorator {\n  wrap(reflector: Reflector, instance: any, methodName: string): Decorator | undefined;\n}CacheLazyDecorator 구현@Aspect()\nexport class CacheLazyDecorator implements LazyDecorator {\n  constructor(@Inject(CACHE_MANAGER) private readonly cache: CacheManager) {}\n\n  wrap(reflector: Reflector, instance: any, methodName: string) {\n    const ttl = reflector.get(CACHEABLE, instance[methodName]);\n    if (!ttl) {\n      return;\n    }\n\n    const methodRef = instance[methodName];\n    const name = `${instance.constructor.name}.${methodName}`;\n    return async (...args: any[]) => {\n      const value = await this.cache.get(name);\n      if (value) {\n        return value;\n      }\n\n      const result = await methodRef.call(instance, ...args);\n      this.cache.set(name, result, ttl);\n      return result;\n    };\n  }\n}접근하고자 하는 Provider는 이제 생성자에 주입하여 사용할 수 있습니다.4. AutoAspectExecutoronModuleInit 단계에서 AopModule의 AutoAspectExecutor 가 ASPECT가 붙은 데코레이터 클래스들의 wrap 함수를 실행시키며 기존 메서드를 덮어씌웁니다.AutoAspectExecutor 코드@Injectable()\nexport class AutoAspectExecutor implements OnModuleInit {\n  constructor(\n    private readonly discoveryService: DiscoveryService,\n    private readonly metadataScanner: MetadataScanner,\n    private readonly reflector: Reflector,\n  ) {}\n\n  onModuleInit() {\n    const providers = this.discoveryService.getProviders();\n    const lazyDecorators = this.lookupLazyDecorators(providers);\n    if (lazyDecorators.length === 0) {\n      return;\n    }\n\n    providers\n      .filter((wrapper) => wrapper.isDependencyTreeStatic())\n      .filter(({ instance }) => instance && Object.getPrototypeOf(instance))\n      .forEach(({ instance }) => {\n        this.metadataScanner.scanFromPrototype(\n          instance,\n          Object.getPrototypeOf(instance),\n          (methodName) =>\n            lazyDecorators.forEach((lazyDecorator) => {\n              const wrappedMethod = lazyDecorator.wrap(this.reflector, instance, methodName);\n              if (wrappedMethod) {\n                instance[methodName] = wrappedMethod;\n              }\n            }),\n        );\n      });\n  }\n\n  private lookupLazyDecorators(providers: any[]): LazyDecorator[] {\n    // this.reflector.get(ASPECT, metatype) 결과값이 존재하는 providers만 필터링\n  }\n}Provider에 접근 가능한 데코레이터를 만드는 과정을 다시 요약하면 이렇습니다.SetMetadata로 필터링할 클래스에 메타데이터를 등록하고DiscoveryService로 모든 Provider를 조회하며등록된 Metadata로 특정 클래스나 메서드를 필터링하여 원하는 작업을 하면 됩니다.Provider에 접근이 필요없는 경우 일반 메서드 데코레이터를 구현하면 될 것입니다. 하지만 메타데이터를 사용하는 NestJS 데코레이터를 일반 데코레이터와 함께 사용해도 괜찮을까요?결론부터 말하자면 둘을 함께 사용하면 예상치 못한 버그가 발생할 수 있습니다.일반 메서드 데코레이터를 사용하면 안되는 이유메타데이터를 등록하는 다른 데코레이터와 함께 쓰이는 경우, 기존 메서드가 덮어씌워지면서 프로토타입에 등록된 메타데이터가 사라질 수 있습니다.export function OnError(handler: (e: Error) => void) {\n  return (target: object, key?: any, descriptor?: any) => {\n    const originMethod = descriptor.value;\n    descriptor.value = (...args: any[]) => {\n      try {\n        return originMethod.call(this, ...args);\n      } catch (error) {\n        handler(error);\n      }\n    };\n  };\n}OnError 데코레이터는 기존 메서드를 새로운 메서드로 덮어씌웁니다.아래 코드에서는 메타데이터를 등록하는 RegisterMetadata 데코레이터와 OnError 데코레이터를 함께 사용하고 있습니다. 데코레이터 선언 순서에 따라 기존에 등록된 메타데이터는 사라질 수 있습니다.아래 메서드 중에 Reflect.getMetadata를 했을 때 메타데이터가 사라지는 메서드는 무엇일까요?@Injectable()\nclass TestService {\n  @OnError(console.log)\n  @RegisterMetadata('value')\n  test() {\n    throw new Error('error');\n  }\n\n  @RegisterMetadata('value2')\n  @OnError(console.log)\n  test2() {\n    throw new Error('error');\n  }\n}정답은 test 메서드입니다. 실행 결과는 타입스크립트 플레이그라운드에서 직접 확인하실 수 있습니다.const testService = new TestService()\n\n// undefined\nconsole.log('test metadata', Reflect.getMetadata(REGISTER_METADATA, testService.test))\n// value2\nconsole.log('test2 metadata', Reflect.getMetadata(REGISTER_METADATA, testService.test2))왜 이렇게 되는 걸까요? 데코레이터의 실행 순서가 힌트입니다.g∘f(x) = g(f(x)) 와 같은 합성 함수가 있을 때 선언은 g가 f보다 먼저 되었지만 실행은 f 함수가 먼저 실행됩니다. 마찬가지로 데코레이터는 평가될 때는 선언된 순서대로 위에서 아래로, 실행될 때는 아래에서 위로 실행됩니다.RegisterMetadata에서 Reflector.defineMetadata가 먼저 실행되고 그 다음 OnError 데코레이터가 기존 함수를 덮어씌웁니다.덮어씌워지면서 기존에 메타데이터가 저장된 프로토타입과 끊기게 되고 test 메서드에서 메타데이터를 찾을 수 없게 됩니다.이런 사례도 있을 수 있습니다.@Injectable()\nclass TossScheduler {\n\n  @OnError(console.log)\n  @Cron('*/10 * * * *')\n  task() {\n    // do something\n  }\n}@nestjs/schedule 의 Cron 데코레이터 역시 CRON 심볼을 메타데이터로 등록합니다. 모듈이 초기화되는 시점에 해당 메타데이터가 등록된 메서드들을 조회하여 cron job을 등록합니다.하지만 OnError 데코레이터가 Cron 데코레이터 이후에 실행됨으로써 메타데이터가 사라지게 되고, NestJS에서는 task 메서드를 찾지 못해 cron job을 등록하지 못하게 됩니다.이렇듯 일반 메서드 데코레이터를 NestJS 환경에서 그냥 사용하게 되면 개발자의 실수에 의해 코드의 동작이 바뀔 수 있습니다. 데코레이터 실행 순서나 메타데이터 환경에 대해 알고 있지 못하다면 이런 류의 버그를 찾는 데는 시간이 오래 걸릴 지도 모릅니다.이를 방지하기 위해서는 메타데이터를 고려하여 데코레이터를 생성해야 합니다.메타데이터를 유지하는 데코레이터메타데이터를 유지하는 가장 naive한 방법은, 오버라이딩 되기 전에 메타데이터를 저장해둔 뒤 오버라이딩이 끝나면 메타데이터를 다시 등록해주는 것입니다.OnErrorPreserveMeta 코드export function OnErrorPreserveMeta(handler: (e: Error) => void) {\n  return (target: object, key?: any, descriptor?: any) => {\n    const originMethod = descriptor.value;\n\n    //  오버라이딩 되기 전의 메타데이터를 저장해놨다가\n    const metaKeys = Reflect.getOwnMetadataKeys(descriptor.value);\n    const metas = metaKeys.map((k) => [\n      k,\n      Reflect.getMetadata(k, descriptor.value),\n    ]);\n\n    descriptor.value = (...args: any[]) => {\n      try {\n        return originMethod.call(this, ...args);\n      } catch (error) {\n        handler(error);\n      }\n    };\n\n    // 오버라이딩 된 메서드에 대해 메타데이터 재등록\n    metas.forEach(([k, v]) => Reflect.defineMetadata(k, v, descriptor.value));\n  };\n}직관적이지만 매번 Decorator를 만들어줄 때마다 이런 과정을 거쳐야 하는 게 불편합니다. 이를 해결하는 좀 더 간단한 방법이 있습니다.프로토타입을 사용해 메타데이터 유지하기SetMetadata 파트에서 Reflect.defineMetadata 는 타겟 객체에 [[Metadata]] 라는 내부 슬롯을 정의한다고 말씀드렸습니다.내부 슬롯 또한 프로토타입의 내부 프로퍼티이니, 기존 프로토타입에 메타데이터 내부 슬롯이 저장되어있을 것입니다. 따라서 새롭게 정의한 메서드에 기존 프로토타입을 연결해주면 됩니다.변경된 OnErrorPreserveMeta 코드export function OnErrorPreserveMeta(handler: (e: Error) => void) {\n  return (target: object, key?: any, descriptor?: any) => {\n    const originMethod = descriptor.value;\n\n    const wrapper = (...args: any[]) => {\n      try {\n        return originMethod.call(this, ...args);\n      } catch (error) {\n        handler(error);\n      }\n    };\n\n    Object.setPrototypeOf(wrapper, originMethod); // 이 줄만 추가\n    descriptor.value = wrapper;\n  };\n}Object.setPrototypeOf(arg1, arg2) 은 arg1 객체의 프로토타입을 arg2로 설정합니다.기존 메서드를 덮어씌운 후 Object.setPrototypeOf(wrapper, originMethod)로 originMethod를 wrapper의 프로토타입으로 설정해주면 메타데이터가 유지됩니다.@Injectable()\nclass TestService {\n  @OnError(console.log)\n  @RegisterMetadata('value')\n  test() {\n    throw new Error('error');\n  }\n}\n\nconst testService = new TestService()\n\nconsole.log('test metadata', Reflect.getMetadata(REGISTER_METADATA, testService.test)) // 'value'메타데이터와 NestJS의 DiscoveryModule 을 사용하여 NestJS의 IoC 컨테이너에 접근할 수 있는 데코레이터, 그리고 메타데이터를 유지할 수 있는 데코레이터를 만들어보았습니다.메타데이터 태깅, DiscoveryModule, 프로토타입을 사용해 NestJS 환경에 맞는 데코레이터를 만들 수 있었습니다. 이 글을 통해 더욱 더 NestJS의 Aop 패턴에 맞는 프로그래밍을 하게 되었기를 바랍니다.또한 토스 Node.js 챕터는 토스의 다양한 제품과 라이브러리 개발을 위해 팀원들의 지속적인 성장이 중요하다고 믿으며, 이를 위해 코드 리뷰, 스터디와 엔지니어링 세미나 등을 통해 꾸준히 공부하고 공유하는 자리를 가지고 있으니 많은 관심 부탁드립니다.Referenceshttps://zuminternet.github.io/nestjs-custom-decorator/https://github.com/nestjs/nesthttps://github.com/nestjs/swagger/issues/217https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/setPrototypeOf재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "15": "토스증권 QA 문화 ‘통합테스트’를 아시나요? (feat. 해외주식)황채은ㆍQA Manager2022. 12. 12안녕하세요. 토스증권 QA팀에서 일하고 있는 QA Manager 황채은입니다.지난 글에서 잠깐 언급되었던 토스증권의 Closed Beta Test 기억하실까요? 🔗지난 포스팅 참고하기 링크오늘은 해외 증권 서비스 출시를 위해 토스증권 임직원 대상으로 진행한 Closed Beta Test 에 대해 소개하려고 합니다.토스증권 서비스는 어떤 테스트 과정을 통해 출시되고 있을까요?토스증권에서는 기능별로 Silo가 만들어져있고, 그 Silo에는 PO, Developer, PD, DA, QA가 함께 서비스를 만들고 있는데요.PO(Product Owner), 개발자(Front/Server/Core), PD(Product Designer), DA(Data Analyst), QA(Quality Assurance)개발이 완료된 서비스들은 1차로 개발자 단위 테스트를 진행 후 QA Manager는 알파 환경에서 계획한 테스트를 진행하고, 이슈 수정이 모두 완료되면 QA 환경에서 최종 확인된 기능에 대해 서비스 Release를 Silo 별로 진행하고 있습니다. 이런 과정은 일반적인 IT 회사의 QA Cycle과 비슷하게 진행하고 있어요.하지만, 토스증권에서는 내부 임직원들 대상으로 테스트 참여를 유도하여 미처 발견하지 못한 Edge Case 들과 보다 다양한 사용자의 피드백을 미리 얻고자  Closed Beta Test를 진행하기도 한답니다. 실제 제가 작년에 진행했던 토스증권의 해외 주식 서비스 런칭 경험을 소개해 드리려고 해요.토스증권에서 진행한 내부 임직원 테스트는 아래의 다양한 테스트 방식의 개념을 조금씩 섞어서 진행했어요.QA라면 모두가 알고 있는 탐색적 테스팅의 개념, Dog Fooding, Bug Bash를 모두 활용하였는데요. 그전에 개념들을 한번 언급하고 지나갈게요!Closed Beta Test란?비공개 베타 테스트로, 서비스를 정식으로 오픈하기 전에 프로그램의 버그를 찾거나 사용성, 요구사항 충족 등을 검증하기 위해 개발자와 관련되지 않은 사용자에 의해 진행되는 테스트탐색적 테스팅이란?테스트 대상 제품을 사용하면서 제품의 정보 습득과 동시에 테스트를 설계하고 실행하는 방식으로 탐색적 테스팅의 주요 구성요소는 Test Charter, Test Note, Time Boxing, Debrief가 있음Dog Fooding 이란?서비스 출시 전 사용자의 입장에서 내부 인원이나 개발자가 직접 사용하며 제품을 개선하는 것Bug Bash란?제품이 최종 릴리즈되기 전 다양한 직군의 인원들이 제품을 사용하며 아직 남아있을 수 있는 버그를 찾아내는 활동으로 짧은 시간에 많은 인원이 진행하기 때문에 상대적으로 빠르게 버그를 찾을 수 있음토스증권의 Closed Beta Test는 어떻게 진행되었을까요?업무시간에 임직원 대상으로 진행하는 만큼, 효율적으로 진행하기 위해 여러 준비과정이 필요했는데요.개개인이 자율적으로 참석 가능한 임직원의 신청을 받았는데, 개발자를 제외한 임직원 중에서 70% 이상이 참여할 정도로 많은 관심을 가져주셨어요.서비스에 대해 여러 피드백을 받기 위해 직군에 관계없이 참여가 가능한 분들은 모두 신청을 받았는데요. 그러다보니, 알파 환경 자체를 처음 접해보는 임직원분들도 있어 사용성에 어려움이 많아 사전 테스트 환경 준비가 중요했어요. 마치, 실제 서비스가 런칭되었을 때와 동일하게 사용하는 경험을 주기 위해 알파 테스트 환경이지만 운영과 동일한 환경에서 볼 수 있도록 테스트 환경을 마련하였는데, 모두의 시간을 할애하는 만큼 테스트 환경을 위해 InfraOps, Network Engineer, Devops 팀에서 테스트 환경 구축에 많은 도움을 주셨어요.신청자 대다수가 토스증권 해외 주식에 대한 기본적인 Spec을 전혀 모르기 때문에 테스트 시나리오를 제공해야했어요. 그래서, 탐색적 테스팅의 Test Charter 개념을 섞어서 Test Scenario를 Use Case 형식으로 작성했는데요. 서비스의 주요 퍼널을 통과할 수 있게 작성하고 QA Team의 Peer Review를 통해 시나리오를 준비하였어요.시나리오는 총 36건 작성을 하였고, 해외 증권 거래소 시간에 맞게 테스트를 진행해야 해서, Use Case를 테스트할 수 있는 Timeline을 설정했어요. 해외 증권 거래소의 테스트 거래소와 연동을 하여 실제 주식이 체결되는 것처럼 보이게 환경을 만들어 두었기 때문에, 먼저 증권 거래소 운영 시간에 맞추고 주식의 매수/매도 유형에 맞춰 시나리오를 만들었는데요. 그래서 해외 주식 서비스는 밤 11시가 넘은 심야에도 계속 진행되어야 했어요.해당 Timeline에 맞게 임직원 모두가 같은 시간대에 테스트를 하는 것을 유도하기 위해 캘린더 초대도 잊지 않았어요! (참석자 모두, 심야 시간인데도 불구하고 수락을 눌러주셨어요!)이렇게 준비된 테스트 형식을 모두가 이해하기 쉽게 “통합테스트” 라는 용어로 정리하였어요!이제 마지막으로 테스트 진행 관련된 사항을 어떻게 공유할지, 어떻게 Debrief 할지를 정해야 했는데요.토스증권은 업무를 보다 빠르고 효율적으로 진행할 수 있도록 만들어진 다양한 사내 툴들이 있어요.대표적으로 Slack에서 JIRA(Bug Tracking System)를 바로 연동하여 이슈를 생성하고 완료까지 처리할 수 있는 (Bot)이 있어요. 또 Slack의 Workflow에서 Emoji를 남기면, 해당 게시글이 다른 채널로 전달되는 기능을 활용해서 보다 편하게 이슈를 모아볼 수 있었어요.글로 설명하려니 이해가 잘 안되시죠?그러면, 이제 저와 함께 통합테스트를 함께 진행해 볼까요?완료 후 미션에 DONE Emoji를 남기면 Thread에 완료 표기미션 중 이슈로 인해 진행하지 못하는 경우 SOS Emoji를 남기면 Thread에 표기이슈에 Emoji를 남기면 별도 채널로 이동되고, 해당 채널에 이슈 등록이런 미션의 형태로 진행하니, 임직원들도 게임에 참여하듯이 (미션 클리어! 효과음이 들리시나요?) 즐겁고 적극적으로 진행해 주셨어요. 또한 테스트를 진행하는 동안 이전에 사용하였던 해외증권 서비스의 경험을 비롯한 다양한 의견들을 제시해 주셨어요.그렇게 통합테스트 기간 동안 약 100개가 넘는 피드백이 등록되었는데요. 👀전달된 의견들은 실제로 Bug나 결함보다는 사용자로서의 의견에 더 가까운 내용들이 많았어요. 제시해 주신 여러 의견들을 통해 오픈 전 서비스의 사용성 및 품질을 더 높일 수 있었다고 생각해요.3일이라는 짧다면 짧고 길다면 긴 기간 동안 모두가 즐겁게 테스트에 참여해 주셨어요.서비스 출시 전, 통합테스트는 어떤 의미였을까요?임직원 대상으로 진행한 통합테스트는 QA팀에서 QA Plan에 맞게 여러 테스트 Iteration을 수행하고, 개발 Side의 단위 테스트, 성능 테스트가 진행이 되었다 하더라도 우리가 만든 서비스가 만든 의도에 맞게 동작하는지에 대해 미리 임직원 대상으로 실험할 수 있었어요. 우리가 만들었던 기능 중에서 우려했던 부분 역시 다시 드러나면서 개선이 더 필요한 부분에 대해 Maker 들과 공감대 형성에 가장 큰 역할이 되었습니다.또한, 우리가 사용자에게 의도한 대로 동작하는 부분이 제대로 동작하는지까지 미리 알 수 있었고 그 장치들을 더 효과 있게 바꾸는 계기도 되었어요.토스증권 QA팀에서는 정량적인 서비스 품질뿐만 아니라, 회사에서 품질을 생각하는 문화에 대해서도 모두가 참여하도록 유도하고 서비스 품질에 대한 정성적인 기준에 대해 다 같이 고민하는 시간을 만들어가는 과정에도 주도적으로 참여해야 한다고 생각합니다.그래서 bugbash, closed beta testing, 탐색적 테스팅 등과 같이, QA라면 흔히 알고 있는 이런 여러 테스트 방식 중에서 필요한 부분으로 토스증권만의 ‘통합테스트’ 라는 컨셉을 만들었어요. 또한, 임직원 70% 이상이 참여하도록 만들었고 그 결과 또한 성황리에 마무리할 수 있었어요.이후로 임직원과 함께하는 통합테스트는 토스증권의 QA 문화로 자리매김할 수 있었고, 지금도 큰 서비스가 출시될 때마다 통합테스트를 주기적으로 진행하고 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "16": "똑똑하게 브라우저 Polyfill 관리하기박서진ㆍFrontend Developer2023. 1. 21토스 앱은 넓은 범위의 기기를 지원하면서도 현대적인 JavaScript를 이용해서 개발되고 있습니다. 그렇지만 최신 JavaScript를 오래된 브라우저 위에서 실행하기 위해서는 “Polyfill” 문제를 해결해야 하는데요.이번 아티클에서는 Polyfill 문제가 무엇인지 알아보고, 토스에서 어떻게 똑똑하게 다루고 있는지 살펴보려고 합니다.Polyfill이란?오래된 버전의 브라우저에서는 현재 JavaScript가 당연하게 사용하고 있는 Promise나 Set 객체가 없는 경우가 있습니다. 편리한 Array.prototype.at() API는 Chrome 92 이상에서만 지원되기도 합니다.예를 들어서, 아래와 같은 코드는 최신 브라우저에서는 잘 동작하지만, 오래된 브라우저에서는 실패합니다. 객체나 메서드에 대한 구현이 없기 때문이죠.[1, 2, 3].at(-1);\n\nPromise.resolve(1);\n\nnew Set(1, 2, 3);이런 문제를 해결하기 위해서는 오래된 브라우저에서 없는 구현을 채워주어야 합니다. 이렇게 구현을 채워주는 스크립트를 Polyfill이라고 합니다. 대부분의 Polyfill은 아래와 같이 이미 브라우저에 포함되어 있는지 체크하고, 없으면 값을 채워주는 형태로 동작합니다.Array.prototype.at = Array.prototype.at ?? /* Array.prototype.at에 대한 자체 구현 */;\n\n위 스크립트를 실행한 이후에는, 오래된 브라우저에서도 안전하게 [1, 2, 3].at(-1) 코드를 실행할 수 있습니다.표준적으로 사용되는 Polyfill들은 core-js 리포지토리에 모여 있습니다. 아래 코드를 실행하면 대부분의 ECMAScript 표준 객체와 메서드를 오래된 브라우저에서도 사용할 수 있게 됩니다.import 'core-js/actual';Polyfill의 문제위와 같이 코드를 작성하면 폭넓은 브라우저를 지원할 수 있다는 장점이 있지만 문제가 하나 생깁니다. 불러와야 하는 JavaScript 코드가 많아진다는 점입니다. 실행해야 하는 Polyfill 스크립트가 많아질수록 사용자가 경험하는 웹 서비스의 성능은 나빠집니다.특히, 위와 같이 설정하면 최신 버전의 브라우저에서는 대부분의 ECMAScript 표준 객체와 메서드가 포함되어 있음에도 불구하고 불필요한 Polyfill 스크립트를 내려받아야 합니다. 꼭 필요한 Polyfill 스크립트만 선택적으로 불러올 수 있는 방법은 없을까요?첫 번째 방법: @babel/preset-env 사용하기이 문제를 해결하기 위해 사용할 수 있는 첫 번째 방법은 @babel/preset-env Smart Preset을 사용하는 것입니다. 이 Smart Preset은 이미 정의된 브라우저 목록에 따라서 자동으로 필요 없는 Polyfill을 제거해 줍니다.예를 들어서, 웹 페이지가 Internet Explorer 11을 지원해야 한다면 아래와 같이 babel.config.js 를 설정할 수 있습니다.module.exports = {\n  presets: [\n    ['@babel/preset-env', { targets: { ie: 11 } }],\n  ],\n  /* 그 외의 설정 */\n};\n\n이후에 동일하게 core-js/actual 을 import 하더라도 Internet Explorer 11에 필요한 Polyfill 목록만 포함되는 것을 확인할 수 있습니다. 총 221개의 Polyfill이 포함됩니다.// 입력 코드\nimport 'core-js/actual';// 출력 코드\nrequire(\"core-js/modules/es.symbol.js\");\nrequire(\"core-js/modules/es.symbol.description.js\");\nrequire(\"core-js/modules/es.symbol.async-iterator.js\");\nrequire(\"core-js/modules/es.symbol.has-instance.js\");\nrequire(\"core-js/modules/es.symbol.is-concat-spreadable.js\");\nrequire(\"core-js/modules/es.symbol.iterator.js\");\n// ... 계속 (총 221개의 Polyfill)Babel playgroundInternet Explorer 11을 지원 브라우저 목록에서 제외하면 훨씬 적은 25개의 Polyfill이 포함됩니다.module.exports = {\n  presets: [\n    ['@babel/preset-env', { targets: 'defaults, not ie 11' }],\n  ],\n  /* 그 외의 설정 */\n};// 입력 코드\nimport 'core-js/actual';// 출력 코드\nrequire(\"core-js/modules/es.error.cause.js\");\nrequire(\"core-js/modules/es.aggregate-error.cause.js\");\nrequire(\"core-js/modules/es.array.at.js\");\nrequire(\"core-js/modules/es.array.includes.js\");\nrequire(\"core-js/modules/es.object.has-own.js\");\nrequire(\"core-js/modules/es.regexp.flags.js\");\nrequire(\"core-js/modules/es.string.at-alternative.js\");\nrequire(\"core-js/modules/es.typed-array.at.js\");\nrequire(\"core-js/modules/esnext.array.find-last.js\");\n// ... 계속 (총 25개의 Polyfill)\n\nBabel playground이렇게 @babel/preset-env에 브라우저 지원 범위를 설정하면 Polyfill을 안정적으로 포함하면서 스크립트의 크기를 감축할 수 있습니다.두 번째 방법: User-agent에 따라 동적으로 스크립트 생성하기Babel을 올바르게 설정함으로써 포함되는 Polyfill 스크립트의 크기를 줄일 수 있지만, 최신 버전의 브라우저에서 불필요한 스크립트를 내려받게 되는 문제는 동일합니다. 예를 들어서, Chrome 최신 버전은 문제없이 [1, 2, 3].at(-1) 을 실행할 수 있지만, 관련한 Polyfill 스크립트를 내려받습니다.이 문제를 해결하는 또다른 방법은 브라우저의 User-agent에 따라서 동적으로 Polyfill 스크립트를 생성하는 것입니다.예를 들어서, Financial Times에서 관리하고 있는 polyfill.io 서비스에서는 https://polyfill.io/v3/polyfill.min.js 라고 하는 경로로 동적인 Polyfill 스크립트를 제공합니다.최신 버전의 Chrome에서 해당 경로에 접속하면, 아무 Polyfill 스크립트도 내려오지 않는다는 것을 알 수 있습니다.$ curl -XGET \"https://polyfill.io/v3/polyfill.min.js\" \\\n   -H \"User-Agent: Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Mobile Safari/537.36\" \\\n   -v\n/* 빈 스크립트 */반대로, Internet Explorer 11에서 실행하면 많은 양의 Polyfill 스크립트가 내려온다는 것을 알 수 있습니다.$ curl -XGET \"https://polyfill.io/v3/polyfill.min.js\" \\\n   -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko\" \\\n   -v\n(function(self, undefined) {!function(t){t.DocumentFragment=function n(){return document.createDocumentFragment()\n\n이렇게 User-agent에 따라 동적으로 Polyfill 스크립트를 생성하면 최신 브라우저에서는 아무 Polyfill도 내려주지 않고, 오래된 브라우저에서는 필요한 Polyfill 만 내려줄 수 있게 됩니다. ✨  브라우저가 꼭 필요한 Polyfill 스크립트만 내려받을 수 있는 것이죠.자체 Polyfill 서비스 구축하기토스에서는 polyfill.io 서비스를 그대로 사용할 수도 있었지만, Financial Times가 제공하는 Polyfill 중 일부가 ECMAScript 표준대로 작동하지 않아 오류가 발생한 경험이 있어서 자체적으로 구현했습니다.core-js 와 core-js-compat, browserslist-useragent 라이브러리를 사용하면 손쉽게 동적인 Polyfill을 제공하는 Node.js 서버를 만들 수 있었습니다.먼저, User-agent에 따라서 필요한 core-js polyfill 목록을 계산하기 위해서 아래와 같은 getCoreJSPolyfillList 함수를 작성할 수 있습니다.import { resolveUserAgent } from 'browserslist-useragent';\nimport compat from 'core-js-compat';\n\n/**\n * userAgent에 따라 필요한 Polyfill의 목록을 반환합니다.\n * e.g. ['es.symbol', 'es.symbol.description', 'es.symbol.async-iterator']\n */\nfunction getCoreJSPolyfillList(userAgent: string) {\n  try {\n    const result = resolveUserAgent(userAgent);\n    const majorVersion = parseMajorVersion(result.version);\n\n    return compat({\n      targets: `${result.family} >= ${majorVersion}`,\n      version: coreJSVersion,\n    }).list;\n  } catch {\n    // 일반적이지 않은 User-Agent인 경우\n    return compat({\n      targets: 'IE >= 11',\n      version: coreJSVersion,\n    }).list;\n  }\n}\n\nfunction parseMajorVersion(versionString: string) {\n  const match = versionString.match(/^(\\\\d+)\\\\.*/);\n\n  if (match == null) {\n    return versionString;\n  }\n\n  return match[1];\n}\n\n이제 필요한 Polyfill 리스트를 하나의 스크립트로 만들면 됩니다. 토스에서는 esbuild 를 이용하여 core-js 스크립트를 하나로 이어붙이는 방법을 선택했습니다.import { build } from 'esbuild';\n\n/*\n * userAgent에 맞는 완성된 Polyfill 스크립트를 생성한다.\n */\nasync function buildPolyfillScript(userAgent: string) {\n  const script = getCoreJSPolyfillScript(userAgent);\n\n  const result = await build({\n    stdin: {\n      contents: script,\n      loader: 'js',\n    },\n    target: 'es5',\n    bundle: true,\n    minify: true,\n    write: false,\n  });\n\n  return result.outputFiles[0].contents;\n}\n\nfunction createCoreJSPolyfillScript(userAgent: string) {\n  return getCoreJSPolyfillList(userAgent)\n    /* 실험적인 esnext 기능은 제외합니다. */\n    .filter(x => !x.startsWith('esnext.'))\n    .map(item => `import \"core-js/modules/${item}\";`)\n    .join('\\\\n');\n};\n\n이제 이 함수를 Node.js 서버에 포함시키거나, Lambda@Edge, Compute@Edge 와 같은 Edge Runtime에 포함하면 손쉽게 나만의 Polyfill 서버를 띄울 수 있습니다.마치며토스팀에서는 자체 제작한 Polyfill 시스템을 이용하여 최신 JavaScript API는 마음껏 활용하면서도 오래된 버전의 브라우저도 빠짐없이 지원할 수 있었습니다.글을 마무리하면서, 글의 내용을 요약해보자면 아래와 같습니다.Polyfill이란 신규 JavaScript API를 오래된 버전의 브라우저에서도 사용할 수 있도록 하는 방법입니다. 그렇지만, Polyfill 스크립트가 많아지면 웹 성능이 나빠집니다.Babel의 @babel/preset-env 스마트 프리셋을 이용하여 포함할 Polyfill 스크립트의 범위를 지정할 수 있습니다. 다만, 이 경우에도 최신 브라우저는 오래된 브라우저를 위한 Polyfill을 내려받습니다.User-agent에 따라 동적으로 Polyfill 스크립트를 생성할 수 있습니다. 이로써 최신 브라우저에서 내려받는 Polyfill 스크립트를 거의 없게 만들 수 있습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "17": "선언적인 코드 작성하기박서진ㆍFrontend Developer2023. 3. 16선언적인 코드(Declarative Code)는 프론트엔드 개발을 하다 보면 자주 만나게 되는 개념입니다. 특히 React 생태계에서 웹 서비스를 개발하다 보면 선언적인 코드에 대해 고민하게 되는데요. 이번 아티클에서는 토스 프론트엔드 챕터에서 생각하는 선언적인 코드란 무엇인지, 그리고 실제로 어떻게 선언적인 코드를 작성하는지 공유해드리려고 합니다.선언적인 코드토스 프론트엔드 챕터에서는 선언적인 코드를 “추상화 레벨이 높아진 코드”로 생각하고 있습니다. 예를 들어서, 아래와 같이 주어진 배열의 합을 구하는 함수 sum 을 생각해봅시다.sum([1, 2, 3]);sum 함수는 아래와 같이 for 문으로 구현할 수 있습니다.function sum(nums: number[]) {\n  let result = 0;\n\n  for (const num of nums) {\n    result += num;\n  }\n\n  return result;\n}여기에서 sum 함수는 초기값이 0이고, 배열이 가지고 있는 각각의 원소를 순회하면서 결과값에 더하는 작업을 추상화합니다. 덕분에 sum 을 다루는 사람은 복잡한 제어 흐름을 이해할 필요 없이, “배열의 합을 구한다” 라고 하는 동작에 집중하여 함수를 사용할 수 있습니다.토스는 이렇게 동작에 집중하여 추상화된 sum 함수를 선언적인 코드로 생각하고 있습니다.여기에서 한 걸음 더 나아가서 sum 함수 내부의 for ... of 문을 살펴봅시다.for (const num of nums) {\n  /* 동작 ... */\n}이 제어 흐름도 선언적인 코드로 볼 수 있습니다. 배열이 가지고 있는 각각의 요소를 순회하는 동작을 추상화하고 있기 때문입니다.실제로 ECMAScript 표준에 따라서 for ... of 가 추상화하는 로직을 그대로 드러내면 아래와 같이 나타낼 수 있습니다.const iterator = nums[Symbol.iterator]();\n\nlet step;\n\nwhile (!(step = iterator.next()).done) {\n  const num = step;\n  /* 동작 ... */\n}위와 같이, for ... of 문은 Iterator를 생성하고, Iterator가 끝날 때까지 다음 요소를 차례차례 가져오는 작업을 “각각의 요소를 순회하는 작업”으로 추상화합니다. 이런 관점에서 봤을 때, for ... of 문은 선언적인 코드입니다.실제로는 생성된 Iterator를 삭제하는 동작도 존재하므로 보다 추상화되는 로직이 많습니다.코드의 관점을 벗어나면 보다 재미있는 예시를 생각할 수 있습니다.“왼쪽으로 10걸음 걸어라” 라고 하는 말을 생각합시다. 여기에서“왼쪽”은 “북쪽을 바라보았을 때 90도 돌아간 위치” 를 추상화한 것입니다.“90도”는 “한 번의 회전을 360등분한 각의 90배만큼 시초선에 대해 시계 반대 방향으로 돌아간 것” 을 추상화한 것입니다.“시계 방향” 의 정의는 “북반구에서 해시계의 바늘이 돌아가는 방향” 을 추상화한 것입니다.그래서 “왼쪽으로 10걸음 걸어라”는 사실 “북쪽을 바라보았을 때 한 번의 회전을 360등분한 각의 90배만큼 북반구에서 해시계의 바늘이 돌아가는 방향으로 돌아서, 동물이 육상에서 다리를 이용해 움직이는 가장 빠른 방법보다 느린, 신체를 한 지점에서 다른 지점으로 옮겨가는 행위를 10번 반복해라” 라는 말을 추상화한, 선언적인 말로 볼 수 있을 것입니다.좋은 선언적인 코드 작성하기위에서 선언적인 코드를 추상화 레벨이 높아진 코드로 살펴보았습니다. 그런데 선언적인 코드는 항상 좋은 것일까요? 토스에서는 추상화가 항상 좋은 것은 아닌 것처럼 선언적인 코드도 잘 쓰는 것이 중요하다고 생각합니다.토스에서는 좋은 코드를 판단하는 제1원칙을 “수정하기 쉬운 코드”라고 생각합니다. 비즈니스 요구사항은 항상 빠르게 변하기 때문에, 개발자가 기민하게 대응하는 것이 중요하기 때문입니다. 그러면 선언적인 코드가 언제 수정하기 쉽고, 언제 그렇지 않은지 살펴봅시다.먼저 아래와 같은 회원가입 폼 컴포넌트를 살펴봅시다.<SignUpForm\n  onSubmit={result => {\n    /* 회원가입 결과에 따라서 특정 동작 수행 ... */\n  }}\n/>위 컴포넌트는 회원가입 로직을 하나의 컴포넌트로 추상화했기 때문에 선언적인 컴포넌트로 볼 수 있습니다.이 코드는 수정하기 쉬울까요?먼저 회원가입 폼을 여러 곳에서 사용한다면 각각의 폼을 중복해서 개발할 필요 없이 한 번만 개발하면 되기 때문에 효율적일 것입니다. 또한 회원가입 폼에 변경이 생긴다고 하더라도, 한 곳에서만 바꾸면 다른 화면들에 모두 반영되기 때문에 빨리 수정할 수 있을 것입니다.수정하기 어려운 지점은 없을까요?화면마다 SignUpForm 이 조금씩 다르다면, 공통화된 것이 오히려 코드의 복잡함을 가져올 수도 있습니다. 예를 들어서, 어떤 페이지에서는 SNS 회원가입을 일반 이메일 회원가입보다 먼저 보여줘야 할 수 있습니다. 또, 다른 페이지에서는 텍스트나 스타일을 조금씩 다르게 보여줘야 할 수 있습니다.아래와 같이 SignUpForm 에서 바뀔 수 있는 부분이 많다면, 내부 구현과 인터페이스도 복잡해지고, 쓰는 쪽에서도 불편할 것입니다.<SignUpForm\n  signUpOrder={['sns', 'normal']\n  title=\"사이트에 어서 오세요\"\n  subtitle=\"먼저 회원가입을 해주세요.\"\n  primaryButtonColor={colors.blue}\n  secondaryButtonColor={colors.grey}\n  /* 많은 Prop 들 ... */\n  onCancel={/* ... */}\n  onSubmit={result => {\n    /* 회원가입 결과에 따라서 특정 동작 수행 ... */\n  }}\n\n이처럼 토스에서는 선언적인 코드가 항상 좋은 것이 아니라, 앞으로 제품이 어떻게 변화할지, 비즈니스 요구사항이 어떻게 되는지에 따라서 달라질 수 있다고 생각하고 있습니다. 앞으로 코드의 어떤 부분이 수정될지 예측하고, 이에 따라 적절한 선언 레벨을 따르는 코드를 작성할 필요가 있습니다.토스의 선언적 라이브러리그렇다면 다양한 상황에서 일반적으로 사용할 수 있는 좋은 선언적 코드는 없을까요? 이번 아티클은 토스 프론트엔드 챕터가 100개가 넘는 서비스들에서 자주 사용하고 있는 선언적 라이브러리에 대해서 소개하고 마무리하려고 합니다.useOverlay토스에서는 BottomSheet, Dialog, Toast와 같이 화면 위에 뜨는 오버레이를 띄워야 하는 상황이 많습니다. 토스는 이렇게 오버레이를 띄우는 동작을 추상화하여 useOverlay 라고 하는 Hook 을 사용합니다.const overlay = useOverlay();\n\n<button\n  onClick={() => {\n    overlay.open(({ isOpen, close }) => {\n      return (\n        <BottomSheet open={isOpen} onClose={close}>\n          나는 바텀시트야\n        </BottomSheet>\n      );\n    })\n  }}\n>\n  바텀시트 열기\n</button>예를 들어서, 위 코드에서는 바텀시트 열기 버튼을 누르면 나는 바텀시트야 라고 하는 바텀시트를 띄웁니다.useOverlay 가 없었더라면 아래와 같이 제어 흐름이 드러나는 코드를 작성했어야 할 것입니다.const [isSheetOpen, setIsSheetOpen] = useState(false);\n\n<button onClick={() => setIsSheetOpen(true)}>\n  바텀시트 열기\n</button>\n<BottomSheet open={isSheetOpen} onClose={() => setIsSheetOpen(false)}>\n  나는 바텀시트야\n</BottomSheet>\n\nuseOverlay에 대한 자세한 정보는 Slash libraries의 useOverlay Hook을 참고해주세요.ImpressionArea토스 앱에서는 어떤 영역이 보여졌는지/숨겨졌는지에 따라서 동작하는 로직이 많습니다. 예를 들어서, 사용자가 특정한 요소를 보면 폭죽을 터뜨리거나 토스트를 보여주는 식이죠. 토스에서는 ImpressionArea 라고 하는 컴포넌트로 이를 추상화하고 있습니다.<ImpressionArea onImpressionStart={() => { /* 보여졌을 때 실행 */ }}>\n  <div>내가 보여졌으면 onImpressionStart가 실행돼</div>\n</ImpressionArea>\n\nImpressionArea 가 없었더라면, 복잡한 IntersectionObserver API 를 사용하거나, 복잡한 Scroll 이벤트 핸들러 로직을 사용해야 했을 것입니다.ImpressionArea 에 대한 자세한 정보는 Slash libraries의 ImpressionArea 컴포넌트를 참고해주세요.LoggingClick토스에서는 데이터 주도 의사결정을 위해서 화면에 진입하는 사용자가 몇 명인지, 그 중 몇 명이 버튼을 누르는지를 기록하는 경우가 있습니다. 이런 누르는 동작에 대한 기록을 추상화하여 LoggingClick 컴포넌트를 사용하고 있습니다.<LoggingClick params={{ price }}>\n  <button onClick={buy}>사기</button>\n</LoggingClick>\n\n예를 들어서, 위 코드에서 사용자가 버튼을 누르면 “사기” 버튼에 대한 동작이 분석 시스템에 기록됩니다.LoggingClick 이 없었다면 아래와 같이 log 함수를 실행하는 것이 그대로 드러났어야 할 것입니다.<button\n  onClick={() => {\n    log({ title: '사기', price });\n    buy();\n  }}\n>\n  사기\n</button>\n\n재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "18": "tosspayments-restdocs: 선언형 문서 작성 라이브러리이준희ㆍServer Developer2023. 3. 22최소한의 코드로 문서 작성하기들어가며토스페이먼츠에서는 두 가지 장점 때문에 Spring REST Docs를 권장하고 있는데요. 첫 번째는 문서 작성 단계부터 API를 통합 테스트할 수 있다는 점, 두 번째는 인터페이스의 의도치 않은 변경을 감지할 수 있다는 점 때문입니다.하지만 Spring REST Docs에는 단점도 있습니다. 장황한 코드 때문에 가독성이 떨어지고, 코드 반복으로 인해 생산성이 떨어지는 아쉬움이 있었습니다. 이런 문제를 해소하기 위해 Kotlin DSL을 구현해서 반복적이고 지루한 Spring REST Docs 코드 작성에 필요한 노력을 줄이는 방법을 한규주님의 이전 글 'Kotlin으로 DSL 만들기: 반복적이고 지루한 REST Docs 벗어나기'에서 소개했었습니다.이번에는 나아가 더 높은 가독성, 더 최소화된 코드 중복, 세부 구현 및 의존성 은닉, 마지막으로 확장에 열려있는 특성을 갖춘 문서화 라이브러리 'tosspayments-restdocs'를 소개하고 개발 후기를 공유합니다.동일한 컨트롤러를 Spring REST Docs(좌측)와 tosspayments-restdocs(우측)로 문서화했을 때의 차이. 작성한 문서화 코드의 양이 크게 감소했다.다시 살펴보기: Spring REST Docs의 문제들Spring REST Docs 기반 문서화 코드의 아쉬운 점을 아래 코드를 통해 다시 한번 살펴볼게요.요청 필드 2개, 응답 필드 3개로 구성된 단순한 PUT 인터페이스지만, 장황하게 작성된 코드 때문에 전체 구조를 한눈에 파악하기 어렵습니다. 이런 구조의 코드는 처음 작성하는 비용이 많이 들 뿐만 아니라 유지보수 비용도 늘립니다. 위 코드의 문제점을 좀 더 구체적으로 살펴보겠습니다.1. 코드 중복먼저, 같은 내용이 반복적으로 명세에 포함되어 있습니다. 예를 들어 MockMvcResponse와 REST Docs Snippet을 만들기 위해 Path Variable, 그리고 Request Body Field 코드 중복이 발생했습니다.또한 Request Body값을 MockMvc에 전달하는 과정에서 샘플 데이터를 통해 필드의 타입 및 샘플이 추론 가능함에도 REST Docs Snippet에 불필요하게 다시 명세하고 있습니다.2. 불필요한 명령given(), prettyPrint(), then(), preprocessRequest(), preprocessResponse() 등의 메서드는 인터페이스 명세에 꼭 필요한 핵심이라 하기 어렵습니다. 문서 작성과 관계 없이 빌드에 필요하기 때문에 추가된 사항입니다.3. 인터페이스 명세 순서실제 HTTP 프로토콜에서는 Request Line → Request Header → Request Body → Response Line → Response Header → Response Body 순으로 페이로드가 만들어집니다.그러나 위의 예시 코드는 이러한 흐름에 맞춘 자연스러운 명세 작성 대신, MockMvc를 구성하는 기반 기술에 의존하는 명령을 나열하는데 집중하고 있습니다. 그래서 코드를 작성할 때나 읽을 때 모두 코드 블록을 왔다갔다 해야 하는 불편을 겪습니다.4. 기반 기술에 강한 의존기반 기술에 강하게 의존하고 있기 때문에 기반 기술에 브레이킹 체인지가 생기거나, 유지보수가 중단되면 대응이 어렵습니다.또, 이렇게 기반 기술에 많이 의존하게 되면 문서를 작성하는 모든 개발자가 기반 기술의 세부 항목을 학습해야 합니다. 가령 MultiPart API, Streaming API, Reactive API 등 다른 형태의 API를 문서화할 때마다 개발자는 사용할 MockMvc의 세부 기능을 각각 학습해야 합니다. 이런 방식은 문서화에 필요한 학습 비용을 높이고 지속적인 대응을 어렵게 만듭니다.위 코드를 tosspayments-restdocs 라이브러리를 사용해서 다시 작성한 코드 예시입니다.생성될 문서를 직관적으로 예측할 수 있고, 기반 기술을 완전히 추상화해서 코드가 절반 이하로 감소했습니다.tosspayments-restdocs를 적용한 결과이런 개선점을 만든 tosspayments-restdocs 라이브러리에는 선언형 프로그래밍, 타입 추론 개념이 녹아있는데요. 어떻게 적용됐는지 하나씩 설명해보겠습니다.선언형 프로그래밍https://developer.mozilla.org/en-US/docs/Glossary/Element선언적으로 코드를 작성하는 대표적인 사례는 HTML입니다. HTML에서는 표현하고자 하는 항목을 요소(Element)로 추상화하고, 요소의 태그(Tag), 속성(Attribute), 내용(Content)을 명세해서 최종 결과물을 만듭니다.같은 방식으로, tosspayments-restdocs에서는 API의 실제 형태(HTTP의 페이로드)를 있는 그대로 표현할 수 있도록 Documentation, Request Line, Request Header, Request Body, Response Body를 요소화했습니다. 이렇게 모든 문서화 항목을 요소로 만들었기 때문에 HTML을 작성하는 방식처럼 선언적으로 문서를 작성할 수 있습니다.선언적인 작성에는 이런 장점이 있습니다.명세의 내용에 집중하게 됩니다. 세부 기술 및 이를 위한 불필요한 명령 없이, 문서화 코드의 본질인 인터페이스 명세에 집중할 수 있습니다.꼭 필요한 정보만 명세할 수 있습니다. 꼭 필요한 항목만 요소의 속성으로 선언하고, 추가적인 속성들은 중괄호를 열어 표현하게 됩니다.읽기 쉬운 코드가 됩니다. 최종 결과물의 실제 형태가 그대로 녹아져 있으므로, 위에서 아래로 한번만 읽으면 결과를 파악할 수 있습니다.확장이 쉽습니다. 새로운 문서화 항목이 생기는 경우 요소를 새로 정의하면 됩니다. 기존 항목에 자식 항목이 새롭게 생기는 경우에도 마찬가지로 대응이 가능합니다.구현 방법에서 자유로워집니다. 각 요소를 어떻게, 무엇으로 렌더링 할 지를 모두 라이브러리에 위임해서 기반 기술의 변화나 결과물의 포맷 변화가 코드에 미치는 영향을 줄일 수 있습니다.선언형 프로그래밍 구현 - 함수와 확장 함수tosspayments-restdocs의 문서화 코드 진입점인 documentation 요소 함수를 살펴보겠습니다.documentation 함수 호출 예documentation 내부 구현(이해를 돕기 위해 단순화하였습니다)documentation은 문서의 이름(documentName)을 필수 속성으로, requestLine 등 세부 스펙 요소 추가적인 속성으로 취급하는 요소 함수로, 문서화의 시작(문서 스펙 정의)과 끝(문서 출력)을 담당합니다.앞서 꼭 필요한 항목만 요소의 속성으로 선언하고, 추가적인 속성들은 중괄호를 열어 표현한다고 했는데요. 이 개념을 코틀린으로 구현하면서 필수 속성은 함수의 파라미터로, 추가 속성은 람다 함수 파라미터로 표현했습니다. 요소의 모든 속성을 파라미터로 펼쳐두면 함수가 장황해지고 확장성이 떨어지기 때문입니다.추가 속성을 람다 확장 함수(Extension Function) 스코프 내에서 정의하도록 하면 스코프 내에서 일어나는 일들에는 함수가 관여하지 않습니다. 그래서 편의성을 확보하면서 확장성을 유지할 수 있고, 스코프를 계층화하여 도메인을 더욱 잘 표현할 수 있습니다.예를 들어 documentation 요소 함수의 람다 확장 함수(specCustomizer: DocumentSpec.() -> Unit) 스코프에서는 RequestLineSpec, RequestBodySpec, ResponseBodySpec 등을 세부 요소 스펙으로 품고 있는 DocumentSpec의 this가 주어집니다. 스코프 및 스코프에 this를 주입하는 이런 방식에는 다음과 같은 장점이 있습니다.스코프 단위로 특화된 기능을 제공할 수 있습니다. 예를 들어 requestLine 함수는 DocumentSpec의 세부 요소 스펙인 RequestLineSpec을 정의하기 위한 요소 함수입니다.구조화된 확장성을 갖습니다. 새로운 속성이 추가되었을 때 스코프 내에 함수나 프로퍼티를 추가하여 쉽게 대응할 수 있습니다. 또한 새로운 자식 요소를 갖게 되는 경우에도 동일한 방식으로 람다 확장 함수 스코프를 정의해 대응할 수 있습니다. documentation → requestBody → field 로 이어지는 nested scope 가 그 예입니다.타입 추론Spring REST Docs의 문제 중 하나는 필드의 타입을 REST Docs Snippet에 다시 명세하는 비효율적인 작성 방식입니다.Kotlin에서는 Inline Function 한정으로 Reified Type Parameter를 제공합니다. 타입 정보가 소거되는 일반적인 Generic Function의 Type Parameter와 달리, Reified Type Parameter의 경우에는 타입 정보가 소거되지 않아 라이브러리에서 접근할 수 있습니다.tosspayments-restdocs에서는 항상 문서화 요소가 샘플을 받게 강제하고, 샘플의 타입과 값을 내부 자료구조에 저장하도록 했습니다.sample이 reified T로 선언되어 타입 정보(T::class.java)에 접근이 가능합니다.타입 정보가 남아있다면 문서를 작성하는 개발자를 대신해 다양한 작업을 자동화 할 수 있습니다. tosspayments-restdocs에서는 타입 명세, 열거형 예시 작성, 포멧 명세 등에 타입 정보를 활용하고 잇습니다.타입 정보 활용 예(타입별 양식 자동생성) – 열거형은 엔트리 나열, 시간 타입은 타임 포멧을 반환타입 정보 활용 예(생성된 문서) – 열거형 타입으로부터 얻은 정보로 ResultType의 엔트리(SUCCESS, ERROR)가 자동생성 되었습니다.지금까지 문서를 최소한의 코드로 작성하면서 변화에도 더 유연하게 대처할 수 있는 tosspayments-restdocs 라이브러리와, 라이브러리에 녹인 생각들을 소개했습니다.tosspayments-restdocs는 토스페이먼츠의 다양한 팀에서 Spring REST Docs, kotlin-dsl-restdocs를 대신해 활용하며 생산성을 높이고 있습니다.개발자가 더욱 변하지 않고 가치있는 일에 집중할 수 있도록, 토스페이먼츠에서는 생산성을 높이기 위한 다양한 활동을 이어나가고 있습니다. 함께 고민하며 더욱 좋은 문화와 기술을 만들어나가면 좋겠습니다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "19": "ESLint와 AST로 코드 퀄리티 높이기전성ㆍFrontend Platform Engineer2023. 3. 31코딩 컨벤션을 일관적으로 유지하기일관적인 코딩 컨벤션을 가지면 코드를 읽기 쉬워지고, 안티패턴을 방지할 수 있습니다. 결과로 버그도 줄고, 코드를 쉽게 유지보수할 수 있죠.하지만 이것을 사람이 직접 적용하는 것은 한계가 있기 때문에, 여러 가지 정적 분석 도구를 활용하게 됩니다. JavaScript/TypeScript 코드베이스에서는 주로 ESLint를 통해 컨벤션과 맞지 않는 코드를 사전에 감지하게 되는데요. 이러한 정적 분석 도구를 이용하게 되면 코드 리뷰 등 사람이 직접 읽지 않아도 컨벤션과 다른 부분을 기계적으로 잡아낼 수 있습니다.이미 만들어진 규칙에서 오는 한계ESLint에서는 생태계 내 다양한 플러그인 등을 통해 많은 수의 자주 사용되는 코딩 컨벤션을 커버할 수 있습니다. 하지만 우리 회사의 컨벤션에 맞는 규칙이 없다면 어떨까요? 조직이 커지고 요구 사항이 변화하게 되면서 커뮤니티에서 만들어진 규칙만으로는 조직 내 사용례에 정확히 부합하지 않는 경우가 생깁니다. 사내 라이브러리 내 사용 방식에 대한 컨벤션을 정의하거나, 조직 내 컨벤션과 커뮤니티에서 통용되는 컨벤션이 다소 다를 수도 있죠.예를 들어 토스에서는 SSR을 통해 서버 사이드에서 React 렌더링을 한 뒤 애플리케이션에 제공해서 로딩 속도를 높이고 있는데요, 이로 인해 브라우저 환경에서 작동하는 코드가 서버 사이드에서 실행되면서 의도치 않은 버그를 유발하는 케이스가 있습니다. 이러한 경우 미리 브라우저에서 호환되지 않는 코드를 감지하여 사전에 오류를 예방할 수 있다면 큰 도움이 되겠죠.린터는 어떻게 규칙을 적용할까?이러한 제약 사항을 해결하기 위해서는 우리만의 ESLint 규칙을 정의할 수 있어야 합니다. 그런데 ESLint는 어떻게 코드에 대한 규칙을 만들고 적용하고 있을까요?예를 들어서, Production 환경에서 로그가 함부로 찍히지 않도록 console.log 사용을 제한하는 규칙을 만드는 상황을 생각합시다.간단하게 정규식으로 구현해보면 이런 형식이 될 것입니다.if (sourceCode.match(/console\\\\.log/) != null) {\n  console.log(\"console.log를 사용하면 안 돼요!\") \n}하지만 이 방법은 생각했던 만큼 잘 작동하지 않습니다.예를 들어서, 이 코드에서 console.log가 문자열 안에 있는지도 알 수 없습니다.const message = \"console.log()를 쓰지 마세요.\";\n\n<Button>console.log 로그 활성화</Button>주석 안에 있는지도 알 수 없죠.// console.log(…) 를 쓰지 마세요.이러한 작은 케이스들을 하나하나 대응할 수도 있지만, ESLint는 좀 더 강력한 방법을 사용합니다.AST에서 원하는 정보 찾아내기ESLint는 Abstract Syntax Tree(AST)를 이용해서 규칙을 정의하고 적용합니다.AST는 소스 코드를 읽어낸 뒤 각 코드에서 구문 정보를 정리하여 나타낸 트리 형태의 자료 구조입니다. 예를 들어서, console.log 함수 호출과, 문자열이나 주석 속의 console.log 를 구별할 수 있게 해 줍니다.AST의 상세한 구조는 파서마다 약간의 차이가 있지만, AST Explorer라는 도구를 사용하면 소스 코드를 넣었을 때 어떤 AST가 나오는 지를 쉽게 확인할 수 있습니다. 일례로 console.log() 을 acorn이라고 하는 파서에서 파싱을 시도하면 이런 AST를 얻을 수 있습니다.{\n  \"type\": \"ExpressionStatement\",\n  \"expression\": {\n    \"type\": \"CallExpression\",\n    \"callee\": {\n      \"type\": \"MemberExpression\",\n      \"object\": { \"type\": \"Identifier\", \"name\": \"console\" },\n      \"property\": { \"type\": \"Identifier\", \"name\": \"log\" }\n    },\n    \"arguments\": []\n  }\n}이와 다르게, 문자열에 포함되어 있는 console.log() 의 파싱을 시도하면 이런 AST를 얻을 수 있습니다.{ \n  \"type\": \"Literal\", \n  \"value\": \"console.log()\", \n  \"raw\": \"\\\"console.log()\\\"\" \n}함수를 호출하는 경우, CallExpression과 MemberExpression이 사용되고, 문자열 안에 있는 경우 Literal이 사용되는 것을 볼 수 있네요.여기서 얻은 정보를 바탕으로 acorn을 이용해 console.log를 감지하는 스크립트를 작성해볼 수 있습니다.import { Parser } from \"acorn\";\nimport { simple } from \"acorn-walk\";\n\nsimple(Parser.parse(sourceCode), {\n  CallExpression({ callee }) {\n    if (callee.object.name === \"console\" && callee.property.name === \"log\") {\n      console.log(\"console.log를 사용하면 안 돼요!\");\n    }\n  },\n});이렇게 acorn-walk 를 사용하면 CallExpression에 해당하는 console.log만 감지할 수 있습니다. 주석이나 문자열, 화이트스페이스에 관계없이 안전하게 소스코드를 분석할 수 있는 것이죠.ESLint에서 사용할 규칙 직접 정의하기ESLint는 espree라고 하는 파서를 통해 소스 코드를 파싱하고, 이 결과를 각 플러그인에서 순회하며 규칙을 실행합니다. 우리가 원하는 규칙을 직접 플러그인을 통해 정의하고, 실행할 수 있어요.Espree AST만 읽을 수 있다면 ESLint 규칙도 쉽게 만들 수 있습니다.토스에서는 소스 코드 내에서 HTTP 링크를 찾아 HTTPS 링크로 바꿔야 한다고 알려주는 ban-http 와 같은 규칙을 정의하고 있습니다. 이런 규칙을 어떻게 직접 정의할 수 있는지 알아볼까요?먼저 소스 코드 내 문자열이 Espree AST에서 어떻게 표현되는 지를 알아봐야 합니다. AST Explorer에서 상단의 파서 설정을 Espree로 변경해주면 이를 쉽게 알 수 있습니다.Literal 타입의 노드에서 value를 읽으면 문자열 내용을 알 수 있네요.{\n  \"type\": \"Literal\",\n  \"value\": \"http://toss.im\",\n  \"raw\": \"\\\"http://toss.im\\\"\"\n}이를 기반으로 아래와 같이 ESLint 규칙을 새로 정의할 수 있습니다.module.exports = {\n  meta: {\n    /* ... */\n  },\n  create: function (context) {\n    return {\n      Literal: function (node) {\n        if (typeof node.value !== \"string\") {\n          return;\n        }\n        if (node.value.indexOf(\"http://\") >= 0) {\n          context.report({ node, messageId: \"isHttpBanned\" });\n        }\n      },\n    };\n  },\n};위 코드는 Literal을 만났을 때, 그 Literal의 값이 “http://” 로 시작하는 문자열이면 에러를 리포트하는 코드입니다. 생각보다 복잡하지는 않죠?이렇게 작성된 규칙을 ESLint에 추가하면 개발자들이 개발 중 규칙에 맞지 않는 코드를 작성했을 때 이렇게 알려줄 수 있어요.토스에서 사용하는 여러가지 규칙들이를 바탕으로 토스에서는 여러 가지 ESLint 규칙을 만들어서 플러그인으로 배포하고, 이를 서비스에서 사용하여 코딩 컨벤션을 유지하고 있습니다. 몇 가지 사용하는 규칙들은 아래와 같은 규칙들이 있어요.토스 프론트엔드 챕터 내 맥락이 강한 규칙들사내 라이브러리 사용 시 deprecated된 API 사용 금지이전 토스 도메인 사용 금지외부 라이브러리 사용에 관련한 규칙들사용하지 않기로 한 패키지 사용 제한 (ban-axios, ban-lodash)훅 이름에서 한글 허용 (rules-of-hooks)SSR에서 사용 시 오류를 내는 라이브러리 사용 제한 (ban-ssr-unsafe-method)또한 ESLint 외에도 자체 제작한 도구를 통해 사용해 deperecated 된 API의 사용이나 중복된 코드를 감지하기도 해요.더 알아보기ESLint 플러그인을 만드는 방법과 ESLint API 자체에 대한 글은 ESLint 공식 문서에서 더 자세히 알 수 있어요.Create Plugins – ESLint – Pluggable JavaScript Linter재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기다른 글 둘러보기JSCodeShift로 기술 부채 청산하기2021. 05. 04",
    "20": "Spring Boot Actuator의 헬스체크 살펴보기양권성ㆍServer Developer2023. 4. 1뭐든 알고 쓰는 게 참 중요한 것 같습니다. 단순히 “지금은 잘 돌아가니까 문제 없다”는 접근은 문제가 발생하기 전까지는 문제를 방치하기 마련입니다.사용하는 기술이나 구조에 대해 끊임없이 질문을 던지고 탐구하는 과정은 토스팀 코어밸류 3.0 중 하나인 Question Every Assumption, 모든 기본 가정에 근원적 물음을 제기한다에도 부합하는 사례인것 같습니다. 이번 포스트에서는 제가 개발 과정에서 헬스 체크를 별다른 생각 없이 Spring Boot Actuator가 제공하는 기능을 사용하면서 겪은 이슈를 간략하게 설명해보겠습니다.들어가기에 앞서이 포스트는 작성 시점 기준에서 최신 Spring Boot GA(General Availability) 버전인 3.0.5 버전을 기준으로 설명합니다. 해당 버전의 하위/상위 버전에서는 기능이 미묘하게 다르게 동작할 수 있습니다. 2.x 버전에서도 큰 맥락에서는 동일한 동작을 보장하리라 추측되지만 본인이 사용하는 버전에 해당하는 자세한 내용을 찾아보시길 권장합니다.헬스 체크란?서비스의 고가용성(HA, High Availability), 고성능을 위한 부하 분산 등의 이유로 우리는 서버의 이중화(혹은 그 이상)를 하고, 앞에서 어떤 서버로 요청을 보낼지 라우팅 역할을 하는 로드 밸런서를 둡니다.로드 밸런서가 적절히 부하를 분산하여 A/B 서버 중 한 대에게 클라이언트의 요청을 보냅니다.하지만 아래와 같이 서버 한 대가 서비스 불가 상태라면 어떻게 해야할까요? 해당 서버에 요청이 들어가야할까요?혹은 대량의 트래픽이 들어올 것을 대비하는 등등의 이유로 서버를 증설해야 하는데 해당 서버가 관련된 소스코드를 로딩하고 있다면 어떻게 해야할까요? 이 때도 마찬가지로 해당 서버에 요청이 들어가야할까요?두 케이스 모두 해당 서버로 요청을 보내면 안 됩니다. 정상적인 서비스가 불가능해서 클라이언트의 요청을 수행할 수 없습니다. 장애를 유발하거나 해당 서버의 부하를 크게 증가시켜 오히려 장애를 더 심각하게 만들 수도 있습니다.따라서 로드 밸런서에서는 각 서버의 헬스 체크 API를 호출해서 해당 서버가 현재 서비스 가능한 상태인지 아닌지 주기적으로 점검합니다.헬스 체크 API 경로는 커스텀하게 설정 가능합니다.헬스 체크에서 서버에 문제가 발견되면 로드 밸런서는 해당 서버로 요청을 보내지 않게 됩니다.헬스 체크는 정상적으로 서비스가 가능한 서버에만 트래픽을 보내서 서비스의 고가용성을 확보하는 데 도움됩니다.Spring Boot Actuator의 헬스 체크Spring Boot Acutator를 의존성으로 추가하면 기본적으로 헬스 체크 엔드포인트가 활성화됩니다.Spring Boot 3.x 기준으로 헬스 체크 엔드포인트는 /actuator/health이고, 설정을 바꾸지 않아도 해당 엔드포인트로 접속하면 HTTP 200 상태 코드와 해당 서버의 상태가 Response Body로 응답됩니다.크롬 개발자 도구로 확인해본 Spring Boot Actuator의 헬스 체크 결과Spring Boot Actuator는 어떠 기준으로 서버의 헬스 체크를 할까요? 확인하려면 Health Information 문서를 살펴보면 됩니다. 해당 정보는 보안에 민감한 요소가 들어있을 수 있어서 퍼블릭하게 접근이 가능해서는 안 됩니다. 저는 로컬에서 간단하게 확인만 해보는 목적으로 application.yml(application.properties) 파일에 management.endpoint.health.show-details: always로 설정한 후에 다시 헬스 체크 결과를 확인했습니다.Auto-configured HealthIndicators(WebMVC 전용)와 Auto-configured ReactiveHealthIndicators(Webflux 전용)에 나열된 HealthIndicator(혹은 ReactiveHealthIndicator)는 Spring Boot Auto Configuration에 의해 자동으로 활성화되는데 관련된 의존성이 존재할 때만 활성화 되는 것들도 있습니다. 예를 들어, DataSourceHealthIndicator는 DataSourceHealthContributorAutoConfiguration에 의해 설정되는데 Spring Data JPA 같이 DataSource를 사용하는 의존성을 추가했을 때 활성화됩니다.그럼 코드레벨에서 각 (Reactive)HealthIndicator들이 어떻게 사용되는지 보겠습니다.먼저 /actuator/health에 접속한 뒤에 브레이크 포인트를 걸고 디버그 모드로 살펴보면 HealthEndpointSupport 클래스의 getAggregateContribution 메서드에서 각 HealthContributor(혹은 ReactiveHealthContributor)를 순회하면서 헬스 체크하는 코드를 보실 수 있습니다. (헬스 체크하는 코드에 있는 HealthIndicator 인터페이스는 HealthContributor 인터페이스를 상속받았습니다.)HealthEndpointSupport 클래스의 getCompositeHealth 메서드에서는 각 HealthIndicator로부터 수집한 상태를 바탕으로 현재 서버의 상태를 진단합니다.@Override\npublic Status getAggregateStatus(Set<Status> statuses) {\n    return statuses.stream().filter(this::contains).min(this.comparator).orElse(Status.UNKNOWN);\n}/**\n * {@link Comparator} used to order {@link Status}.\n */\nprivate class StatusComparator implements Comparator<Status> {\n\n    @Override\n    public int compare(Status s1, Status s2) {\n        List<String> order = SimpleStatusAggregator.this.order;\n        int i1 = order.indexOf(getUniformCode(s1.getCode()));\n        int i2 = order.indexOf(getUniformCode(s2.getCode()));\n        return (i1 < i2) ? -1 : (i1 != i2) ? 1 : s1.getCode().compareTo(s2.getCode());\n    }\n\n}SimpleStatusAggregator의 getAggregateStatus 메서드에서는 각 상태를 수집해서 하나의 Status로 반환하고 있는데 이 때 StatusComparator가 사용됩니다.defaultOrder.add(Status.DOWN.getCode());\ndefaultOrder.add(Status.OUT_OF_SERVICE.getCode());\ndefaultOrder.add(Status.UP.getCode());\ndefaultOrder.add(Status.UNKNOWN.getCode());\nDEFAULT_ORDER = Collections.unmodifiableList(getUniformCodes(defaultOrder.stream()));이 때 가장 중요한 것은 Status의 순서인데 SimpleStatusAggragtor의 static 생성자 블럭을 보게되면 위와 같은 순서로 추가하고 있고,public SimpleStatusAggregator() {\n    this.order = DEFAULT_ORDER;\n}별도의 순서를 주지 않은 기본 생성자는 defaultOrder에 추가한 순서를 사용하는 것을 볼 수 있습니다.getAggregateStatus는 Status 중에 가장 순서가 빠른(오름차순) 것 하나를 반환하게 되어있기 때문에 만약에 Down을 반환한 HealthIndicator가 하나라도 존재하면 서비스의 상태를 Down으로 생각해서 503을 반환하게 됩니다.헬스 체크에서 조심해야 하는 점Spring Boot Actuator 헬스 체크의 동작원리를 잘 모르고 사용하면 일어날 수 있는 문제를 설명하겠습니다.1. 의도치 않은 장애 발생각 서버에서는 서비스를 제공하는 서비스 DB와 데이터를 분석하는 로그 DB가 있다고 가정하겠습니다. 그리고 로그 DB에 적재하는 작업은 비동기로 별도의 스레드에서 처리하도록 작업을 해놨다고 가정하겠습니다. 로그 데이터 저장이 불가능하더라도 실시간 서비스에는 문제가 없도록 하기 위해서죠.이 때 만약 로그 DB에 작업을 해야해서 순단이 발생하거나 접속에 문제가 생긴다면 어떻게 될까요? 아래 정답을 확인하기 전에 1분 동안 한 번 생각해보시길 바랍니다.위에 Spring Boot Actuator의 헬스 체크는 여러 HealthIndicator가 수집한 상태를 토대로 서비스의 상태를 판단한다고 말씀드렸습니다. 그 순서를 차근차근 설명해보겠습니다.RoutingDataSourceHealthContributor에 의해 여러 DataSource의 헬스를 체크합니다.DataSourceHealthIndicator에 의해 서비스 DB의 상태를 체크했을 때는 UP이 반환됩니다.DataSourceHealthIndicator에 의해 로그 DB의 상태를 체크했을 때는 DOWN이 반환됩니다.수집한 상태들은 SimpleStatusAggregator에 의해 서비스 상태를 판단하게 되는데 아무런 순서 설정을 하지 않았으면 DOWN인 게 하나라도 있다면 DOWN이 반환됩니다.서비스의 상태가 DOWN(503)으로 판단됐기 때문에 로드 밸런서에서는 서버로 트래픽을 보내지 않게 됩니다.서비스 DB에 문제가 없음에도 불구하고 클라이언트의 요청은 처리되지 않고 장애가 발생합니다.우리는 분명 최대한 높은 가용성을 보장하기 위해 로그 DB의 장애가 전파되지 않도록 격리했음에도 불구하고 장애가 발생할 수 있습니다. 이를 해결하기 위해서는 아래와 같은 방법 등등이 있습니다.Spring Boot Actuator의 헬스 체크가 아닌 직접 헬스 체크 API를 구현할 수도 있습니다.HealthIndicator 중에 헬스 체크에 영향을 끼치지 않길 희망하는 것들은 비활성화 시키는 방법도 있습니다. (RDB를 예로 들자면 management.health.db.enabled: false(기본값 true)로 설정한다거나)문제가 되는 HealthIndicator 빈을 직접 생성해서 Auto Configuration의 동작을 오버라이딩 하는 방법 등등이 있습니다.다만 헬스 체크에 이런 저런 로직들이 들어간다는 것은 일반적으로 예측 가능하지 못할 수 있으므로 팀 내에 꼭 공유가 잘 되어야할 것입니다.2. 트러블 슈팅의 지연비슷한 상황으로, 예전에 API 서버에서 외부 의존성 중에 ES만 죽었는데, API 서버가 죽었다고 판단돼서 DOWN이 된적이 있었어요. 헬스 체크에서 detail 옵션을 키면, 상세하게 쭉 나오더라고요. 당시에 LB 통해서 접근이 안 됐는데, WAS는 개별로 접근했을 때는 문제가 없어 보여서 트러블 슈팅이 늦어졌었습니다.이는 실제 사내에서 비슷한 상황이 발생했을 때 트러블 슈팅이 지연된 사례입니다. Spring Boot Actuator 헬스체크의 동작원리를 정확히 이해했다면 ES(Elasticsearch) 서버가 죽었을 때 해당 서버의 헬스체크도 같이 죽게 된다는 걸 예측할 수 있습니다. (ElasticsearchRestClientHealthIndicator 혹은 ElasticsearchReactiveHealthIndicator가 ES 서버의 헬스체크를 해서 헬스체크 API 응답에 전체적으로 영향을 끼치기 때문에)하지만 헬스체크의 동작원리를 잘 모르면 우리가 장애를 격리했다고 생각한 시스템(위의 상황에서는 ES)에만 문제가 있는데 왜 장애가 발생하는지, 왜 도메인을 통해서 접근하면 접근이 안 되는지 상황 파악이 안 될 수 있습니다. 서버는 정상적으로 살아있고 부하도 없는 상황이라면 헬스 체크 API를 호출할 생각도 못 하고, 로드 밸런서의 버그인지부터 의심을 할 수도 있습니다. 이렇게 엉뚱한 포인트를 의심하게 되면 장애 상황은 계속 되고, 서버를 재시작해도 근본적인 문제를 해결(위 상황에서는 ES 서버의 복구)하기 전까지는 여전히 헬스 체크에 실패할테니 장시간 장애가 지속될 수도 있습니다.결국 각 서버 인스턴스마다 직접 헬스 체크 API를 호출해서 정상 응답을 받는지 확인해봐야하는데 여기까지 사고의 흐름이 다다르는데 너무 많은 시간 소요와 불필요한 리소스 낭비들을 초래하게 됩니다.마치며평상시에는 헬스 체크하면 그냥 200 OK만 응답하는 정말 심플한 API 수준으로만 생각하고 큰 신경도 쓰지 않았습니다. 근데 사소한 것에 한 번 데인 뒤로부터는 개발자가 왜 호기심이 많아야하는지 한 번 더 깨닫게 되었습니다. 그냥 단순히 돌아만가는 코드가 아닌 이 코드가 왜 그렇게 돌아가는지, 우리가 왜 이 기술을 선택하게 된 것인지, 끊임없이 고민하고 탐구하기 위해서는 강력한 호기심이 동기부여가 되기 때문입니다. 이러한 고민을 미리했다면 장애 상황을 미연에 방지할 수 있고, 장애 발생 이후에라도 이슈 분석을 통해 트러블 슈팅 능력도 크게 향상된다는 것을 다시 한번 깨닫게 되는 소중한 경험이었습니다.참고 링크Spring Boot Actuator DocsNHN Forward spring-boot-actuator documentation재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기다른 글 둘러보기Kotlin으로 DSL 만들기: 반복적이고 지루한 REST Docs 벗어나기2022. 04. 11에러 핸들링을 다른 클래스에게 위임하기 (Kotlin 100% 활용)2022. 05. 14tosspayments-restdocs: 선언형 문서 작성 라이브러리2023. 03. 22",
    "21": "Node.js url.parse() 취약점 컨트리뷰션표상영ㆍSecurity Researcher2023. 5. 12토스 보안기술팀(Security Tech)에서는 개발 서비스 외에도 서비스에서 사용하고 있는 프레임워크나 Third-party에 대한 취약점 연구도 수행하고 있습니다.이번 아티클은 Node.js의 Built-in API 중 하나인 url.parse() 의 Hostname Spoofing 취약점을 발견하고 안전한 코드로 패치될 수 있도록 컨트리뷰션 했던 과정을 다뤄보려 합니다.https://github.com/nodejs/node/pull/45011url.parse() 취약점 발생 원인Node.js의 url.parse()는 WHATWG URL API 가 아닌 자체적인 스펙으로 개발된 함수입니다.WHATWG URL API\nWHATWG는 Web Hypertext Application Technology Working Group의 약어로 국제 웹 표준화 그룹을 뜻하고, WHATWG URL API 는 국제 표준 스펙으로 URL(Uniform Resource Locator)을 다룰 수 있도록 제공되는 API를 말합니다.  WHATWG URL API 가 등장하기 전, URL 파싱 기능을 제공하기 위해 자체적으로 개발된 함수로 보이는데요. 표준 스펙이 아닌 자체적으로 해석하다보니 다른 파서(parser)들과 해석 결과가 달라지게 되고, 이 때문에 의도치 않은 코드흐름이 발생하게 됩니다.url.parse()에서는 hostname을 잘못된 방식으로 파싱하여 취약점이 발생하게 되었는데요. Node.js의 url라이브러리는 여기에서 확인할 수 있고, 취약점이 발생했던 부분은 아래의 getHostname() 함수입니다./* comment\n해당 취약점은 v19.1.0에서 패치되었습니다. \n아래 코드는 v19.1.0 이전 버전에서 확인할 수 있습니다.\n*/\nfunction getHostname(self, rest, hostname) {\n  for (let i = 0; i < hostname.length; ++i) {\n    const code = hostname.charCodeAt(i);\n    const isValid = (code >= CHAR_LOWERCASE_A && code <= CHAR_LOWERCASE_Z) ||\n                    code === CHAR_DOT ||\n                    (code >= CHAR_UPPERCASE_A && code <= CHAR_UPPERCASE_Z) ||\n                    (code >= CHAR_0 && code <= CHAR_9) ||\n                    code === CHAR_HYPHEN_MINUS ||\n                    code === CHAR_PLUS ||\n                    code === CHAR_UNDERSCORE ||\n                    code > 127;\n\n    // Invalid host character\n    if (!isValid) {\n      self.hostname = hostname.slice(0, i);\n      return `/${hostname.slice(i)}${rest}`;\n    }\n  }\n  return rest;\n}getHostname() 함수의 로직은 단순합니다. 반복문을 통해 전달된 값의 문자를 하나씩 가져온 뒤, 조건에 맞는 값을 구하는 로직인데요. 여기서 isValid 라는 특정 조건을 정의해두고, 현재 문자가 조건에 충족되지 않으면 해당 문자의 앞 인덱스까지 slice하여 그 문자열을 hostname으로 설정합니다. 그리고 그 뒤 문자들은 앞에 /를 붙여 경로(path)로 사용합니다.const isValid = (code >= CHAR_LOWERCASE_A && code <= CHAR_LOWERCASE_Z) ||\n                code === CHAR_DOT ||\n                (code >= CHAR_UPPERCASE_A && code <= CHAR_UPPERCASE_Z) ||\n                (code >= CHAR_0 && code <= CHAR_9) ||\n                code === CHAR_HYPHEN_MINUS ||\n                code === CHAR_PLUS ||\n                code === CHAR_UNDERSCORE ||\n                code > 127;isValid 의 조건을 정규식으로 표현해보면 /[a-zA-Z0-9\\.\\-\\+_]/u 와 같은데요. *ECMAScript기준 \n“hostname으로는 저 범위의 문자들만 올 수 있어!” 라고 설정해둔 것이죠.디버깅 코드getHostname() 함수에 디버깅 코드를 추가하고 확인해보면, 실제로 isValid 범위 밖 문자일 경우 hostname 파싱을 중단하고, 나머지 문자열은 앞에 / 를 붙여 경로(path)로 사용하는 것을 볼 수 있습니다.때문에, http://EVIL_DOMAIN*.toss.im 의 hostname이 EVIL_DOMAIN 이 되어버리고, Hostname Spoofing 취약점이 발생하게 됩니다.Hostname Spoofing\nHostname Spoofing은 시스템을 대상으로 Hostname을 속이는 해킹 기법을 말합니다.\nSpoofing은 ‘속이다’라는 사전적 의미를 갖고 있으며, 시스템을 대상으로 어떠한 정보를 속이는 해킹 기법을 Spoofing이라고 합니다.\nWHATWG URL API ↔ url.parse() 비교WHATWG URL API 의 hostname 파싱 결과를 보면 evil_domain*.toss.im 으로 Node.js와 다른 것을 확인할 수 있습니다.Reserved CharactersNode.js에서 이렇게 파싱하는 이유는 RFC(Request For Comments) 문서를 보면 알 수 있습니다. RFC 3986는 Standard URI Syntax 를 정의해둔 문서인데요.문서의 여러 항목 중 아래 2.2 Reserved Characters를 보면 그 이유를 찾을 수 있습니다.https://www.rfc-editor.org/rfc/rfc3986#section-2.2Reserved Characters 는 URI를 구성할 수 있는 문자 중 특수 목적을 갖고 사용할 문자들을 미리 예약해둔 것인데요. 예를 들면 port 구분자로 사용되는 :(colon) , path 구분자로 사용되는 /(slash) 가 예약된 문자인 것이죠. 따라서, *, !, $, :, # 와 같은 문자들은 hostname으로 사용할 수 없습니다.그 아래를 보면 사용해도 되는 Unreserved Characters 도 정의되어 있습니다.https://www.rfc-editor.org/rfc/rfc3986#section-2.3정의된 문자들을 보면 위에서 확인한 url.parse()의 isValid 조건과 비슷한 것을 알 수 있죠.취약점 악용 시나리오이러한 WHATWG URL API ↔ url.parse() 간 파싱 결과 차이는 서비스의 도메인 검증로직을 우회하는데 악용할 수 있습니다.// server.ts (exec command: ts-node server.ts)\n/* dependencise\n\n\texpress@^4.18.1\n\tts-node@^10.9.1\n\ttypescript@^4.3.2\n\tnode-fetch@2\n\t@types/node-fetch@^2.6.2\n\n*/\n\nimport express, { Request, Response, NextFunction } from 'express';\n\nconst node_fetch = require(\"node-fetch\");\nconst app = express();\n\napp.get(\"/image/resize\", async (req: Request, res: Response) => {\n\t// GET메소드로 url파라미터 입력 받음\n\tconst url = req.query.url as string; // [1]\n\t// WHATWG URL API를 이용해 hostname 파싱\n\tconst host = new URL(url).hostname; // [2]\n\n\t// 파싱한 hostname 검증 (example.com과 *.example.com일 경우에만 분기문 통과)\n\tif(host === \"toss.im\" || host.endsWith(\".toss.im\")) { // [3]\n\t\t// 검증된 hostname일 경우, node_fetch로 http request\n\t\tvar result = await node_fetch.default(url); // [4]\n\t\tvar requestUrl = result.url; // [5]\n\t\t// 파라미터로 입력된 url과 node_fetch로 실제 요청한 url 콘솔 출력\n\t\tconsole.log(`Input URL: ${url} / Request URL: ${requestUrl}`); // [6]\n\t// 그 외 경우는 reject\n\t} else {\n\t\tconsole.log(\"reject\");\n\t}\n});\n\napp.listen(4540, () => {\n});위 코드는 사용자에게 파라미터로 url을 입력받고, 입력된 url 검증 후 fetch하는 간단한 웹서버입니다.new URL(url).hostname 으로 hostname을 가져온 뒤, hostname이 toss.im 과 일치하거나 .toss.im 으로 끝나는지 확인합니다. 일반적으로 도메인을 검증할 때 사용하는 방식이죠.파라미터로 입력한 url과 node-fetch에서 실제로 요청한 url 콘솔 출력이 경우, 위 그림처럼 url파라미터 값을 https://google.com!.toss.im 로 입력하면 검증 로직이 우회되고 서버는 https://google.com/!.toss.im/ 으로 요청하게 되는데요. 그 이유는 node-fetch 라이브러리 코드를 보면 알 수 있습니다./* node-fetch v2.6.11 \n * [https://github.com/node-fetch/node-fetch/tree/v2.6.11]\n * request.js\n *\n * Request class contains server only options\n *\n * \n */\n\nimport Url from 'url'; // [1]\nimport Stream from 'stream';\nimport whatwgUrl from 'whatwg-url';\nimport Headers, { exportNodeCompatibleHeaders } from './headers.js';\nimport Body, { clone, extractContentType, getTotalBytes } from './body';\n\nconst INTERNALS = Symbol('Request internals');\nconst URL = Url.URL || whatwgUrl.URL;\n\n// fix an issue where \"format\", \"parse\" aren't a named export for node <10\nconst parse_url = Url.parse; // [2]\nconst format_url = Url.format;\n\n/**\n * Wrapper around `new URL` to handle arbitrary URLs\n *\n * @param  {string} urlStr\n * @return {void}\n */\nfunction parseURL(urlStr) {\n\t/*\n\t\tCheck whether the URL is absolute or not\n\n\t\tScheme: https://tools.ietf.org/html/rfc3986#section-3.1\n\t\tAbsolute URL: https://tools.ietf.org/html/rfc3986#section-4.3\n\t*/\n\tif (/^[a-zA-Z][a-zA-Z\\d+\\-.]*:/.exec(urlStr)) {\n\t\turlStr = new URL(urlStr).toString() // [3]\n\t}\n\n\t// Fallback to old implementation for arbitrary URLs\n\treturn parse_url(urlStr); // [4]\n}위 코드는 node-fetch 라이브러리에서 url을 파싱하는 코드 부분입니다. 중요한 부분은 주석으로 번호 표시를 해두었는데요.[1]: Node.js의 url 라이브러리를 가져옵니다.[2]: Node.js의 url.parse()함수를 parse_url 변수에 저장합니다.[3]: 파싱할 url이 정규식 조건과 일치하면 WHATWG URL API 로 파싱합니다.[4]: 그 외에 경우는 Node.js의 url.parse()함수로 파싱합니다.url 파라미터로 입력한 https://google.com!.toss.im 은 [3] 정규식 조건에 충족되지 않으니, [4] url.parse()로 hostname이 파싱되었고 Node.js의 잘못된 파싱 방식으로 인해 https://google.com/*.toss.im 으로 요청하게 된 것입니다.이처럼 서비스 서버의 검증 로직을 우회하고 공격자가 원하는 임의의 도메인으로 요청하도록 하는 공격기법을 SSRF(Server Side Request Forgery) 라고 하는데요. 공격자는 SSRF 공격을 통해 외부에 공개되어 있지 않은 서비스 내부 망에 접근하여 민감한 정보들을 탈취하거나, 관리자 기능들을 악용할 수 있습니다.취약점 패치 컨트리뷰션화이트해커 문화에는 취약점을 제보하고 그에 따른 보상을 지급하는 버그바운티(Bug Bounty) 라는 프로그램이 존재합니다. 보안에 중요한 가치를 두고 있는 기업들이 독립적으로 운영하기도 하고, 국가기관에서 운영하기도 하는데요.토스 버그바운티 챌린지토스에서도 작년에 자체적으로 토스 버그바운티 챌린지 (https://bugbounty.toss.im)를 진행한 바 있고, 국가 기관에서는 한국인터넷진흥원(KISA)이 국내 소프트웨어에 대한 취약점을 제보받고 있습니다.Node.js url.parse() 취약점 제보저 또한 버그바운티 프로그램을 통해 Node.js 측에 취약점을 제보하였고, 취약점을 알맞게 패치할 수 있는 방안들에 대해 논의하면서 컨트리뷰션을 진행하였는데요.기존에 Unreserved Characters 를 화이트리스트로 처리하는 방식 대신 Reserved Characters 를 블랙리스트로 처리하는 방식으로 변경하여 isValid 조건을 좀 더 엄격하게 가져가도록 패치하였습니다.https://github.com/nodejs/node/pull/45011 에서 패치된 코드를 확인해 볼 수 있고, 해당 Pull Request는 v19.1.0, v18.13.0 에서 적용되었습니다.https://nodejs.org/api/url.html#urlparseurlstring-parsequerystring-slashesdenotehost추가로 기존에 Legacy 상태였던 url.parse()함수를 Deprecated 로 변경하였는데요. --pending-deprecation 옵션을 사용하는 경우, 런타임에서 Deprecated 함수임을 경고하도록 패치되었습니다.이 글을 읽으신 분들도 Node.js를 사용하고 계시다면, 취약점이 존재하는 버전을 사용 중인지 확인해보시는 것을 권장드립니다. *취약점은 v19.1.0, v18.13.0 에서 패치되었습니다.그리고 저희 보안기술팀(Security Tech)에서 이와 같은 보안 연구를 같이 해나갈 동료분들을 찾고 있습니다. 관심이 있으시다면 언제든지 문을 두드려 주세요!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "22": "놀러오세요! 프론트엔드 다이빙 클럽진유림2023. 7. 21안녕하세요, 토스 프론트엔드 개발자 진유림입니다. 저는 개발을 처음 배울 때부터 커뮤니티 안에서 성장해왔는데요. 9XD, GDG, Facebook Developer Circle등 다양한 온/오프라인 커뮤니티에서 각양각색의 개발자를 만나며 IT업계에 대한 애정을 키우고, 지식은 나눌수록 커진다는 것을 깨달았어요.그러나 코로나 이후로는 오프라인 모임이 중단되었고, 개발 이야기를 나눌 수 있는 사람들이 회사 내로 한정되어 버린 것이 아쉬웠어요. 그래서 시작했습니다, 다양한 회사와 다양한 연차의 개발자들이 모여 노하우를 나누는 프론트엔드 커뮤니티, ‘프론트엔드 다이빙 클럽’을요.프론트엔드 다이빙 클럽?커뮤니티 이름으로부터 유추할 수 있듯이, ‘프론트엔드 다이빙 클럽’(이하 프다클)은 프론트엔드에 관한 깊은 이야기를 나눌 수 있는 공간입니다.격월로 소규모 오프라인 모임을 개최하며, 한 번 이상 참석한 사람들은 프라이빗 슬랙에 가입하여 온라인으로도 소통을 이어갈 수 있습니다.각 모임마다 주제가 바뀌고, 해당 주제에 관심이 많은 분들이 참가하여 다양하고 깊은 의견을 들을 수 있는 게 특징입니다.지난 세 번의 모임은 다음과 같은 주제로 진행되었습니다:웹뷰 위에서 서비스 개발하기프론트엔드 자동화프론트엔드 일하는 방식/문화 공유지금까지 네이버, 유니크굿, 무신사, 아임포트, 오늘의집, 뱅크샐러드, 29cm, 카카오페이, 당근마켓, 우아한형제들, 두나무 등 30개 이상의 회사로부터 1년차부터 20년차까지 다양한 연차의 프론트엔드 개발자 분들이 참석하여 의견을 공유하였습니다. 앞으로의 모임에서는 다음과 같은 주제를 기획하고 있습니다:프론트엔드 테스팅좋은 면접이란프론트엔드 커리어 패스이 주제들은 타 회사의 시각을 듣고 인사이트를 얼마나 많이 얻을 수 있는지를 기준으로 선택되고 있습니다.주제별 발표는 2세션 씩 진행하며, 다양한 회사의 관점을 짧지만 뾰족하게 접할 수 있는 시간으로 준비하고 있습니다. 발표자 신청은 모두가 할 수 있으니 관심 있는 분들의 신청 부탁 드려요(토스 프론트엔드 트위터https://twitter.com/TossFrontend에 공지).모임은 어떻게 진행되나요?금요일 7시, 역삼역 토스 오피스에서 진행되는 오프라인 모임은 다음과 같은 타임라인으로 진행됩니다.7:00 인트로설문조사로 응답해주신 관심 주제(e.g. 코드리뷰, 기술부채 관리 등)를 기반으로 미리 배치된 조(6명 규모)에 착석하와이 풍 음식 먹으며 조별 아이스 브레이킹7:30 첫 번째 발표세션 듣기 및 질의응답조별 토의 및 발표8:10 두 번째 발표세션 듣기 및 질의응답조별 토의 및 발표8:50 마무리 및 조 섞기 + 네트워킹발표를 듣고 그냥 돌아가는 것이 아니라 주제에 대해 활발히 토론하고 살아있는 인사이트를 얻을 수 있도록, 조별로 토론하는 시간을 많이 배치했습니다.주제에 대한 관심이 많은 분들이 참석하셔서 그런지, 아니면 네트워킹을 통해 친밀도가 높아져서 그런지 모든 발표에서 질문이 10~15개씩 나오는 것이 인상적이었습니다.참여자 후기발표자의 한 마디Q. 발표를 하며 얻어간 것이 있나요?A. 비슷한 고민을 회사 밖에서도 함께하는 동료 개발자들과 이야기하는 경험은 정말 귀하다고 느꼈어요. 무언가 만들거나 해결했지만 '이게 정말 좋은 방법일까'라는 고민이 남을 때가 있는데요. 프다클은 '이렇게 풀 수도 있구나', '이런 고민도 있구나'를 공유하면서 생각을 넓힐 수 있는 자리였습니다.참여자의 한 마디Q. 프다클에 매번 참여하는 이유는?A. 좋은 스피커분들의 발표를 다양한 분들과 같이 들으며 식견을 넓혀갈 수 있는 점, 다양한 회사의 프론트 개발자분들과 네트워킹을 할 수 있는 기회, 바빠서 자주 보지 못하는 지인과, 겸사겸사 좋은 자리에 함께 하는 것, 그리고 여러 멋진 분들을 보면서 더 자극받는 시간이 되기도!준비 위원회의 한 마디Q. 어떤 분들이 프다클에 참여하시면 좋을까요?A. 다이빙 클럽은 컨퍼런스 보다는 토론의 장에 가깝습니다. 지식과 경험을 공유하는 것에서 의미와 가치를 느끼시는 분들을 환영해요. 발표를 듣고 자신의 현업에서 적용하는 것 뿐 아니라, 관련해서 본인의 경험들을 적극적으로 나눠주시고 다른 분들의 경험을 경청해주시는 분들께서 참여해 주시기를 기대하고 있어요. 어떻게 참가할 수 있나요?프다클 커뮤니티는 초대 기반으로 발전해오고 있습니다. 매 모임마다 슬랙에 가입된 기존 멤버 대상으로 티켓을 20장씩 제공하고(선착순 10명, 추첨 10명), 이 분들이 지인을 동행하여 총 40명의 사람들이 모이게 됩니다.발표자 신청도 프다클에 참여할 수 있는 또 다른 방법입니다. 발표자 신청 폼은 토스 프론트엔드 트위터(https://twitter.com/TossFrontend) 에 올라갈 예정이니, 참여 원하시는 분들은 구독해주세요. 초대 기반인지라 커뮤니티 확장이 느리다는 아쉬움이 있긴 하지만, 그래도 해당 주제에 진정으로 관심이 있는 분들이 모이게 되어서 오프라인에서도, 온라인에서도 깊은 토론을 할 수 있다는 장점이 있습니다.프다클 합류를 희망하신다면, 구글폼을 통해 간단한 경력 정보와 함께 자기소개를 남겨주세요. 남겨주신 정보를 토대로, 매 회차 모임의 방향성에 따라 함께할 분을 선별하여 초대권을 보내드릴게요.올 해 안에는 더 큰 규모의 공개 모임을 진행할 계획도 있습니다. 기대해주세요!재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기",
    "23": "레고처럼 조립하는 토스 앱이준석/송범근ㆍiOS Developer2023. 8. 22100만 줄.이게 뭐냐고요?바로, 토스 iOS 앱의 코드량입니다.토스팀은 사용자에게 가치를 전달하기 위해 끊임없이 서비스를 개발해왔어요. 지금 토스 앱 안에는 수백 개의 서비스가 들어있습니다. 그렇게 성장해오는 동안, 토스 iOS 앱도 Swift 100만 줄이 넘는 거대한 프로젝트로 자라났습니다.이 글을 읽고 계신 iOS 개발자분들에게 질문을 드려볼게요. 이렇게 프로젝트가 크고 복잡해지면 뭘 해야 할까요?바로 모듈 분리입니다!앱을 하나의 큰 Xcode 프로젝트로 관리하는 대신, 여러 개의 작은 모듈로 나눕니다. 그리고 모듈 간의 적절한 구조를 설계하는 거죠.코드 베이스가 커지면, 모듈 분리도 점점 더 중요해지죠. 그래서 토스 iOS 챕터도 모듈화에 대한 많은 고민을 했는데요. 이 글에서는, 저희가 어떻게 슈퍼 앱 토스의 모듈을 관리하고 있는지 살펴볼게요.먼저 기존 토스 앱의 구조를 알아봐야겠죠? 기존 토스 앱은 가장 일반적인 계층 구조로 이루어져 있었어요. 책임과 역할에 따라 계층을 나눠 해당 계층에는 그에 맞는 모듈들이 위치하고, 하위 계층에 있는 모듈은 상위 계층에 있는 모듈을 의존할 수 없는 형태죠.공통적으로 쓰이는 유틸리티 모듈들이 모여있는 Foundation 계층.수백 개의 서비스 모듈이 위치한 Feature 계층.최종적으로 사용자에게 제공될 앱이 위치한 App 계층.이렇게 계층적으로만 모듈을 관리하고 있었어요.일반적인 계층 구조도 꽤 오랜 시간 잘 작동했습니다. 계층 안에서의 모듈 분리도 되어있었고요. 하위 계층의 모듈이라면 필요에 따라 가져다 활용할 수 있었죠.하지만 수년간 토스에는 정말 많은 서비스가 생겼습니다. 다시 말해 Feature 계층에 모듈들이 엄청나게 늘어났죠.그러다 보니 문제가 발생했습니다.단순히 모듈 수가 많아서는 아니었어요. 많은 서비스가 생겨나면서, 모듈 간의 의존성이 폭발적으로 늘어나는 게 문제였죠.하나의 서비스가 Feature 레이어의 여러 모듈을 활용하는 경우가 많이 생겼습니다. 결과적으로 같은 계층 내에 의존 관계가 복잡해졌고요.그러자 순환 참조와 같은 문제가 발생하기도 했어요. 전반적인 모듈 구조도 이해하기에 너무 어려워졌습니다. 처음에는 계층 구조를 유지하면서 대응을 해봤어요. 공통 기능을 다시 묶어서 하위 계층의 모듈로 분리하거나, 새로운 계층을 추가하여 공통 모듈을 추가로 분리하는 방식도 시도했고요.그런데 이렇게 하다 보니 지나치게 많은 계층이 생겨났어요. 적절한 계층 및 모듈 분리의 기준을 세우기도 애매해지더라고요.이대로는 안 되겠다. 다른 방법이 필요하다. iOS 개발자 모두가 느끼기 시작했습니다.Microfeatures 아키텍처는 Tuist가 소개한 모듈 구조예요. (Tuist는 iOS 프로젝트 관리 툴입니다.)크게 보면, Microfeatures 아키텍처는 아래와 같이 5개의 요소로 구성되어 있어요.Feature(Source)Feature의 실제 기능이 구현된 코드가 위치한 모듈이에요.\nInterfaceFeature에서 제공하는 기능에 대한 외부 인터페이스와 모델을 제공하는 모듈이에요.\nTesting단위 테스트나 Example 앱에서 사용될 코드와 Mock 데이터 등을 제공하는 모듈이에요.\nTests단위 테스트, UI 테스트 등이 위치한 모듈이에요.\nExampleFeature의 기능을 간단히 체험해 볼 수 있는 작은 앱이에요.그래서 이 요소들을 가지고 하나의 서비스를 담당하던 모듈을 쪼개는 거예요.홈 서비스를 예로 들어볼까요?기존에 하나였던 Home 모듈을 이렇게 5개의 모듈로 나누게 되는 거예요.Home (Feature)HomeInterface (Interface)HomeTesting (Testing)HomeTests (Tests)HomeExample (Example)이것을 여러 서비스로 확장해 보면 이렇게 돼요.Interface 모듈에 있는 인터페이스를, Feature, Testing 모듈이 구현합니다. Tests 모듈에 테스트를 작성하고요.Example 앱을 구성할 때는 Feature 혹은 Testing 모듈 중 필요한 것을 골라서 사용해요.이때 자체적으로 개발한 DI(Dependency Injection) Container를 사용해요. Interface 모듈에 대한 Feature 모듈의 구현을 주입하는 역할을 맡고 있죠.Microfeatures 아키텍처 구조로 바뀌고 난 이후에는 뭐가 달라졌을까요?다른 서비스에서 Home 서비스의 코드를 필요로 할 때 Home 모듈을 직접 사용하지 않아요. 대신 HomeInterface 모듈을 사용하죠. HomeInterface에는 외부에 제공되는 인터페이스가 있어요. 이 Home의 기능을 사용할 때는 이 인터페이스를 씁니다.이렇게 하면 한 서비스가 다른 서비스의 코드를 사용하더라도, 같은 계층 (Feature ↔ Feature, Interface ↔ Interface) 내의 의존 관계가 생기지 않게 돼요.같은 계층 내에 의존 관계가 복잡해지는 문제를 해결했습니다.쉽고 멋있게 해결한 척했지만… 사실 Microfeatures 아키텍처를 도입하는 것은 꽤나 험난한 길이었어요.일단 작업량이 만만치 않았죠. 기존에 하나였던 모듈을 5개의 모듈로 분리해야 하는 작업인데, 기존의 모듈도 이미 수백 개였거든요.수많은 컨플릭이 발목을 잡았어요. 공통 모듈을 자주 건드릴 수밖에 없었고. 현재 작업 중인 모듈이 있고, 그 모듈이 의존하는 다른 모듈 분리 작업이 동시에 진행되었어요. 컨플릭이 자주 발생할 수밖에 없는 상황이었죠.현실적인 리소스의 문제도 있었고요. 토스에선 빠른 실험을 위해 ‘매주’ 앱 배포를 진행하는데요. 이런 환경에서 사일로 업무도 하고, 구조 개선을 위한 모듈 분리도 하는 건 쉽지 않은 일이었죠.iOS 챕터에서는 작업량을 줄이기 위해서, Tuist에서 제공하는 Stencil 템플릿과 Tuist Scaffold 기능을 최대한 활용했어요. 토스 앱의 모듈 구조에 알맞게 Tuist extension 을 적절히 구현했습니다. 다양한 Template 을 만들어서 단 1줄의 코드로 새로운 모듈을 생성할 수 있도록 했죠.하지만 무엇보다도 iOS 챕터 전체가 모두 적극적으로 참여했던 게 도입을 해낼 수 있었던 가장 큰 이유예요. 기존 구조의 문제를 겪고 있던 분들이 도입의 필요성에 많이 공감해 주셨고요. 너 나 할 것 없이 담당하고 있던 기능들에 Microfeatures 아키텍처를 적용해 나갔어요. 담당자가 퇴사를 했다든지, 공통적으로 쓰는 모듈이라든지 그런 애-매한 모듈들도 있었는데요. 다들 본인 것처럼 적극적으로 나서주셨죠.Microfeature 아키텍처의 장점은 이게 끝이 아닙니다.Example 앱이 있기 때문이죠!Example 앱은 필요한 기능만을 담은 별도의 미니 앱이라고 보시면 돼요. Microfeature 아키텍처가 도입된 후로, 개발할 때 Example 앱을 적극 활용하고 있어요. 실제로 송금 Example 앱, 자동이체 Example 앱, 본인확인 Example 앱 등등 다양한 Example 앱이 있습니다.Microfeature 아키텍처가 있다면 Example 앱을 만드는 건 어렵지 않아요. 구현을 확인해 보고 싶은 기능의 Feature 모듈을 사용하고요. 나머지 기능의 경우 Testing 모듈에 있는 Mock을 연결하면 되죠.아래 그림은 송금 Example 앱인데요. 실제 송금 결과 케이스들을 바로바로 볼 수 있도록 만들어놓았어요.특히 UI 개발을 할 때 무척 편리합니다. \n아까 말씀드린 것처럼, 토스 앱은 어마어마하게 큰 앱이거든요. Swift만 100만 줄이 넘고요. 모듈은 약 700개나 있고요. 그니까 빌드 시간이 엄청 길 수밖에 없어요.만약 토스 앱 전체를 빌드하고 나서, 수정한 화면이 어떻게 보이는지 확인해야 된다면? UI 수정 확인 한번 하려고 한참을 기다려야 합니다. 빌드 할 때마다 수십 초씩 걸린다는 건 상당히 불편한 일이잖아요. 해보신 분들은 이게 얼마나 생산성을 떨어뜨리는지 아실 거예요.Example 앱은 이럴 때 빛을 발합니다. Example 앱은 토스 전체 앱 빌드보다 훨씬 빠르거든요. (무려 5배) 전체 앱이 아닌 일부분만을 빌드하기 때문이죠.Example을 하다 보면, 협업을 하는데도 굉장히 편리해요.토스 디자인 시스템(TDS)을 만드는 디자인 플랫폼 팀의 예시를 들어드릴게요.TDS에는 AnimateTop라는 컴포넌트가 있습니다. 말 그대로 애니메이션 효과가 들어있는 제목이에요. 그런데 이걸 코드로만 보면, 실제 느낌은 알기 어렵잖아요.그럴 때 ShowCaseExample 앱을 사용해요. AnimateTop이 다양한 속성을 적용했을 때 어떻게 보이는지 실제로 확인해 볼 수 있죠.개발을 하고 나면, Example 앱을 사내에서 배포할 수 있어요.복잡한 화면이나 애니메이션을 개발하다 보면 디자이너 분과 빌드 된 앱을 같이 확인하는 경우가 꽤 많아요. 중간중간 실제 화면을 확인해보기 위해 다른 팀원에게 앱을 빌드하여 보여주는 경우도 흔히 있고요.그럴 때 Example 앱을 사용하면, “00님, 이거 와서 봐주세요, 어때요?” 매번 여쭤볼 필요가 없어요. Example 앱 배포했습니다! 알려드리면 직접 받아서 확인하면 되거든요. 직접 사용하시는 PO나 디자이너 분들도 정말 편하다고 하시더라고요.디자인 플랫폼 팀에서는 새로운 인터랙션, 컴포넌트 등을 만드는 일이 많은데요. 이 Example 앱을 적극 활용해서 개발부터 QA까지 하고 있습니다.🔔 요약 정리앱의 코드 베이스가 커지고 복잡해질수록, 모듈 분리와 관리가 중요해진다.기존 토스 iOS 앱은 일반적인 계층 구조로 나눠서 모듈을 관리했다.하지만 앱 내 서비스가 계속 많아지면서, 같은 계층 내의 의존 관계가 너무 복잡해졌다.토스 iOS 챕터는 이 문제를 해결하기 위해 Microfeature 아키텍처를 도입했다.Microfeature 아키텍처 도입은 작업량, 컨플릭, 리소스의 문제로 쉽지 않은 과제였지만, iOS 챕터 모두가 합심하여 결국 성공!Microfeature 아키텍처를 도입하면서 Example 앱도 적극적으로 쓰게 되었다.Example 앱은 전체 앱보다 빌드가 5배 빨라서 개발 생산성이 올라가고, 사내 배포가 가능해서 협업에도 도움이 된다.재미있게 읽으셨나요?좋았는지, 아쉬웠는지, 아래 이모지를 눌러 의견을 들려주세요.😍🤔아티클 공유하기"
}